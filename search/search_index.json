{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p>Welcome to your go-to resource hub at Runtime Revolution! This is the place for documentation, tutorials, and troubleshooting guides to make your work life easier.</p> <ul> <li>Node.js</li> <li>Python</li> <li>Rails</li> </ul>"},{"location":"#contribute-to-the-knowledge-base","title":"Contribute to the Knowledge Base","text":"<p>If you have tips, tricks, bugs or anything you would like to share, feel free to create a pull request on GitHub.</p>"},{"location":"node/","title":"Start Here","text":"<p>Welcome to the JavaScript section of our knowledge base, where you'll find essential resources and insights related to JavaScript, Node.js, and React.js.</p> <p>The goal is to equip you with the foundational knowledge needed to start your journey in Node.js and React.js, along with advanced topics to spark your curiosity. Whether you're just getting started or looking to expand your skills, you'll find valuable information here.</p> <p>Feel free to explore the topics that interest you. If you encounter any issues or have new insights to share, you're welcome to open a pull request on GitHub. (1)</p>"},{"location":"node/#overview","title":"Overview","text":""},{"location":"node/#setting-up-your-development-environment","title":"Setting Up Your Development Environment","text":"<p>Whether you're struggling with a new OS or just looking to optimize your setup, this section has something for everyone. We've packed it with tips, recommended tools, and step-by-step setup guides focused on JavaScript development.</p>"},{"location":"node/#javascript-basics-and-beyond","title":"JavaScript Basics and Beyond","text":"<p>New to JavaScript or not, it's always nice to revisit the basics. Here you will find everything you need to get comfortable with the language, from syntax and core concepts to useful functions and best practices, including version and package management. This section will set a strong foundation for your journey with Node.js and React.js.</p>"},{"location":"node/#introduction-to-nodejs","title":"Introduction to Node.js","text":"<p>Under construction!  Introduction to Node.js coming soon.</p>"},{"location":"node/#introduction-to-reactjs","title":"Introduction to React.js","text":"<p>Under construction!  Introduction to React.js coming soon.</p>"},{"location":"node/javascript/","title":"JavaScript Basics and Beyond","text":"<p>Under construction! \ud83d\udea7 JavaScript Basics and Beyond coming soon.</p>"},{"location":"node/setup/","title":"Setting Up Your Development Environment","text":""},{"location":"node/setup/#getting-comfortable-with-mac-os","title":"Getting Comfortable with Mac OS","text":"<p>Transitioning to Mac OS can feel like stepping into a whole new world, especially if you're used to a different operating system. Here are some helpful resources to guide you through the transition and help you master the ins and outs of Mac OS:</p> <ul> <li>Switching from Windows to Mac? Everything You Need to Know </li> <li>How to Use a Mac for Beginners</li> <li>Mac keyboard shortcuts</li> </ul> <p>These resources have been recommended by those who've made the switch and lived to tell the tale. If you find any new resources feel free to add them to this section.</p>"},{"location":"node/setup/#recommended-tools-and-apps","title":"Recommended Tools and Apps","text":"<p>If you're new to macOS or JavaScript development, figuring out which apps to use can be overwhelming. To help with that we created this list with our favorites. Since app preferences vary, feel free to pick and choose what works best for you. And if you have any cool recommendations of your own, we'd love to hear them!</p>"},{"location":"node/setup/tools/","title":"Recommended Tools and Apps","text":"<p>If you're new to macOS or JavaScript development, figuring out which apps to use can be overwhelming. Here's a list of our favorites. Since app preferences vary, feel free to pick and choose what works best for you. And if you have any cool recommendations of your own, we'd love to hear them!</p>"},{"location":"node/setup/tools/#package-management","title":"Package management","text":"<p>A good package manager can make your life a whole lot easier, but since macOS doesn't come with a built-in package manager like some other Unix/Linux distributions we will need to install one.</p>"},{"location":"node/setup/tools/#homebrew","title":"Homebrew","text":"<p>One of the most popular package managers for Mac is Homebrew. To install it just run the following command in your terminal:</p> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> <p>Now that you have Homebrew installed, you can check if everything went well by running:</p> <pre><code>brew doctor\n</code></pre>"},{"location":"node/setup/tools/#so-how-do-i-use-it","title":"So how do I use it?","text":"<p>There are two ways to install packages use Homebrew. The command to use will depend on the type of package you want to install.</p> <p>For CLI (Command Line Interface) packages you can simply run:</p> CommandExample <pre><code>brew install &lt;package-name&gt;\n</code></pre> <pre><code># For example if you want to install wget\n\nbrew install wget\n</code></pre> <p>For GUI (Graphical User Interface) packages we need to specify in the command that we want to install a Cask, not a Formulae. For that you can run:</p> CommandExample <pre><code>brew install --cask &lt;package-name&gt;\n</code></pre> <pre><code># For example if you want to install iterm2\n\nbrew install --cask iterm2\n</code></pre>"},{"location":"node/setup/tools/#alternatives","title":"Alternatives","text":"<ul> <li>Macports</li> <li>Nix</li> </ul>"},{"location":"node/setup/tools/#terminal","title":"Terminal","text":"<p>The default macOS terminal app has its limitations, so you might want to explore more featured and customizable alternatives.</p>"},{"location":"node/setup/tools/#iterm2","title":"iTerm2","text":"<p>One of the most popular choices is iTerm2. Installation is straightforward: simply download the binary from their downloads page and move it to your <code>/Applications</code> folder.</p> <p>Alternatively, if you prefer using Homebrew (which we previously installed) to manage desktop apps, you can install iTerm2 with the following command:</p> <pre><code>brew install --cask iterm2\n</code></pre>"},{"location":"node/setup/tools/#alternatives_1","title":"Alternatives","text":"<ul> <li>Alacritty</li> <li>Kitty</li> <li>Hyper</li> </ul>"},{"location":"node/setup/tools/#shell","title":"Shell","text":"<p>A Unix shell is a command-line interpreter that provides a command line interface for UNIX systems like your MacBook. There are different shells available, so you can choose the one you prefer.</p> <p>One of the most well-known is BASH (Bourne Again Shell), which is the default shell on most Linux distributions and was the default on macOS until the release of macOS Catalina (10.15). Since then, the default shell on macOS is zsh. We'll assume this is the one you have installed and set as default.</p> <p>To check your default shell, run: <pre><code>echo $0\n</code></pre></p> <p>You can choose to use the shell of your choice and follow the install steps for the one you pick.</p>"},{"location":"node/setup/tools/#zsh-z-shell","title":"Zsh (Z Shell)","text":"<p>If you don't have zsh (and want to switch to it) here's how to do it:</p> <pre><code>brew install zsh &amp;&amp; chsh -s $(which zsh)\n</code></pre>"},{"location":"node/setup/tools/#oh-my-zsh","title":"Oh My Zsh","text":"<p>A great way to enhance your zsh configuration is by installing Oh My Zsh. Oh My Zsh is a framework to manage your zsh configuration. You can read more about it here</p> <p>To install Oh My Zsh, run: <pre><code>sh -c \"$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\n</code></pre></p> <p>The installation will create the necessary configuration files, which you can edit as needed.</p> <p>To open your configuration file run: <pre><code>open ~./zsh\n</code></pre></p>"},{"location":"node/setup/tools/#alternatives_2","title":"Alternatives","text":"<ul> <li>Fish</li> <li>Korn </li> </ul>"},{"location":"node/setup/tools/#ide","title":"IDE","text":"<p>An Integrated Development Environment (IDE) is essential for coding and managing your projects efficiently. There are several IDEs available, but if you are not sure what to use we recommend using Visual Studio Code (VS Code).</p> <p>To install VS Code you can download it from the official website or install it via Homebrew:</p> <pre><code>brew install --cask visual-studio-code\n</code></pre>"},{"location":"node/setup/tools/#alternatives_3","title":"Alternatives","text":"<ul> <li>VSCodium</li> <li>Sublime Text</li> </ul>"},{"location":"node/setup/tools/#mongodb-gui","title":"MongoDB GUI","text":"<p>For managing MongoDB databases, we recommend MongoDB Compass, the official GUI. It offers an easy-to-use interface to visualize, query, and manage your databases.</p> <p>To install it, download the binary from their downloads page and move it to your <code>/Applications</code> folder.</p>"},{"location":"node/setup/tools/#alternatives_4","title":"Alternatives","text":"<ul> <li>Studio 3T</li> </ul>"},{"location":"node/setup/tools/#api-client","title":"API Client","text":"<p>For testing and interacting with APIs, we recommend Postman, since it is the tool used by most people, making it easier to share configurations across team members.</p> <p>To install it, download the binary from their downloads page and move it to your <code>/Applications</code> folder.</p>"},{"location":"node/setup/tools/#alternatives_5","title":"Alternatives","text":"<ul> <li>Insomnia</li> </ul>"},{"location":"python/faq/","title":"Frequentky Asked Questions","text":""},{"location":"python/faq/#how-to-setup-mac-m1","title":"How to setup Mac M1?","text":"<p>Since the M1 Macs have a different architecture than the previous releases, there might be some incompatibility issues with some apps/software/libraries. To avoid that experience you're advised to activate Rosetta https://support.apple.com/en-us/HT211861. </p> <ul> <li>Install all OS updates, via System Preferences</li> <li>To activate Rossetta open Finder<ol> <li>if using iTerm2 go to Applications</li> <li>if using default Terminal go to Applications/Utilities</li> </ol> </li> <li>Right-click the terminal app of your choice and select the <code>Get Info</code> option</li> <li>Then just click on the Checkbox that says <code>Open using Rosetta</code></li> <li>Restart the terminal </li> <li>Now your terminal can run as it would on an Apple with an Intel processor</li> <li>Install HomeBrew https://brew.sh/ from that terminal</li> </ul>"},{"location":"python/onboarding/","title":"Onboarding","text":"<p>The onboarding process consists on the creation of the following sections as Github issues. </p>"},{"location":"python/onboarding/#general-references","title":"General References","text":"<ul> <li>A Byte of Python (Learn Python)</li> <li>Awesome Python (List of Python Resources)</li> <li>Getting Started with Django<ul> <li>Quick install guide</li> <li>Write your first Django app</li> <li>Django documentation</li> </ul> </li> <li>Django REST framework</li> <li>Pandas<ul> <li>kaggle</li> <li>cheatsheet</li> </ul> </li> </ul>"},{"location":"python/onboarding/#day-1-getting-familiar","title":"Day 1 - Getting Familiar","text":"<ol> <li>Set up your project environment by following this guide.</li> <li>Update REAME.md with the initial setup</li> <li>A pull request must include: README.md and a requirements.txt file with the installed python packages</li> <li> <p>Then get familiar with the language:</p> <ul> <li>Basics</li> <li>Operators and Expressions</li> <li>Control Flow</li> <li>Functions</li> <li>Modules</li> <li>Data Structures</li> <li>Object Oriented Programming</li> <li>Input and Output</li> <li>Exceptions</li> <li>Map, Filter, Reduce</li> <li>More Challenge yourself \ud83d\udcaa. Do a code challenge in Python3.</li> </ul> </li> </ol>"},{"location":"python/onboarding/#day-2-django-tutorial-part-1-and-2","title":"Day 2 - Django Tutorial part 1 and 2","text":"<p>Django Tutorial (Part 1, 2) Part 1</p> <ul> <li>Creating a project</li> <li>The development server</li> <li>Creating the Polls app</li> <li>Write your first view</li> </ul> <p>Part 2</p> <ul> <li>Database setup</li> <li>Creating models</li> <li>Activating models</li> <li>Playing with the API</li> <li>Introducing the Django Admin</li> <li>Use the models to experience with queryset filters available here https://docs.djangoproject.com/en/4.1/ref/models/querysets/</li> </ul>"},{"location":"python/onboarding/#day-3-django-tutorial-part-3-and-4","title":"Day 3 - Django tutorial part 3 and 4","text":"<p>Django Tutorial (Part 3, 4) Part 3</p> <ul> <li>Writing more views</li> <li>Write views that actually do something</li> <li>Raising a 404 error</li> <li>Use the template system</li> </ul> <p>Part 4</p> <ul> <li>Write a minimal form</li> <li>Use generic views: Less code is better</li> </ul>"},{"location":"python/onboarding/#day-4-django-tutorial-part-56-and-7","title":"Day 4 - Django tutorial part 5,6 and 7","text":"<p>Django Tutorial (Part 5, 6, 7) Part 5</p> <ul> <li>Introducing automated testing</li> <li>Basic testing strategies</li> <li>Writing our first test</li> <li>Test a view</li> <li>When testing, more is better</li> </ul> <p>Part 6 / 7 (Optional)</p> <ul> <li>Customize your app's look and feel</li> <li>Adding a background-image</li> <li>Customize the admin form</li> <li>Adding related objects</li> <li>Customize the admin change list</li> <li>Customize the admin look and feel</li> <li>Customize the admin index page</li> </ul>"},{"location":"python/onboarding/#day-5-django-rest-framework-part-1-2-and-3","title":"Day 5 - Django Rest Framework Part 1, 2 and 3","text":"<p>Django REST Framework Tutorial (Part 1, 2) Part 1</p> <ul> <li>Setting up a new environment</li> <li>Working with Models</li> <li>Working with Serializers</li> <li>Writing regular Django views using our Serializer</li> <li>Testing our first attempt at a Web API</li> </ul> <p>Part 2</p> <ul> <li>Requests and Responses</li> </ul> <p>Part 3</p> <ul> <li>Class Based Views</li> <li>Mixins</li> <li>Generic Class Based Views</li> </ul>"},{"location":"python/onboarding/#day-6-django-rest-framework-part-4-5-and-6","title":"Day 6 - Django Rest Framework Part 4, 5 and 6","text":"<p>Django REST Framework Tutorial (Part 4, 5) Part 4</p> <ul> <li>Adding permissions to views</li> <li>Create a Login API</li> <li>Object level permissions</li> <li>Authenticating with the API</li> </ul> <p>Part 5</p> <ul> <li>Adding a root API</li> <li>Hyperlinking our API</li> <li>Adding pagination</li> </ul> <p>Part 6</p> <ul> <li>Using ViewSets</li> <li>Using routers</li> <li>Adding pagination</li> </ul>"},{"location":"python/onboarding/#day-7-pandas","title":"Day 7 - Pandas","text":"<p>pandas getting started</p> <ul> <li>Installing pandas</li> <li>pandas overview</li> <li>Getting started tutorials</li> </ul>"},{"location":"python/onboarding/#day-8-project","title":"Day 8 - Project","text":"<p>This project consists on the development of a Rest api to manage/views: Authors, Books and Categories.</p>"},{"location":"python/onboarding/#requirements","title":"Requirements","text":"<ul> <li>User needs to be able to manage Authors, Books, and Categories in the app.</li> <li>Each Author can have many Books that he/her has written and each book can be included in multiple categories.</li> <li>The User should be able to view lists of Authors and Books.</li> <li>The Books should be able to be filtered by Author and by Category.</li> </ul> <p>Optional: The App should also include a page to view some basic statistics, like the number of Books per Author, or the number of Books per Category.</p> <p>Optional: To complicate the models. A book can have many instances and users can request an instance to take home with a requested date.</p>"},{"location":"python/onboarding/#acceptance-criteria","title":"Acceptance Criteria","text":"<ul> <li>Design the model entity relation for this project:<ul> <li>use Mermaid, this is supported out of the box by Github's Markdown</li> </ul> </li> <li>Design the API endpoints, including:<ul> <li>path</li> <li>request</li> <li>response</li> </ul> </li> </ul> <p>Once the design/planning part has been taken care of and agreed upon, please create tickets/issues for each of the tasks. Having those created, their commits should respect the nomenclature used in the conventional commits:</p> <ul> <li>if it's a task: <code>task/&lt;number_of_the_ticket&gt;/small-description</code>;</li> <li>if it's a bug: <code>bugfix/&lt;number_of_the_ticket&gt;/small-description</code>;</li> <li>if it's a release: <code>chore/&lt;number_of_the_ticket&gt;/small-description</code>.</li> </ul> <p>Read the conventional commits documentation to check more examples and details of each commit type (i.e body, footer, etc).</p>"},{"location":"python/pre-commit/","title":"Pre-commit","text":""},{"location":"python/pre-commit/#introduction","title":"Introduction","text":"<p>Git hook scripts are useful for identifying simple issues before submission to code review. We run our hooks on every commit to automatically point out issues in code such as missing semicolons, trailing whitespace, and debug statements. By pointing these issues out before code review, this allows a code reviewer to focus on the architecture of a change while not wasting time with trivial style nitpicks.</p>"},{"location":"python/pre-commit/#motivation","title":"Motivation","text":"<ul> <li>When you forget to run a linter, and your commit fails on CI because</li> <li>When you commit a file, and the only change are the import order, because you use VScode or Pycharm, and  the last person who changed the code uses the opposite.</li> </ul>"},{"location":"python/pre-commit/#requirements","title":"Requirements","text":"<p>For this pre-commit, we are assuming that you want at least flake8, isort and black in dev  (requirements/pipenv/poetry). For your project you can remove(although you should be using black isort and a linter) or add other checks from  the complete list like:</p> <ul> <li>type checker for python</li> <li>docstring checker  PEP-257</li> <li>yaml lint</li> </ul> <p>Assuming you are using poetry you would run something like:</p> <pre><code>poetry add --dev pre-commit==2.19.0\npoetry add --dev flake8==3.9.2 \npoetry add --dev black==22.3.0 \npoetry add --dev isort==5.10.1\n</code></pre>"},{"location":"python/pre-commit/#setup","title":"Setup","text":"<p>Add a file on root named <code>.pre-commit-config.yaml</code> <pre><code>default_language_version:\n  python: python3.10\nrepos:\n  - repo: https://github.com/psf/black\n    rev: 22.3.0 # Replace by any tag/version: https://github.com/psf/black/tags\n    hooks:\n      - id: black\n  - repo: https://github.com/pycqa/isort\n    rev: \"5.10.1\" # Use the revision sha / tag you want to point at\n    hooks:\n      - id: isort\n        args: [\"--profile\", \"black\"]\n  - repo: https://github.com/pycqa/flake8\n    rev: \"4.0.1\"\n    hooks:\n      - id: flake8\n</code></pre></p> <p>optionally, but highly recommended, you could also add these useful checks</p> <pre><code>  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.1.0 # Use the ref you want to point at\n    hooks:\n      - id: check-added-large-files\n        args: [\"--maxkb=1024\"] # throw an error if we try to commit a 1MB or greater file\n      - id: check-case-conflict # so we don't rename files that will break case insensitive filesystems\n      - id: check-merge-conflict # don't accidentally commit files with incomplete merges\n      - id: end-of-file-fixer # makes all files end in a newline\n      - id: mixed-line-ending # fixes mixed line endings automatically\n      - id: no-commit-to-branch\n        args: [\"--branch\", \"master\"] # no commits to master\n</code></pre> <p>configure a <code>.pyproject.toml</code> file if you don't have one create one on root folder</p> <pre><code>[tool.black]\nline-length = 120\n\n[tool.isort]\nprofile = \"black\"\n</code></pre> <p>configure a <code>.flake8</code> file if you don't have one create one on root folder</p> <pre><code>[flake8]\nmax-line-length = 120\nmax-complexity=10\nexclude =\n    */migrations/*\n    __pycache__\n    manage.py\n    settings.py\n    env\n    .env\n    ./env\n    env/\n    .env/\n    .venv/\n    inspectdb_models\n</code></pre>"},{"location":"python/pre-commit/#activation-deactivation","title":"Activation / Deactivation","text":"<p>To enable pre-commit just run: <pre><code>pre-commit install\n</code></pre> To disable pre-commit just run: <pre><code>pre-commit uninstall\n</code></pre></p>"},{"location":"python/project-setup-guide/","title":"Project Setup Guide","text":"<p>This is a short guide to get you up and running as programmer working with python.</p> <p>This guide was intended for macOS using the \"old\" Intel x86 Processors, if you're living on the edge with the new Apple M1 Chip please check https://github.com/runtimerevolution/knowledge-base/python/faq/#how-to-setup-mac-m1</p>"},{"location":"python/project-setup-guide/#index","title":"Index","text":"<ul> <li>Pyenv</li> <li>Install</li> <li>Commands</li> <li>Pyenv Virtualenv</li> <li>Main Packages</li> <li>Web framework</li> <li>Linter</li> <li>Auto formatter </li> <li>Optional Packages</li> <li>Django debug toolbar</li> <li>Factory Boy</li> <li>djangorestframework</li> <li>Pandas</li> </ul>"},{"location":"python/project-setup-guide/#pyenv","title":"Pyenv","text":"<p>pyenv is a very handy version management system for python. You might be familiar with <code>nvm</code> if you worked with Node.js or <code>rvm</code> if you worked with Ruby before.</p> <p>Checkout the pyenv project on gitHub:</p> <p>https://github.com/pyenv/pyenv</p> <p>pyenv lets you easily switch between multiple versions of Python. It's simple, unobtrusive, and follows the UNIX tradition of single-purpose tools that do one thing well.</p>"},{"location":"python/project-setup-guide/#install","title":"Install","text":"<p>Depending on your current setup installation may vary, so I would recommend following the official installation guide here, but you can follow along if you have the following  prerequisites:</p> <ul> <li>Homebrew</li> <li>oh-my-zsh</li> </ul> <p>If you haven't done so, install Xcode Command Line Tools.</p> <pre><code>xcode-select --install\n</code></pre> <p>Install the Python build dependencies.</p> <pre><code>brew install openssl readline xz zlib postgresql\n</code></pre> <p>Now install pyenv with Homebrew.</p> <pre><code>brew update\nbrew install pyenv\n</code></pre>"},{"location":"python/project-setup-guide/#commands","title":"Commands","text":"<p>Let's go over some of the most important pyenv commands. You can check the full list here</p> <p>Test if you have pyenv correctly installed by checking pyenv version in my case I have version <code>1.2.21</code> but that's  not that important.</p> <pre><code>$ pyenv -v\npyenv 2.2.3\n</code></pre> <p>Lists all Python versions known to pyenv, and shows an asterisk next to the currently active version.</p> <pre><code>$ pyenv versions\n* system (set by /Users/Roberto/.pyenv/version)\n</code></pre> <p>To list the all available versions of Python. (You're probably only interested in the versions at the top without a  name)</p> <pre><code>pyenv install --list\n</code></pre> <p>Install a Python version</p> <pre><code>pyenv install 3.10.2\n</code></pre> <p>Sets the global version of Python to be used in all shells by writing the version name to the <code>~/.pyenv/version</code>  file. This version can be overridden by an application-specific <code>.python-version</code> file, or by setting the  <code>PYENV_VERSION</code> environment variable.</p> <pre><code>pyenv global 3.10.2\n</code></pre> <p>Sets a local application-specific Python version by writing the version name to a <code>.python-version</code> file in the  current directory. This version overrides the global version, and can be overridden itself by setting the  <code>PYENV_VERSION</code> environment variable or with the <code>pyenv shell</code> command.</p> <pre><code>pyenv local 3.10.2\n</code></pre>"},{"location":"python/project-setup-guide/#pyenv-virtualenv","title":"Pyenv Virtualenv","text":"<p>pyenv-virtualenv is a pyenv plugin  that provides features to manage virtualenvs and conda environments for Python on UNIX-like systems.</p> <p>install pyenv-virtualenv <pre><code>brew install pyenv-virtualenv\n</code></pre></p> <p>create your virtualenv using the installed python above <pre><code>pyenv virtualenv 3.10.2 your-env-or-project-name\n</code></pre></p> <p>activate your virtualenv <pre><code>source ~/.pyenv/versions/your-env-or-project-name/bin/activate\n</code></pre></p> <p>optionally you can configure your terminal to always start with your environment activated, this is helpful for  development. To do that simply add the previous command to your <code>.zshrc</code> or <code>.bashrc</code></p>"},{"location":"python/project-setup-guide/#main-packages","title":"Main Packages","text":""},{"location":"python/project-setup-guide/#web-framework","title":"Web framework","text":"<p>Your web framework, we are using django, when using the documentation make sure  you have the right version selected on the bottom lower right corner <pre><code>python -m pip install django\n</code></pre></p>"},{"location":"python/project-setup-guide/#db-package","title":"DB Package","text":"<p>psycopg2-binary is a package to interact with the DB</p> <pre><code>python -m pip install psycopg2-binary\n</code></pre>"},{"location":"python/project-setup-guide/#linter","title":"Linter","text":"<p>For linting we are using flake8. Some projects also use  pylint</p> <p>What is a linter? is a tool that analyzes source code to flag programming errors, bugs, stylistic errors, and  suspicious constructs. <pre><code>python -m pip install flake8\n</code></pre></p>"},{"location":"python/project-setup-guide/#auto-formatter","title":"Auto formatter","text":"<p>You can have any colour, as long as it's black</p> <p>Black is the auto-formatter chosen. It is important that the team uses the same  auto-formatter, so the code does not keep changing on different branches because of the developer default formatter IDE.</p> <pre><code>python -m pip install black\n</code></pre> <p>Read more here about how to configure black on your IDE, also sometimes we add the flag <code>--line-length 120</code> </p>"},{"location":"python/project-setup-guide/#optional-packages","title":"Optional Packages","text":"<p>These packages can be skipped as your on onboard might not use/require these libraries.</p>"},{"location":"python/project-setup-guide/#django-debug-toolbar","title":"Django debug toolbar","text":"<p>Django debug toolbar The Django Debug Toolbar is a configurable set of panels that display various debug information about the current  request/response and when clicked, display more details about the panel's content.</p> <p>The most used case for this package, is to check how many queries are performed on a list request, either by a view or  the api. And to know when we need to implement select_related or prefetch related</p> <p>In panels, you can find some extra information. Check the Third-party panels, for example  pimpler is useful to find memory usage</p>"},{"location":"python/project-setup-guide/#factory-boy","title":"Factory Boy","text":"<p>Factory Boy Very useful for making tests setup simpler</p>"},{"location":"python/project-setup-guide/#djangorestframework","title":"djangorestframework","text":"<p>Django REST framework is a powerful and flexible toolkit for building Web APIs.</p> <pre><code>python -m pip install djangorestframework\npython -m pip install markdown\npython -m pip install django-filter\n</code></pre>"},{"location":"python/project-setup-guide/#pandas","title":"Pandas","text":"<p>Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.</p> <pre><code>python -m pip install pandas\n</code></pre>"},{"location":"python/python_virtual_env/","title":"Python Virtual Environment","text":"<p>The virtual environment ecosystem is composed by 3 different parts:</p> <ul> <li>Python version Depending on the system you're using, a particular Python version can be installed via that system package manager or via <code>pyenv</code>.</li> <li>Virtual Environment A virtual environment is a Python environment such that the Python interpreter, libraries and scripts installed into it are isolated from those installed in other virtual environments, and (by default) any libraries installed in a \u201csystem\u201d Python, i.e., one which is installed as part of your operating system. docs.python.org</li> <li>Dependency management A list of all the Python dependencies for a given project.Typically called <code>requirements.txt</code>, but can also take a more complex form where there's a distinction between which dependencies belong to production and which belong to development/testing.</li> </ul>"},{"location":"python/python_virtual_env/#python-version","title":"Python version","text":"<p>The most popular option is <code>pyenv</code>, even when a particular version is available in the OS package manager</p>"},{"location":"python/python_virtual_env/#pyenv","title":"pyenv","text":"<p>Check the documentation at https://github.com/pyenv/pyenv .</p> <p>Allows to specify a particular Python version to be used in a virtual environment.</p> <p>List all Python versions available to pyenv</p> <pre><code>pyenv install -l\n</code></pre> <p>Choose a version and install it</p> <pre><code>pyenv install 3.10.4\n</code></pre> <p>Versions are available under are available under <code>~/.pyenv/versions/</code></p>"},{"location":"python/python_virtual_env/#global","title":"Global","text":"<p>The global command sets the global Python version, which is useful for ensuring a particular Python version by default. If you wanted to use 3.7.10 by default, then you could run this:</p> <pre><code>pyenv global 3.7.10\n</code></pre> <p>This command sets the ~/.pyenv/version to 3.7.10.</p>"},{"location":"python/python_virtual_env/#local","title":"Local","text":"<p>The local command is used to set an application-specific Python version</p> <pre><code>pyenv local 3.10.4\n</code></pre> <p>This command creates a .python-version file in your current directory. If you have pyenv active in your environment, this file will automatically activate this version for you.</p>"},{"location":"python/python_virtual_env/#virtual-environment","title":"Virtual Environment","text":""},{"location":"python/python_virtual_env/#venv","title":"venv","text":"<p>The most simple is <code>venv</code> using the Python system version</p> <pre><code>python -m venv simple-venv\n</code></pre> activate deactivate <code>source simple-venv/bin/activate</code> <code>deactivate</code>"},{"location":"python/python_virtual_env/#pyenv-virtualenv","title":"Pyenv + Virtualenv","text":"<p>Pyenv supports the creation of a virtualenv tied to a particular Python version.</p> <pre><code>pyenv virtualenv 3.10.4 sample-virtual-env\n</code></pre> activate deactivate <code>source ~/.pyenv/versions/sample-virtual-env/bin/activate</code> <code>deactivate</code> <code>pyenv local sample-virtual-env</code> <code>deactivate</code>"},{"location":"python/python_virtual_env/#dependency-management","title":"Dependency Management","text":"pip pipenv Poetry search pip search  pipenv search  poetry search  install pip install  pipenv install  poetry add  install dev pipenv install  --dev poetry add  --dev uninstall pip uninstall  pipenv uninstall  poetry remove  uninstall dev pipenv uninstall  --dev poetry remove  --dev list packages pip list pip list poetry show build poetry build publish poetry publish"},{"location":"python/python_virtual_env/#pip","title":"Pip","text":"<p>https://pip.pypa.io/en/stable/</p> <p>Pip is the package installer for Python. You can use it to install packages from the Python Package Index and other indexes.</p> <p>Pip is available by default</p>"},{"location":"python/python_virtual_env/#requirementstxt","title":"Requirements.txt","text":"<p>Even though this is the most common way of sharing project dependencies, it is not the greatest, since there is no distinction between production and development packages.</p> <p>Can be generated by running</p> <pre><code>pip freeze &gt; requirements.txt\n</code></pre>"},{"location":"python/python_virtual_env/#pipenv","title":"pipenv","text":"<p>https://pipenv.pypa.io/en/latest/</p> <p>Pipenv is a tool that aims to bring the best of all packaging worlds (bundler, composer, npm, cargo, yarn, etc.) to the Python world. Windows is a first-class citizen, in our world. It automatically creates and manages a virtualenv for your projects, as well as adds/removes packages from your Pipfile as you install/uninstall packages. It also generates the ever-important Pipfile.lock, which is used to produce deterministic builds. Pipenv uses Pipfile and Pipfile.lock to separate abstract dependency declarations from the last tested combination.</p>"},{"location":"python/python_virtual_env/#install","title":"Install","text":"<p>Before proceed with the installation, please make sure the virtual environment of your choice is active</p> <pre><code>pip install pipenv\n</code></pre>"},{"location":"python/python_virtual_env/#pipfile-and-pipfilelock","title":"Pipfile and Pipfile.lock","text":"<p>Pipfile holds the settings used in the project, below there's a list most common ones:</p> <ul> <li>source: python package repository, by default is set to pypi</li> <li>dev-packages: list of development packages and macthing version defined by the user, which is * if not specified</li> <li>packages: list of production packages and matching version defined by the user, which is * if not specified</li> <li>requires: python version of the project</li> </ul> <p>Even though pipfile allows for a package to be installed without a particular version being set, this is not advisable, since it will take longer to determine the best version to be installed. This is mostly noticed when the project has a lot of packages.</p> <p>Pipfile.lock holds the exact version that was installed upon the time of the version lock. As an example, lets say we're using Django but haven't set a particular version, this means everytime we run <code>pipenv install</code> the latest version of Django would be installed.</p>"},{"location":"python/python_virtual_env/#poetry","title":"Poetry","text":"<p>This is a packaging and dependency management all in one, which means:</p> <ul> <li>it supports builds</li> <li>you can publish your package to both public and private repositories</li> <li>includes a virtualenv https://python-poetry.org/docs/basic-usage/#using-your-virtual-environment.</li> </ul> <p>Check the documentation available at https://python-poetry.org/docs/, the installation should be system-wide, even though it can be installed on a particular environment.</p> <pre><code>poetry new poetry-demo\n</code></pre> <p>This will create the <code>poetry-demo</code> directory with the following content.</p> <pre><code>poetry-demo\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.rst\n\u251c\u2500\u2500 poetry_demo\n\u2502   \u2514\u2500\u2500 __init__.py\n\u2514\u2500\u2500 tests\n    \u251c\u2500\u2500 __init__.py\n    \u2514\u2500\u2500 test_poetry_demo.py\n</code></pre>"},{"location":"python/python_virtual_env/#poetry-pyenv","title":"Poetry + pyenv","text":"<p>To use poetry with a particular Python version, that version only needs to be available in the system. In this case, we're using a Python version provided by <code>pyenv</code>. For further documentation please check https://python-poetry.org/docs/managing-environments/#switching-between-environments</p> <pre><code>cd poetry-demo\npoetry env use ~/.pyenv/versions/3.10.4/bin/python\n</code></pre>"},{"location":"python/python_virtual_env/#dependency-management_1","title":"Dependency management","text":"<p>https://python-poetry.org/docs/dependency-specification/</p> <p>Similarly to Pipenv, Poetry also allows a version lock and a distinction between production and development packages.</p> <ul> <li>tool.poetry.dependencies: list of production packages and matching version defined by the user, which is * if not specified. Python version is also specified here</li> <li>tool.poetry.dev-dependencies: list of development packages and matching version defined by the user, which is * if not specified</li> </ul>"},{"location":"python/python_virtual_env/#pipx","title":"pipx","text":"<p>https://pypa.github.io/pipx/</p> <p>Pipx allows the installation and execution of Python applications in isolated environments. It's not recommended to use within a project, but very suitable for running standalone python applications.</p>"},{"location":"python/AI-onboarding/1_machine_learning/","title":"Machine Learning","text":"<p>This machine learning onboarding consists on a set of free courses and competitions in Kaggle, which requires registration (also free).</p>"},{"location":"python/AI-onboarding/1_machine_learning/#week-1","title":"Week 1","text":"<ol> <li>Pandas<ul> <li>Optional, only for people with no experience with Pandas</li> </ul> </li> <li>Intro to Machine Learning<ul> <li>Explains the basic of machine learning, shows how to build and evaluate  ML models</li> <li>Focus on regression models, namely decision trees and then random forests </li> </ul> </li> <li>Intermediate Machine Learning<ul> <li>Explain basic data preparation</li> <li>Expands on model evaluation</li> <li>Also introduces gradient boosting</li> </ul> </li> <li>Data Cleaning<ul> <li>Continuation of presentation data preparation techniques already introduced in the previous course</li> </ul> </li> <li>Feature Engineering<ul> <li>Describes techniques to identify important features or potential new features</li> <li>Suggests entering a competition in the end, great to apply the knowledge gathered from the previous courses</li> </ul> </li> <li>House Prices - Advanced Regression Techniques<ul> <li>Competition suggested at the end of the previous course</li> <li>Useful as an exercise for consolidating the knowledge from all the previous courses</li> </ul> </li> </ol>"},{"location":"python/AI-onboarding/1_machine_learning/#week-2","title":"Week 2","text":"<ol> <li>Continue working on your solution to the House Prices - Advanced Regression Techniques competition</li> </ol>"},{"location":"python/AI-onboarding/1_machine_learning/#week-3","title":"Week 3","text":"<ol> <li>Intro to Deep Learning<ul> <li>Shows how to build, evaluate and tune neural networks</li> <li>In addition to regression, explains how to use neural networks to solve classification problems</li> </ul> </li> <li>Enter one of the following courses suggested in the previous course:<ul> <li>Petals to the Metal - Flower Classification on TPU</li> <li>I\u2019m Something of a Painter Myself</li> <li>Natural Language Processing with Disaster Tweets</li> <li>Contradictory, My Dear Watson</li> </ul> </li> </ol>"},{"location":"python/AI-onboarding/2_large_language_models/","title":"Large Language Models","text":""},{"location":"python/AI-onboarding/2_large_language_models/#large-language-models-llms","title":"Large Language Models (LLMs)","text":""},{"location":"python/AI-onboarding/2_large_language_models/#relevant-topics","title":"Relevant Topics","text":"<ul> <li>Pre-training</li> <li>Fine-tuning</li> </ul>"},{"location":"python/AI-onboarding/2_large_language_models/#resources","title":"Resources","text":"Type Title Comments Course Introduction to Large Language Models <ul><li>8 hour free course. Requires enrollment</li><li>Begins with a very good 15 minute video describing LLMs and their main concepts</li><li>Includes a collection of useful pages of information about LLMs.</li><li>Has a mini quiz at the end about LLMs</li></ul> Video A Hackers' Guide to Language Models <ul><li>1h30 YouTube video</li><li>Describes LLMs and important concepts (e.g., tokenization, training, fine-tuning)</li><li>Explains some actual models (e.g., GPT-4) and their limitations</li></ul> Course Training &amp; Fine-Tuning LLMs for Production <ul><li>Hands-on course for training, fine-tuning and adapting LLMs to specific tasks</li><li>Running all of the course\u2019s examples will cost around $100, although it is not necessary to complete the course</li></ul> Video (Playlist) Training &amp; Fine-Tuning LLMs Course <ul><li>Series of 4 YouTube videos of about 1 hour each</li><li>They explain the basics of LLMs and important concepts (e.g., evaluation, data, training and fine-tuning) in a practical way</li><li>Skip video #4 since it has audio isues, video #5 is a reupload with fixed audio</li></ul>"},{"location":"python/AI-onboarding/2_large_language_models/#retrieval-augmented-generation-rag","title":"Retrieval Augmented Generation (RAG)","text":""},{"location":"python/AI-onboarding/2_large_language_models/#relevant-topics_1","title":"Relevant Topics","text":"<ul> <li>Embeddings</li> <li>Vector Databases</li> <li>Document Retrieval</li> <li>LangChain</li> </ul>"},{"location":"python/AI-onboarding/2_large_language_models/#resources_1","title":"Resources","text":"Type Title Comments Course LangChain for LLM Application Development <ul><li>Series of 8 videos</li><li>Each video (except for the first and last) is accompanied by a notebook and you are encouraged to explore it, testing different prompts to produce different outputs</li><li>The course was partially created and taught by the creator of LangChain</li></ul> Course LangChain Chat with Your Data <ul><li>Series of 8 videos</li><li>Follows up the first course</li><li>Thoroughly explains how to implement the RAG pipeline using LangChain, offering several different approaches for each of the steps</li><li>Explains how to produce an end-to-end chatbot that can answer questions about a certain dataset</li></ul> Video Vector Embeddings for Beginners <ul><li>35 minute video</li><li>Covers vector embeddings, vector databases and LangChain</li></ul> Video What is Retrieval-Augmented Generation <ul><li>6 minute video by IBM</li><li>Describes some of the challenges presented by LLMs</li><li>Describes what RAG is and how it solves those problems</li></ul> Video Chatbots with RAG: LangChain Full Walkthrough <ul><li>35 minute video</li><li>Good explanation of the RAG pipeline</li><li>Explains how to build a RAG chatbot with code</li><li>Requires API keys for OpenAI and Pinecone</li></ul>"},{"location":"python/AI-onboarding/2_large_language_models/#pandasai","title":"PandasAI","text":""},{"location":"python/AI-onboarding/2_large_language_models/#relevant-topics_2","title":"Relevant Topics","text":"<ul> <li>Smart Data Frames &amp; Datalakes</li> </ul>"},{"location":"python/AI-onboarding/2_large_language_models/#resources_2","title":"Resources","text":"Type Title Comments Video \u200bPandasAI - Talk to Your Data <ul><li>27 minute video, presented by the creator of PandasAI</li><li>Includes an explanation of how it works</li><li>Shows several examples of PandasAI\u2019s functionalities</li></ul> Webpage An Introduction to Pandas AI <ul><li>Short introduction to PandasAI with examples</li><li>Covers basic aspects like setting up PandasAI, prompting a dataframe for basic answers and charts</li></ul>"},{"location":"python/best_practices/code_principles/","title":"Code Principles","text":"<p>Code should be clear and easy to maintain, by maintaining we also mean writing and revisiting tests if necessary. Below there's a list of principles which used with The Zen of Python, will help us to write better code as well as to test it.</p>"},{"location":"python/best_practices/code_principles/#kiss-keep-it-simple-stupid","title":"KISS - Keep it simple, stupid <sup>1</sup>","text":"<p>The snippet below is hard to read and takes time to understand</p> <pre><code>f = lambda x: x if x in {0, 1} else f(x - 1) + f(x - 2)\n</code></pre> <p>Alternatively, the next code snippet is much easier to understand and maintain. </p> <pre><code>def fibonacci(number: int) -&gt; int:\n    if number in {0, 1}:\n        return number\n    return fibonacci(number - 1) + fibonacci(number - 2)\n</code></pre> <p>By adding descriptive names, type hints, and splitting the line into multiple ones it gets easier to read and maintain. </p> <p>The main goal in design should always be to be as easy to understand as possible.</p>"},{"location":"python/best_practices/code_principles/#dry-dont-repeat-yourself","title":"DRY - Don't repeat yourself <sup>2</sup>","text":"<p>This principle is about writing functions and automating sections of code that are repeated. If you perform the same task multiple times in your code, consider a function or a loop to make your workflow more efficient.</p> <p>Let's consider the next example where we'll be using a math expression to convert temperature values from fahrenheit  to celsius.</p> <pre><code>temp_1 = 32\nres_1 = (temp_1-32) * 5/9\n\ntemp_2 = 40\nres_2 = (temp_2-32) * 5/9\n</code></pre> <p>Some points to consider:</p> <ul> <li>If the calculation changes, we'd need to update both expressions</li> <li>it's not clear what the math expression is calculating, unless you're familiar with it</li> </ul> <p>Let's create a method for the math expression.</p> <pre><code>def conv_fahr_to_celsius(fahr: float) -&gt; float:\n    \"\"\"Convert temperature in Fahrenheit to Celsius.\n\n    Parameters:\n    -----------\n    fahr: float\n        The temperature in Fahrenheit.\n\n    Returns:\n    -----------\n    Celsius : int or float\n        The temperature in Celsius.\n    \"\"\"\n    celsius = (fahr-32) * 5/9\n    return celsius\n</code></pre> <p>And update the previous sample to use this method.</p> <pre><code>fahr_1 = 32\ncelsius_1 = conv_fahr_to_celsius(fahr_1)\n\nfahr_2 = 40\ncelsius_2 = conv_fahr_to_celsius(fahr_2)\n</code></pre> <p>In summary</p> <ul> <li>The code is cleaner, because the repeated calculation was replaced with a function</li> <li>If this function is well-defined with a docstring that describes what it does, it is easier to both understand and use. </li> <li>If you need to change the calculation itself, you can do so once in the function</li> </ul>"},{"location":"python/best_practices/code_principles/#soc-separation-of-concerns","title":"SoC - Separation of concerns <sup>1</sup>","text":"<p>A known example of this is the model-view-controller (MVC) design. MVC separates a program into three distinct areas: the data (model), the logic (controller), and what the page displays (view).</p>"},{"location":"python/best_practices/code_principles/#solid","title":"SOLID <sup>3</sup>","text":"<p>SOLID is a mnemonic acronym for five design principles intended to make software designs more understandable, flexible, and maintainable.</p> <ul> <li>Single-responsibility principle: \"A class should have one, and only one, reason to change.\"</li> <li>Open\u2013closed principle: \"Entities should be open for extension, but closed for modification.\"</li> <li>Liskov substitution principle: \"Functions that use pointers or references to base classes must be able to use objects of derived classes without knowing it.\"</li> <li>Interface segregation principle: \"A client should not be forced to implement an interface that it doesn\u2019t use.\"</li> <li>Dependency inversion principle: \"Depend upon abstractions, not concretions.\"</li> </ul>"},{"location":"python/best_practices/code_principles/#single-responsibility-principle-srp","title":"Single-responsibility principle (SRP)","text":"<p>Every component of your code (in general a class, but also a function) should have one and only one responsibility. As a consequence of that, there should be only a reason to change it.</p> <p>Too often you see a piece of code that takes care of an entire process all at once. I.e., A function that loads data, modifies and, plots them, all before returning its result.</p> <p>Let\u2019s take a simpler example, where we have a list of number L = [n1, n2, \u2026, nx] and we compute some mathematical functions to this list. For example, compute the mean, median, etc.</p> <p>A bad approach would be to have a single function doing all the work:</p> <pre><code>import numpy as np\n\ndef math_operations(list_):\n    # Compute Average\n    print(f\"the mean is {np.mean(list_)}\")\n    # Compute Max\n    print(f\"the max is {np.max(list_)}\") \n\nmath_operations(list_ = [1,2,3,4,5])\n# the mean is 3.0\n# the max is 5\n</code></pre> <p>The first thing we should do, to make this more SRP compliant, is to split the function math_operations into atomic functions! Thus, when a function\u2019s responsibility cannot be divided into more sub-parts.</p> <p>The second step is to make a single function (or class), generically named, \u201cmain\u201d. This will call all the other functions one-by-one in a step-to-step process.</p> <pre><code>import numpy as np\n\ndef get_mean(list_):\n    \"\"\"\n        Compute Mean\n    \"\"\"\n    print(f\"the mean is {np.mean(list_)}\") \n\ndef get_max(list_):\n    \"\"\"\n        Compute Max\n    \"\"\"\n    print(f\"the max is {np.max(list_)}\") \n\ndef main(list_): \n    # Compute Average\n    get_mean(list_)\n    # Compute Max\n    get_max(list_)\n\nmain([1,2,3,4,5])\n# the mean is 3.0\n# the max is 5\n</code></pre> <p>Now, you would only have one single reason to change each function connected with \u201cmain\u201d.</p> <p>The result of this simple action is that now:</p> <ol> <li>It is easier to localize errors. Any error in execution will point out to a smaller section of your code, accelerating your debug phase.</li> <li>Any part of the code is reusable in other section of your code.</li> <li>Moreover and, often overlooked, is that it is easier to create testing for each function of your code. Side note on testing: You should write tests before you actually write the script. But, this is often ignored in favour of creating some nice result to be shown to the stakeholders instead.</li> </ol> <p>This is already a much bigger improvement with respect to the first code example. But, having created a \u201cmain\u201d and calling functions with single responsibility is not the full fulfilment of the SR principle. Indeed, our \u201cmain\u201d has many reasons to be changed. The class is actually fragile and hard to maintain. To solve that, let\u2019s introduce the next principle.</p>"},{"location":"python/best_practices/code_principles/#openclosed-principle-ocp","title":"Open/Closed principle (OCP)","text":"<p>You should not need to modify the code you have already written to accommodate new functionality, but simply add what you now need.</p> <p>This does not mean that you cannot change your code when the code premises needs to be modified, but that if you need to add new functions similar to the one present, you should not require to change other parts of the code. To clarify this point let\u2019s refer to the example we saw earlier. If we wanted to add new functionality, for example, compute the median, we should have created a new method function and add its invocation to \u201cmain\u201d. That would have added an extension but also modified the main.</p> <p>We can solve this by turning all the functions we wrote into subclasses of a class. In this case, I have created an abstract class called \u201cOperations\u201d with an abstract method \u201cget_operation\u201d. (Abstract classes are generally an advanced topic. If you don\u2019t know what an abstract class is, you can run the following code even without).</p> <p>Now, all the old functions, now classes are called by the subclasses() method. That will find all classes inheriting from Operations and operate the function \u201coperations\u201d that is present in all subclasses.</p> <p>for additional information on abstractmethod decorator please check https://docs.python.org/3/library/abc.html#abc.abstractmethod</p> <pre><code>import numpy as np\nfrom abc import ABC, abstractmethod\n\nclass Operations(ABC):\n    \"\"\"Operations\"\"\"\n    @staticmethod\n    @abstractmethod\n    def operation(list_):\n        pass\n\nclass Mean(Operations):\n    \"\"\"Compute Max\"\"\"\n    @staticmethod\n    def operation(list_):\n        print(f\"The mean is {np.mean(list_)}\")\n\nclass Max(Operations):\n    \"\"\"Compute Max\"\"\"\n    @staticmethod\n    def operation(list_):\n        print(f\"The max is {np.max(list_)}\")\n\nclass Main:\n    \"\"\"Main\"\"\"\n    @staticmethod\n    @abstractmethod\n    def get_operations(list_):\n        # __subclasses__ will find all classes inheriting from Operations\n        for operation in Operations.__subclasses__():\n            operation.operation(list_)\n\n\nif __name__ == \"__main__\":\n    Main.get_operations([1,2,3,4,5])\n# The mean is 3.0\n# The max is 5\n</code></pre> <p>If now we want to add a new operation e.g.: median, we will only need to add a class \u201cMedian\u201d inheriting from the class \u201cOperations\u201d. The newly formed subclass will be immediately picked up by subclasses() and no modification in any other part of the code needs to happen.</p> <p>The result is a very flexible class, that requires minimum time to be maintained.</p>"},{"location":"python/best_practices/code_principles/#the-liskov-substitution-principle-lsp","title":"The Liskov substitution principle (LSP)","text":"<p>Functions that use pointers or references to base classes must be able to use objects of derived classes without knowing it, that alternatively can be expressed as, derived classes must be substitutable for their base classes.</p> <p>In (maybe) simpler words, if a subclass redefines a function also present in the parent class, a client-user should not be noticing any difference in behaviour, and it is a substitute for the base class. For example, if you are using a function and your colleague change the base class, you should not notice any difference in the function that you are using.</p> <p>Among all the SOLID principle, this is the most abstruse to understand and to explain. For this principle, there is no standard \u201ctemplate-like\u201d solution where it must be applied, and it is hard to offer a \u201cstandard example\u201d to showcase.</p> <p>In the most simplistic way, I can put it, this principle can be summarised by saying: If in a subclass, you redefine a function that is also present in the base class, the two functions ought to have the same behaviour. This, though, does not mean that they must be mandatory equal, but that the user, should expect that the same type of result, given the same input. In the example ocp.py, the \u201coperation\u201d method is present in the subclasses and in the base class, and an end-user should expect the same behaviour from the two.</p> <p>The result of this principle is that we\u2019d write our code in a consistent manner and, the end-user will need to learn how our code works, only one.</p> <p>A consequence of LSP is that: the new redefined function in the subclass should be valid and be possibly used wherever the same function in the parent class is used.</p> <p>This is not, typically the case, indeed usually we, human, think in terms of set theory. Having a class that define a concept and subclasses that expand the first with an exception or different behaviour.</p> <p>For example, the subclass \u201cPlatypus\u201d, of the base class \u201cMammals\u201d, would have the exception that these mammals lay eggs. The LSP, tell us that it would create a function called \u201cgive_birth\u201d, this function will have different behaviour for the subclass Platypus and the subclass Dog. Therefore, we should have had a more abstract base class than Mammals that accommodate this. If this sounds very confusing, do not worry, the application of this latter aspect of the LSP is rarely fully implemented, and it rarely leaves the theoretical textbooks.</p>"},{"location":"python/best_practices/code_principles/#the-interface-segregation-principle-isp","title":"The Interface Segregation Principle (ISP)","text":"<p>Many client-specific interfaces are better than one general-purpose interface. In the context of classes, an interface is considered, all the methods and properties exposed, thus, everything that a user can interact with that belongs to that class.</p> <p>In this sense, the IS principles tell us that a class should only have the interface needed (SRP) and avoid methods that won\u2019t work or that have no reason to be part of that class.</p> <p>This problem arises, primarily, when, a subclass inherits methods from a base class that it does not need.</p> <p>Let\u2019s see an example:</p> <p>for additional information on abstractmethod decorator please check https://docs.python.org/3/library/abc.html#abc.abstractmethod</p> <pre><code>from abc import ABC, abstractmethod\n\nclass Mammals(ABC):\n    @staticmethod\n    @abstractmethod\n    def swim():\n        print(\"Can Swim\")\n\n    @staticmethod\n    @abstractmethod\n    def walk():\n        print(\"Can Walk\")\n\nclass Human(Mammals):\n    @staticmethod\n    def swim():\n        return print(\"Humans can swim\")\n\n    @staticmethod\n    def walk():\n        return print(\"Humans can walk\")\n\nclass Whale(Mammals):\n    @staticmethod\n    def swim():\n        return print(\"Whales can swim\") \n</code></pre> <p>For this example, we have got the abstract class \u201cMammals\u201d that has two abstract methods: \u201cwalk\u201d and \u201cswim\u201d. These two elements will belong to the subclass \u201cHuman\u201d, whereas only \u201cswim\u201d will belong to the subclass \u201cWhale\u201d.</p> <p>And indeed, if we run this code we could have:</p> <pre><code>Human.swim()\nHuman.walk()\n\nWhale.swim()\nWhale.walk()\n\n# Humans can swim\n# Humans can walk\n# Whales can swim\n# Can Walk\n</code></pre> <p>The subclass whale can still invoke the method \u201cwalk\u201d but it shouldn\u2019t, and we must avoid it.</p> <p>The way suggested by ISP is to create more client-specific interfaces rather than one general-purpose interface. So, our code example becomes:</p> <pre><code>from abc import ABC, abstractmethod\n\nclass Walker(ABC):\n    @staticmethod\n    @abstractmethod\n    def walk():\n        return print(\"Can Walk\")\n\nclass Swimmer(ABC):\n    @staticmethod\n    @abstractmethod\n    def swim():\n        return print(\"Can Swim\")\n\nclass Human(Walker, Swimmer):\n    @staticmethod\n    def walk():\n        return print(\"Humans can walk\")\n    @staticmethod\n    def swim():\n        return print(\"Humans can swim\")\n\nclass Whale(Swimmer):\n    @staticmethod\n    def swim():\n        return print(\"Whales can swim\") \n\nif __name__ == \"__main__\":\n  Human.walk()\n  Human.swim()\n\n  Whale.swim()\n  Whale.walk()\n\n# Humans can walk\n# Humans can swim\n# Whales can swim\n# AttributeError: type object 'Whale' has no attribute 'walk'\n</code></pre> <p>Now, every subclass inherits only what it needs, avoiding invoking an out-of-context (wrong) sub-method. That might create an error hard to catch.</p> <p>This principle is closely connected with the other ones and specifically, it tells us to keep the content of a subclass clean from elements of no use to that subclass. This has the final aim to keep our classes clean and minimise mistakes.</p>"},{"location":"python/best_practices/code_principles/#dependency-inversion-principle-dip","title":"Dependency Inversion Principle (DIP)","text":"<p>Abstractions should not depend on details. Details should depend on abstraction. High-level modules should not depend on low-level modules. Both should depend on abstractions. So, that abstractions (e.g., the interface, as seen above) should not be dependent on low-level methods but both should depend on a third interface.</p> <p>To better explain this concept, I prefer to think of a sort of information flow.</p> <p>Imagine that you have a program that takes in input a specific set of info (a file, a format, etc) and you wrote a script to process it. What would happen if that info were subject to change? You would have to rewrite your script and adjust the new format. Losing the retro compatibility with the older files.</p> <p>However, you could solve this by creating a third abstraction that takes the info as input and passes it to the others. This is basically what an API is also, used for.</p> <pre><code>flowchart LR\n    ObjectA --&gt; |references| ObjectB\n    subgraph Package B\n    ObjectB\n    end\n    subgraph Package A\n    ObjectA\n    end</code></pre> <pre><code>flowchart TB\n    ObjectB --&gt; |inherits| Interface\n    subgraph Package B\n    ObjectB\n    end\n    subgraph Package A\n    ObjectA--&gt; |references| Interface\n    end</code></pre> <p>The interesting design concept of this principle is that it is the reverse approach to what we would normally do.</p> <p>With the DIP in mind, we would start from the end of the project, in which our code is independent of what takes in input, and it is not susceptible to changes and out of our direct control.</p>"},{"location":"python/best_practices/code_principles/#yagni-you-aint-gonna-need-it","title":"YAGNI - You ain't gonna need it","text":"<p>It's a mantra from Extreme Programming that's often used generally in agile software teams. It's a statement that some capability we presume our software needs in the future should not be built now because \"you aren't gonna need it\". </p> <p>For additional information on this make sure to check https://www.martinfowler.com/bliki/Yagni.html</p>"},{"location":"python/best_practices/code_principles/#document-your-code","title":"Document your code","text":"<ol> <li>Don't comment bad code, rewrite it</li> <li>Readable code doesn't need comments</li> <li>Don't add noise comments</li> </ol> <ol> <li> <p>https://testdriven.io/blog/clean-code-python/\u00a0\u21a9\u21a9</p> </li> <li> <p>https://www.earthdatascience.org/courses/intro-to-earth-data-science/write-efficient-python-code/intro-to-clean-code/dry-modular-code/\u00a0\u21a9</p> </li> <li> <p>https://towardsdatascience.com/solid-coding-in-python-1281392a6a94\u00a0\u21a9</p> </li> </ol>"},{"location":"python/best_practices/styleguide/","title":"Style Guide","text":"<p>Inspired by https://google.github.io/styleguide/pyguide.html and https://phalt.github.io/django-api-domains/styleguide/.</p>"},{"location":"python/best_practices/styleguide/#introduction","title":"Introduction","text":"<p>This style guide is split into two sections, one for the project structure and another which is a list of dos and don'ts for Python programs.</p>"},{"location":"python/best_practices/styleguide/#1-project-structure","title":"1. Project structure","text":""},{"location":"python/best_practices/styleguide/#11-domains","title":"1.1 Domains","text":"<p>A domain is a piece of software that provides a distinct business value for your application. What this styleguide calls a domain is roughly an extension of what Django would call an app. Therefore a business domain should have at least one distinct software domain mirroring it.</p> <p>This guide tries to keep the key benefits of Django's app pattern - namely Django's models to represent tables in a datastore, but with an emphasis on skinny models.</p>"},{"location":"python/best_practices/styleguide/#111-domain-rules","title":"1.1.1 Domain rules","text":"<ol> <li>You should split a domain if it becomes too big to work on.</li> </ol> <p>A domain should allow between 4-6 developers (3 pairs) to comfortably work on it. If you find your developers being blocked by each other then it is time to consider splitting the domain or checking whether the software has not diverged too far from the styleguide.</p> <ol> <li>You should adhere to the styleguide patterns in this document in order to maintain strong bounded contexts between your domains.</li> </ol> <p>This applies even in situations where you extract one domain into two domains to increase velocity, but they still have to maintain a dependency between one another. We have found that if you relax the bounded context between domains, the boundary will erode and you will lose the ability to work on them independent of each other.</p>"},{"location":"python/best_practices/styleguide/#12-structure","title":"1.2 Structure","text":"<ul> <li>views.py: Public functions and access points</li> <li>serializers.py: Public functions and access points presentation logic</li> <li>interfaces.py: Integrations with other domains or external services</li> <li>models.py: Object models and storage, simple information logic</li> <li>services.py: coordination and transactional logic</li> <li>urls.py: route definition</li> <li>apps.py: Django application configuration</li> <li>migrations/*: database migrations</li> <li>management/*: Django custom commands definition</li> <li>routes/*: Django custom databse routes</li> <li>tests/*: tests</li> </ul> <p>You can mask one of the required files as a directory for better file organisation. For example, you might want to split views.py file into this structure:</p> <pre><code>views/\n  __init__.py\n  brand.py\n  legacy_brand.py\n</code></pre>"},{"location":"python/best_practices/styleguide/#121-migrations","title":"1.2.1 Migrations","text":"<p>You might notice that Django creates migrations even when nothing has changed in the model definition, these migrations should not be added to the codebase.</p> <p>Besides the initial migration, all the remaining migrations must include a name that represent the change while including the date and time of it's creation, in the following format <code>&lt;migration_number&gt;_&lt;migration_description&gt;_&lt;date&gt;_&lt;time&gt;.py</code> which as an example can be represented as <code>0004_added_status_to_sample_table_20210404_12_30.py</code>.</p>"},{"location":"python/best_practices/styleguide/#122-management","title":"1.2.2 Management","text":"<p>Following the Django documentation this folder holds the definition for the custom Django commands for a given Django app/domain. For instance, instead of a defining a traditional custom python script, a custom Django command should be added to take advantage of the already existing logic.</p>"},{"location":"python/best_practices/styleguide/#13-absolute-and-relative-imports","title":"1.3 Absolute and Relative Imports","text":"<p>The ruling for absolute or relative imports is as follows:</p> <ul> <li>When importing files within a domain, you must use relative imports.</li> <li>When importing other domains in the same project, you must use absolute imports.</li> <li>When importing domains in tests, you should use absolute imports.</li> <li>When importing third-party packages you should use absolute imports.</li> </ul> <p>TL;DR - relative imports inside a domain, absolute for everything else!</p> <p>With this ruling domains are easy to package and move around. When it comes time to move it into it's own project; tidying up imports will be one less thing you have to do.</p>"},{"location":"python/best_practices/styleguide/#2-python-language-rules","title":"2. Python Language Rules","text":""},{"location":"python/best_practices/styleguide/#21-lint","title":"2.1 Lint","text":"<p>Run <code>pylint</code> over your code using this pylintrc.</p>"},{"location":"python/best_practices/styleguide/#211-definition","title":"2.1.1 Definition","text":"<p><code>pylint</code> is a tool for finding bugs and style problems in Python source code. It finds problems that are typically caught by a compiler for less dynamic languages like C and C++. Because of the dynamic nature of Python, some warnings may be incorrect; however, spurious warnings should be fairly infrequent.</p>"},{"location":"python/best_practices/styleguide/#212-pros","title":"2.1.2 Pros","text":"<p>Catches easy-to-miss errors like typos, using-vars-before-assignment, etc.</p>"},{"location":"python/best_practices/styleguide/#213-cons","title":"2.1.3 Cons","text":"<p><code>pylint</code> isn't perfect. To take advantage of it, sometimes we'll need to write around it, suppress its warnings or fix it.</p>"},{"location":"python/best_practices/styleguide/#214-decision","title":"2.1.4 Decision","text":"<p>Make sure you run <code>pylint</code> on your code.</p> <p>Suppress warnings if they are inappropriate so that other issues are not hidden. To suppress warnings, you can set a line-level comment:</p> <pre><code>dict = 'something awful'  # Bad Idea... pylint: disable=redefined-builtin\n</code></pre> <p><code>pylint</code> warnings are each identified by symbolic name (<code>empty-docstring</code>) Google-specific warnings start with <code>g-</code>.</p> <p>If the reason for the suppression is not clear from the symbolic name, add an explanation.</p> <p>Suppressing in this way has the advantage that we can easily search for suppressions and revisit them.</p> <p>You can get a list of <code>pylint</code> warnings by doing:</p> <pre><code>pylint --list-msgs\n</code></pre> <p>To get more information on a particular message, use:</p> <pre><code>pylint --help-msg=C6409\n</code></pre> <p>Prefer <code>pylint: disable</code> to the deprecated older form <code>pylint: disable-msg</code>.</p> <p>Unused argument warnings can be suppressed by deleting the variables at the beginning of the function. Always include a comment explaining why you are deleting it. \"Unused.\" is sufficient. For example:</p> <pre><code>def viking_cafe_order(spam, beans, eggs=None):\n    del beans, eggs  # Unused by vikings.\n    return spam + spam + spam\n</code></pre> <p>Other common forms of suppressing this warning include using '<code>_</code>' as the identifier for the unused argument or prefixing the argument name with '<code>unused_</code>', or assigning them to '<code>_</code>'. These forms are allowed but no longer encouraged. These break callers that pass arguments by name and do not enforce that the arguments are actually unused.</p>"},{"location":"python/best_practices/styleguide/#22-imports","title":"2.2 Imports","text":"<p>Use <code>import</code> statements for packages and modules only, not for individual classes or functions. Note that there is an explicit exemption for imports from the typing module.</p>"},{"location":"python/best_practices/styleguide/#221-definition","title":"2.2.1 Definition","text":"<p>Reusability mechanism for sharing code from one module to another.</p>"},{"location":"python/best_practices/styleguide/#222-pros","title":"2.2.2 Pros","text":"<p>The namespace management convention is simple. The source of each identifier is indicated in a consistent way; <code>x.Obj</code> says that object <code>Obj</code> is defined in module <code>x</code>.</p>"},{"location":"python/best_practices/styleguide/#223-cons","title":"2.2.3 Cons","text":"<p>Module names can still collide. Some module names are inconveniently long.</p>"},{"location":"python/best_practices/styleguide/#224-decision","title":"2.2.4 Decision","text":"<ul> <li>Use <code>import x</code> for importing packages and modules.</li> <li>Use <code>from x import y</code> where <code>x</code> is the package prefix and <code>y</code> is the module     name with no prefix.</li> <li>Use <code>from x import y as z</code> if two modules named <code>y</code> are to be imported or if     <code>y</code> is an inconveniently long name.</li> <li>Use <code>import y as z</code> only when <code>z</code> is a standard abbreviation (e.g., <code>np</code> for     <code>numpy</code>).</li> </ul> <p>For example the module <code>sound.effects.echo</code> may be imported as follows:</p> <pre><code>from sound.effects import echo\n...\necho.EchoFilter(input, output, delay=0.7, atten=4)\n</code></pre> <p>Do not use relative names in imports. Even if the module is in the same package, use the full package name. This helps prevent unintentionally importing a package twice.</p> <p>Imports from the typing module and the six.moves module are exempt from this rule.</p>"},{"location":"python/best_practices/styleguide/#23-packages","title":"2.3 Packages","text":"<p>Import each module using the full pathname location of the module.</p>"},{"location":"python/best_practices/styleguide/#231-pros","title":"2.3.1 Pros","text":"<p>Avoids conflicts in module names or incorrect imports due to the module search path not being what the author expected. Makes it easier to find modules.</p>"},{"location":"python/best_practices/styleguide/#232-cons","title":"2.3.2 Cons","text":"<p>Makes it harder to deploy code because you have to replicate the package hierarchy. Not really a problem with modern deployment mechanisms.</p>"},{"location":"python/best_practices/styleguide/#233-decision","title":"2.3.3 Decision","text":"<p>All new code should import each module by its full package name.</p> <p>Imports should be as follows:</p> <p>Yes:</p> <pre><code># Reference absl.flags in code with the complete name (verbose).\nimport absl.flags\nfrom doctor.who import jodie\n\nFLAGS = absl.flags.FLAGS\n</code></pre> <pre><code># Reference flags in code with just the module name (common).\nfrom absl import flags\nfrom doctor.who import jodie\n\nFLAGS = flags.FLAGS\n</code></pre> <p>No: (assume this file lives in <code>doctor/who/</code> where <code>jodie.py</code> also exists)</p> <pre><code># Unclear what module the author wanted and what will be imported.  The actual\n# import behavior depends on external factors controlling sys.path.\n# Which possible jodie module did the author intend to import?\nimport jodie\n</code></pre> <p>The directory the main binary is located in should not be assumed to be in <code>sys.path</code> despite that happening in some environments. This being the case, code should assume that <code>import jodie</code> refers to a third party or top level package named <code>jodie</code>, not a local <code>jodie.py</code>.</p>"},{"location":"python/best_practices/styleguide/#24-exceptions","title":"2.4 Exceptions","text":"<p>Exceptions are allowed but must be used carefully.</p>"},{"location":"python/best_practices/styleguide/#241-definition","title":"2.4.1 Definition","text":"<p>Exceptions are a means of breaking out of normal control flow to handle errors or other exceptional conditions.</p>"},{"location":"python/best_practices/styleguide/#242-pros","title":"2.4.2 Pros","text":"<p>The control flow of normal operation code is not cluttered by error-handling code. It also allows the control flow to skip multiple frames when a certain condition occurs, e.g., returning from N nested functions in one step instead of having to plumb error codes through.</p>"},{"location":"python/best_practices/styleguide/#243-cons","title":"2.4.3 Cons","text":"<p>May cause the control flow to be confusing. Easy to miss error cases when making library calls.</p>"},{"location":"python/best_practices/styleguide/#244-decision","title":"2.4.4 Decision","text":"<p>Exceptions must follow certain conditions:</p> <ul> <li> <p>Make use of built-in exception classes when it makes sense. For example,     raise a <code>ValueError</code> to indicate a programming mistake like a violated     precondition (such as if you were passed a negative number but required a     positive one). Do not use <code>assert</code> statements for validating argument values     of a public API. <code>assert</code> is used to ensure internal correctness, not to     enforce correct usage nor to indicate that some unexpected event occurred.     If an exception is desired in the latter cases, use a raise statement. For     example:</p> <pre><code>Yes:\n  def connect_to_next_port(self, minimum):\n    \"\"\"Connects to the next available port.\n\n    Args:\n      minimum: A port value greater or equal to 1024.\n\n    Returns:\n      The new minimum port.\n\n    Raises:\n      ConnectionError: If no available port is found.\n    \"\"\"\n    if minimum &lt; 1024:\n      # Note that this raising of ValueError is not mentioned in the doc\n      # string's \"Raises:\" section because it is not appropriate to\n      # guarantee this specific behavioral reaction to API misuse.\n      raise ValueError(f'Min. port must be at least 1024, not {minimum}.')\n    port = self._find_next_open_port(minimum)\n    if not port:\n      raise ConnectionError(\n          f'Could not connect to service on port {minimum} or higher.')\n    assert port &gt;= minimum, (\n        f'Unexpected port {port} when minimum was {minimum}.')\n    return port\n</code></pre> <pre><code>No:\n  def connect_to_next_port(self, minimum):\n    \"\"\"Connects to the next available port.\n\n    Args:\n      minimum: A port value greater or equal to 1024.\n\n    Returns:\n      The new minimum port.\n    \"\"\"\n    assert minimum &gt;= 1024, 'Minimum port must be at least 1024.'\n    port = self._find_next_open_port(minimum)\n    assert port is not None\n    return port\n</code></pre> </li> <li> <p>Libraries or packages may define their own exceptions. When doing so they     must inherit from an existing exception class. Exception names should end in     <code>Error</code> and should not introduce stutter (<code>foo.FooError</code>).</p> </li> <li> <p>Never use catch-all <code>except:</code> statements, or catch <code>Exception</code> or     <code>StandardError</code>, unless you are</p> </li> <li> <p>re-raising the exception, or</p> </li> <li> <p>creating an isolation point in the program where exceptions are not         propagated but are recorded and suppressed instead, such as protecting a         thread from crashing by guarding its outermost block.</p> <p>Python is very tolerant in this regard and <code>except:</code> will really catch everything including misspelled names, sys.exit() calls, Ctrl+C interrupts, unittest failures and all kinds of other exceptions that you simply don't want to catch.</p> </li> <li> <p>Minimize the amount of code in a <code>try</code>/<code>except</code> block. The larger the body     of the <code>try</code>, the more likely that an exception will be raised by a line of     code that you didn't expect to raise an exception. In those cases, the     <code>try</code>/<code>except</code> block hides a real error.</p> </li> <li> <p>Use the <code>finally</code> clause to execute code whether or not an exception is     raised in the <code>try</code> block. This is often useful for cleanup, i.e., closing a     file.</p> </li> </ul>"},{"location":"python/best_practices/styleguide/#25-global-variables","title":"2.5 Global variables","text":"<p>Avoid global variables.</p>"},{"location":"python/best_practices/styleguide/#251-definition","title":"2.5.1 Definition","text":"<p>Variables that are declared at the module level or as class attributes.</p>"},{"location":"python/best_practices/styleguide/#252-pros","title":"2.5.2 Pros","text":"<p>Occasionally useful.</p>"},{"location":"python/best_practices/styleguide/#253-cons","title":"2.5.3 Cons","text":"<p>Has the potential to change module behavior during the import, because assignments to global variables are done when the module is first imported.</p>"},{"location":"python/best_practices/styleguide/#254-decision","title":"2.5.4 Decision","text":"<p>Avoid global variables.</p> <p>While they are technically variables, module-level constants are permitted and encouraged. For example: <code>MAX_HOLY_HANDGRENADE_COUNT = 3</code>. Constants must be named using all caps with underscores. See Naming below.</p> <p>If needed, globals should be declared at the module level and made internal to the module by prepending an <code>_</code> to the name. External access must be done through public module-level functions. See Naming below.</p>"},{"location":"python/best_practices/styleguide/#26-nestedlocalinner-classes-and-functions","title":"2.6 Nested/Local/Inner Classes and Functions","text":"<p>Nested local functions or classes are fine when used to close over a local variable. Inner classes are fine.</p>"},{"location":"python/best_practices/styleguide/#261-definition","title":"2.6.1 Definition","text":"<p>A class can be defined inside of a method, function, or class. A function can be defined inside a method or function. Nested functions have read-only access to variables defined in enclosing scopes.</p>"},{"location":"python/best_practices/styleguide/#262-pros","title":"2.6.2 Pros","text":"<p>Allows definition of utility classes and functions that are only used inside of a very limited scope. Very ADT-y. Commonly used for implementing decorators.</p>"},{"location":"python/best_practices/styleguide/#263-cons","title":"2.6.3 Cons","text":"<p>Nested functions and classes cannot be directly tested. Nesting can make the outer function longer and less readable.</p>"},{"location":"python/best_practices/styleguide/#264-decision","title":"2.6.4 Decision","text":"<p>They are fine with some caveats. Avoid nested functions or classes except when closing over a local value. Do not nest a function just to hide it from users of a module. Instead, prefix its name with an _ at the module level so that it can still be accessed by tests.</p>"},{"location":"python/best_practices/styleguide/#27-comprehensions-generator-expressions","title":"2.7 Comprehensions &amp; Generator Expressions","text":"<p>Okay to use for simple cases.</p>"},{"location":"python/best_practices/styleguide/#271-definition","title":"2.7.1 Definition","text":"<p>List, Dict, and Set comprehensions as well as generator expressions provide a concise and efficient way to create container types and iterators without resorting to the use of traditional loops, <code>map()</code>, <code>filter()</code>, or <code>lambda</code>.</p>"},{"location":"python/best_practices/styleguide/#272-pros","title":"2.7.2 Pros","text":"<p>Simple comprehensions can be clearer and simpler than other dict, list, or set creation techniques. Generator expressions can be very efficient, since they avoid the creation of a list entirely.</p>"},{"location":"python/best_practices/styleguide/#273-cons","title":"2.7.3 Cons","text":"<p>Complicated comprehensions or generator expressions can be hard to read.</p>"},{"location":"python/best_practices/styleguide/#274-decision","title":"2.7.4 Decision","text":"<p>Okay to use for simple cases. Each portion must fit on one line: mapping expression, <code>for</code> clause, filter expression. Multiple <code>for</code> clauses or filter expressions are not permitted. Use loops instead when things get more complicated.</p> <pre><code>Yes:\n  result = [mapping_expr for value in iterable if filter_expr]\n\n  result = [{'key': value} for value in iterable\n            if a_long_filter_expression(value)]\n\n  result = [complicated_transform(x)\n            for x in iterable if predicate(x)]\n\n  descriptive_name = [\n      transform({'key': key, 'value': value}, color='black')\n      for key, value in generate_iterable(some_input)\n      if complicated_condition_is_met(key, value)\n  ]\n\n  result = []\n  for x in range(10):\n      for y in range(5):\n          if x * y &gt; 10:\n              result.append((x, y))\n\n  return {x: complicated_transform(x)\n          for x in long_generator_function(parameter)\n          if x is not None}\n\n  squares_generator = (x**2 for x in range(10))\n\n  unique_names = {user.name for user in users if user is not None}\n\n  eat(jelly_bean for jelly_bean in jelly_beans\n      if jelly_bean.color == 'black')\n</code></pre> <pre><code>No:\n  result = [complicated_transform(\n                x, some_argument=x+1)\n            for x in iterable if predicate(x)]\n\n  result = [(x, y) for x in range(10) for y in range(5) if x * y &gt; 10]\n\n  return ((x, y, z)\n          for x in range(5)\n          for y in range(5)\n          if x != y\n          for z in range(5)\n          if y != z)\n</code></pre>"},{"location":"python/best_practices/styleguide/#28-default-iterators-and-operators","title":"2.8 Default Iterators and Operators","text":"<p>Use default iterators and operators for types that support them, like lists, dictionaries, and files.</p>"},{"location":"python/best_practices/styleguide/#281-definition","title":"2.8.1 Definition","text":"<p>Container types, like dictionaries and lists, define default iterators and membership test operators (\"in\" and \"not in\").</p>"},{"location":"python/best_practices/styleguide/#282-pros","title":"2.8.2 Pros","text":"<p>The default iterators and operators are simple and efficient. They express the operation directly, without extra method calls. A function that uses default operators is generic. It can be used with any type that supports the operation.</p>"},{"location":"python/best_practices/styleguide/#283-cons","title":"2.8.3 Cons","text":"<p>You can't tell the type of objects by reading the method names (e.g. <code>has_key()</code> means a dictionary). This is also an advantage.</p>"},{"location":"python/best_practices/styleguide/#284-decision","title":"2.8.4 Decision","text":"<p>Use default iterators and operators for types that support them, like lists, dictionaries, and files. The built-in types define iterator methods, too. Prefer these methods to methods that return lists, except that you should not mutate a container while iterating over it.</p> <pre><code>Yes:  for key in adict: ...\n      if key not in adict: ...\n      if obj in alist: ...\n      for line in afile: ...\n      for k, v in adict.items(): ...\n      for k, v in six.iteritems(adict): ...\n</code></pre> <pre><code>No:   for key in adict.keys(): ...\n      if not adict.has_key(key): ...\n      for line in afile.readlines(): ...\n      for k, v in dict.iteritems(): ...\n</code></pre>"},{"location":"python/best_practices/styleguide/#29-generators","title":"2.9 Generators","text":"<p>Use generators as needed.</p>"},{"location":"python/best_practices/styleguide/#29-definition","title":"2.9 Definition","text":"<p>A generator function returns an iterator that yields a value each time it executes a yield statement. After it yields a value, the runtime state of the generator function is suspended until the next value is needed.</p>"},{"location":"python/best_practices/styleguide/#292-pros","title":"2.9.2 Pros","text":"<p>Simpler code, because the state of local variables and control flow are preserved for each call. A generator uses less memory than a function that creates an entire list of values at once.</p>"},{"location":"python/best_practices/styleguide/#293-cons","title":"2.9.3 Cons","text":"<p>None.</p>"},{"location":"python/best_practices/styleguide/#294-decision","title":"2.9.4 Decision","text":"<p>Fine. Use \"Yields:\" rather than \"Returns:\" in the docstring for generator functions.</p>"},{"location":"python/best_practices/styleguide/#210-lambda-functions","title":"2.10 Lambda Functions","text":"<p>Okay for one-liners. Prefer generator expressions over <code>map()</code> or <code>filter()</code> with a <code>lambda</code>.</p>"},{"location":"python/best_practices/styleguide/#2101-definition","title":"2.10.1 Definition","text":"<p>Lambdas define anonymous functions in an expression, as opposed to a statement.</p>"},{"location":"python/best_practices/styleguide/#2102-pros","title":"2.10.2 Pros","text":"<p>Convenient.</p>"},{"location":"python/best_practices/styleguide/#2103-cons","title":"2.10.3 Cons","text":"<p>Harder to read and debug than local functions. The lack of names means stack traces are more difficult to understand. Expressiveness is limited because the function may only contain an expression.</p>"},{"location":"python/best_practices/styleguide/#2104-decision","title":"2.10.4 Decision","text":"<p>Okay to use them for one-liners. If the code inside the lambda function is longer than 60-80 chars, it's probably better to define it as a regular nested function.</p> <p>For common operations like multiplication, use the functions from the <code>operator</code> module instead of lambda functions. For example, prefer <code>operator.mul</code> to <code>lambda x, y: x * y</code>.</p>"},{"location":"python/best_practices/styleguide/#211-conditional-expressions","title":"2.11 Conditional Expressions","text":"<p>Okay for simple cases.</p>"},{"location":"python/best_practices/styleguide/#2111-definition","title":"2.11.1 Definition","text":"<p>Conditional expressions (sometimes called a \u201cternary operator\u201d) are mechanisms that provide a shorter syntax for if statements. For example: <code>x = 1 if cond else 2</code>.</p>"},{"location":"python/best_practices/styleguide/#2112-pros","title":"2.11.2 Pros","text":"<p>Shorter and more convenient than an if statement.</p>"},{"location":"python/best_practices/styleguide/#2113-cons","title":"2.11.3 Cons","text":"<p>May be harder to read than an if statement. The condition may be difficult to locate if the expression is long.</p>"},{"location":"python/best_practices/styleguide/#2114-decision","title":"2.11.4 Decision","text":"<p>Okay to use for simple cases. Each portion must fit on one line: true-expression, if-expression, else-expression. Use a complete if statement when things get more complicated.</p> <pre><code>Yes:\n    one_line = 'yes' if predicate(value) else 'no'\n    slightly_split = ('yes' if predicate(value)\n                      else 'no, nein, nyet')\n    the_longest_ternary_style_that_can_be_done = (\n        'yes, true, affirmative, confirmed, correct'\n        if predicate(value)\n        else 'no, false, negative, nay')\n</code></pre> <pre><code>No:\n    bad_line_breaking = ('yes' if predicate(value) else\n                         'no')\n    portion_too_long = ('yes'\n                        if some_long_module.some_long_predicate_function(\n                            really_long_variable_name)\n                        else 'no, false, negative, nay')\n</code></pre>"},{"location":"python/best_practices/styleguide/#212-default-argument-values","title":"2.12 Default Argument Values","text":"<p>Okay in most cases.</p>"},{"location":"python/best_practices/styleguide/#2121-definition","title":"2.12.1 Definition","text":"<p>You can specify values for variables at the end of a function's parameter list, e.g., <code>def foo(a, b=0):</code>. If <code>foo</code> is called with only one argument, <code>b</code> is set to 0. If it is called with two arguments, <code>b</code> has the value of the second argument.</p>"},{"location":"python/best_practices/styleguide/#2122-pros","title":"2.12.2 Pros","text":"<p>Often you have a function that uses lots of default values, but on rare occasions you want to override the defaults. Default argument values provide an easy way to do this, without having to define lots of functions for the rare exceptions. As Python does not support overloaded methods/functions, default arguments are an easy way of \"faking\" the overloading behavior.</p>"},{"location":"python/best_practices/styleguide/#2123-cons","title":"2.12.3 Cons","text":"<p>Default arguments are evaluated once at module load time. This may cause problems if the argument is a mutable object such as a list or a dictionary. If the function modifies the object (e.g., by appending an item to a list), the default value is modified.</p>"},{"location":"python/best_practices/styleguide/#2124-decision","title":"2.12.4 Decision","text":"<p>Okay to use with the following caveat:</p> <p>Do not use mutable objects as default values in the function or method definition.</p> <pre><code>Yes: def foo(a, b=None):\n         if b is None:\n             b = []\nYes: def foo(a, b: Optional[Sequence] = None):\n         if b is None:\n             b = []\nYes: def foo(a, b: Sequence = ()):  # Empty tuple OK since tuples are immutable\n         ...\n</code></pre> <pre><code>No:  def foo(a, b=[]):\n         ...\nNo:  def foo(a, b=time.time()):  # The time the module was loaded???\n         ...\nNo:  def foo(a, b=FLAGS.my_thing):  # sys.argv has not yet been parsed...\n         ...\nNo:  def foo(a, b: Mapping = {}):  # Could still get passed to unchecked code\n         ...\n</code></pre>"},{"location":"python/best_practices/styleguide/#213-properties","title":"2.13 Properties","text":"<p>Use properties for accessing or setting data where you would normally have used simple, lightweight accessor or setter methods.</p>"},{"location":"python/best_practices/styleguide/#2131-definition","title":"2.13.1 Definition","text":"<p>A way to wrap method calls for getting and setting an attribute as a standard attribute access when the computation is lightweight.</p>"},{"location":"python/best_practices/styleguide/#2132-pros","title":"2.13.2 Pros","text":"<p>Readability is increased by eliminating explicit get and set method calls for simple attribute access. Allows calculations to be lazy. Considered the Pythonic way to maintain the interface of a class. In terms of performance, allowing properties bypasses needing trivial accessor methods when a direct variable access is reasonable. This also allows accessor methods to be added in the future without breaking the interface.</p>"},{"location":"python/best_practices/styleguide/#2133-cons","title":"2.13.3 Cons","text":"<p>Can hide side-effects much like operator overloading. Can be confusing for subclasses.</p>"},{"location":"python/best_practices/styleguide/#2134-decision","title":"2.13.4 Decision","text":"<p>Use properties in new code to access or set data where you would normally have used lightweight accessor or setter methods. Properties should be created with the <code>@property</code> decorator.</p> <p>Inheritance with properties can be non-obvious if the property itself is not overridden. Thus one must make sure that accessor methods are called indirectly to ensure methods overridden in subclasses are called by the property (using the template method design pattern).</p> <pre><code>Yes: import math\n\n     class Square:\n         \"\"\"A square with two properties: a writable area and a read-only perimeter.\n\n         To use:\n         &gt;&gt;&gt; sq = Square(3)\n         &gt;&gt;&gt; sq.area\n         9\n         &gt;&gt;&gt; sq.perimeter\n         12\n         &gt;&gt;&gt; sq.area = 16\n         &gt;&gt;&gt; sq.side\n         4\n         &gt;&gt;&gt; sq.perimeter\n         16\n         \"\"\"\n\n         def __init__(self, side):\n             self.side = side\n\n         @property\n         def area(self):\n             \"\"\"Area of the square.\"\"\"\n             return self._get_area()\n\n         @area.setter\n         def area(self, area):\n             return self._set_area(area)\n\n         def _get_area(self):\n             \"\"\"Indirect accessor to calculate the 'area' property.\"\"\"\n             return self.side ** 2\n\n         def _set_area(self, area):\n             \"\"\"Indirect setter to set the 'area' property.\"\"\"\n             self.side = math.sqrt(area)\n\n         @property\n         def perimeter(self):\n             return self.side * 4\n</code></pre>"},{"location":"python/best_practices/styleguide/#214-truefalse-evaluations","title":"2.14 True/False Evaluations","text":"<p>Use the \"implicit\" false if at all possible.</p>"},{"location":"python/best_practices/styleguide/#2141-definition","title":"2.14.1 Definition","text":"<p>Python evaluates certain values as <code>False</code> when in a boolean context. A quick \"rule of thumb\" is that all \"empty\" values are considered false, so <code>0, None, [], {}, ''</code> all evaluate as false in a boolean context.</p>"},{"location":"python/best_practices/styleguide/#2142-pros","title":"2.14.2 Pros","text":"<p>Conditions using Python booleans are easier to read and less error-prone. In most cases, they're also faster.</p>"},{"location":"python/best_practices/styleguide/#2143-cons","title":"2.14.3 Cons","text":"<p>May look strange to C/C++ developers.</p>"},{"location":"python/best_practices/styleguide/#2144-decision","title":"2.14.4 Decision","text":"<p>Use the \"implicit\" false if possible, e.g., <code>if foo:</code> rather than <code>if foo != []:</code>. There are a few caveats that you should keep in mind though:</p> <ul> <li> <p>Always use <code>if foo is None:</code> (or <code>is not None</code>) to check for a <code>None</code> value.     E.g., when testing whether a variable or argument that defaults to <code>None</code>     was set to some other value. The other value might be a value that's false     in a boolean context!</p> </li> <li> <p>Never compare a boolean variable to <code>False</code> using <code>==</code>. Use <code>if not x:</code>     instead. If you need to distinguish <code>False</code> from <code>None</code> then chain the     expressions, such as <code>if not x and x is not None:</code>.</p> </li> <li> <p>For sequences (strings, lists, tuples), use the fact that empty sequences     are false, so <code>if seq:</code> and <code>if not seq:</code> are preferable to <code>if len(seq):</code>     and <code>if not len(seq):</code> respectively.</p> </li> <li> <p>When handling integers, implicit false may involve more risk than benefit     (i.e., accidentally handling <code>None</code> as 0). You may compare a value which is     known to be an integer (and is not the result of <code>len()</code>) against the     integer 0.</p> <pre><code>Yes: if not users:\n         print('no users')\n\n     if foo == 0:\n         self.handle_zero()\n\n     if i % 10 == 0:\n         self.handle_multiple_of_ten()\n\n     def f(x=None):\n         if x is None:\n             x = []\n</code></pre> <pre><code>No:  if len(users) == 0:\n         print('no users')\n\n     if foo is not None and not foo:\n         self.handle_zero()\n\n     if not i % 10:\n         self.handle_multiple_of_ten()\n\n     def f(x=None):\n         x = x or []\n</code></pre> </li> <li> <p>Note that <code>'0'</code> (i.e., <code>0</code> as string) evaluates to true.</p> </li> </ul>"},{"location":"python/best_practices/styleguide/#216-lexical-scoping","title":"2.16 Lexical Scoping","text":"<p>Okay to use.</p>"},{"location":"python/best_practices/styleguide/#2161-definition","title":"2.16.1 Definition","text":"<p>A nested Python function can refer to variables defined in enclosing functions, but cannot assign to them. Variable bindings are resolved using lexical scoping, that is, based on the static program text. Any assignment to a name in a block will cause Python to treat all references to that name as a local variable, even if the use precedes the assignment. If a global declaration occurs, the name is treated as a global variable.</p> <p>An example of the use of this feature is:</p> <pre><code>def get_adder(summand1):\n    \"\"\"Returns a function that adds numbers to a given number.\"\"\"\n    def adder(summand2):\n        return summand1 + summand2\n\n    return adder\n</code></pre>"},{"location":"python/best_practices/styleguide/#2162-pros","title":"2.16.2 Pros","text":"<p>Often results in clearer, more elegant code. Especially comforting to experienced Lisp and Scheme (and Haskell and ML and ...) programmers.</p>"},{"location":"python/best_practices/styleguide/#2163-cons","title":"2.16.3 Cons","text":"<p>Can lead to confusing bugs. Such as this example based on PEP-0227:</p> <pre><code>i = 4\ndef foo(x):\n    def bar():\n        print(i, end='')\n    # ...\n    # A bunch of code here\n    # ...\n    for i in x:  # Ah, i *is* local to foo, so this is what bar sees\n        print(i, end='')\n    bar()\n</code></pre> <p>So <code>foo([1, 2, 3])</code> will print <code>1 2 3 3</code>, not <code>1 2 3 4</code>.</p>"},{"location":"python/best_practices/styleguide/#2164-decision","title":"2.16.4 Decision","text":"<p>Okay to use.</p>"},{"location":"python/best_practices/styleguide/#217-function-and-method-decorators","title":"2.17 Function and Method Decorators","text":"<p>Use decorators judiciously when there is a clear advantage. Avoid <code>staticmethod</code> and limit use of <code>classmethod</code>.</p>"},{"location":"python/best_practices/styleguide/#2171-definition","title":"2.17.1 Definition","text":"<p>Decorators for Functions and Methods (a.k.a \"the <code>@</code> notation\"). One common decorator is <code>@property</code>, used for converting ordinary methods into dynamically computed attributes. However, the decorator syntax allows for user-defined decorators as well. Specifically, for some function <code>my_decorator</code>, this:</p> <pre><code>class C:\n    @my_decorator\n    def method(self):\n        # method body ...\n</code></pre> <p>is equivalent to:</p> <pre><code>class C:\n    def method(self):\n        # method body ...\n    method = my_decorator(method)\n</code></pre>"},{"location":"python/best_practices/styleguide/#2172-pros","title":"2.17.2 Pros","text":"<p>Elegantly specifies some transformation on a method; the transformation might eliminate some repetitive code, enforce invariants, etc.</p>"},{"location":"python/best_practices/styleguide/#2173-cons","title":"2.17.3 Cons","text":"<p>Decorators can perform arbitrary operations on a function's arguments or return values, resulting in surprising implicit behavior. Additionally, decorators execute at import time. Failures in decorator code are pretty much impossible to recover from.</p>"},{"location":"python/best_practices/styleguide/#2174-decision","title":"2.17.4 Decision","text":"<p>Use decorators judiciously when there is a clear advantage. Decorators should follow the same import and naming guidelines as functions. Decorator pydoc should clearly state that the function is a decorator. Write unit tests for decorators.</p> <p>Avoid external dependencies in the decorator itself (e.g. don't rely on files, sockets, database connections, etc.), since they might not be available when the decorator runs (at import time, perhaps from <code>pydoc</code> or other tools). A decorator that is called with valid parameters should (as much as possible) be guaranteed to succeed in all cases.</p> <p>Decorators are a special case of \"top level code\" - see main for more discussion.</p> <p>Never use <code>staticmethod</code> unless forced to in order to integrate with an API defined in an existing library. Write a module level function instead.</p> <p>Use <code>classmethod</code> only when writing a named constructor or a class-specific routine that modifies necessary global state such as a process-wide cache.</p>"},{"location":"python/best_practices/styleguide/#218-threading","title":"2.18 Threading","text":"<p>Do not rely on the atomicity of built-in types.</p> <p>While Python's built-in data types such as dictionaries appear to have atomic operations, there are corner cases where they aren't atomic (e.g. if <code>__hash__</code> or <code>__eq__</code> are implemented as Python methods) and their atomicity should not be relied upon. Neither should you rely on atomic variable assignment (since this in turn depends on dictionaries).</p> <p>Use the Queue module's <code>Queue</code> data type as the preferred way to communicate data between threads. Otherwise, use the threading module and its locking primitives. Prefer condition variables and <code>threading.Condition</code> instead of using lower-level locks.</p>"},{"location":"python/best_practices/styleguide/#219-power-features","title":"2.19 Power Features","text":"<p>Avoid these features.</p>"},{"location":"python/best_practices/styleguide/#2191-definition","title":"2.19.1 Definition","text":"<p>Python is an extremely flexible language and gives you many fancy features such as custom metaclasses, access to bytecode, on-the-fly compilation, dynamic inheritance, object reparenting, import hacks, reflection (e.g. some uses of <code>getattr()</code>), modification of system internals, etc.</p>"},{"location":"python/best_practices/styleguide/#2192-pros","title":"2.19.2 Pros","text":"<p>These are powerful language features. They can make your code more compact.</p>"},{"location":"python/best_practices/styleguide/#2193-cons","title":"2.19.3 Cons","text":"<p>It's very tempting to use these \"cool\" features when they're not absolutely necessary. It's harder to read, understand, and debug code that's using unusual features underneath. It doesn't seem that way at first (to the original author), but when revisiting the code, it tends to be more difficult than code that is longer but is straightforward.</p>"},{"location":"python/best_practices/styleguide/#2194-decision","title":"2.19.4 Decision","text":"<p>Avoid these features in your code.</p> <p>Standard library modules and classes that internally use these features are okay to use (for example, <code>abc.ABCMeta</code>, <code>dataclasses</code>, and <code>enum</code>).</p>"},{"location":"python/best_practices/styleguide/#220-modern-python-python-3-and-from-__future__-imports","title":"2.20 Modern Python: Python 3 and from __future__ imports","text":"<p>Python 3 is here! While not every project is ready to use it yet, all code should be written to be 3 compatible (and tested under 3 when possible).</p>"},{"location":"python/best_practices/styleguide/#2201-definition","title":"2.20.1 Definition","text":"<p>Python 3 is a significant change in the Python language. While existing code is often written with 2.7 in mind, there are some simple things to do to make code more explicit about its intentions and thus better prepared for use under Python 3 without modification.</p>"},{"location":"python/best_practices/styleguide/#2202-pros","title":"2.20.2 Pros","text":"<p>Code written with Python 3 in mind is more explicit and easier to get running under Python 3 once all of the dependencies of your project are ready.</p>"},{"location":"python/best_practices/styleguide/#2203-cons","title":"2.20.3 Cons","text":"<p>Some people find the additional boilerplate to be ugly. It's unusual to add imports to a module that doesn't actually require the features added by the import.</p>"},{"location":"python/best_practices/styleguide/#2204-decision","title":"2.20.4 Decision","text":""},{"location":"python/best_practices/styleguide/#from-__future__-imports","title":"from __future__ imports","text":"<p>Use of <code>from __future__ import</code> statements is encouraged. All new code should contain the following and existing code should be updated to be compatible when possible:</p> <pre><code>from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n</code></pre> <p>For more information on these imports, see absolute imports, <code>/</code> division behavior, and the <code>print</code> function.</p> <p>Please don't omit or remove these imports, even if they're not currently used in the module, unless the code is Python 3 only. It is better to always have the future imports in all files so that they are not forgotten during later edits when someone starts using such a feature.</p> <p>There are other <code>from __future__</code> import statements. Use them as you see fit. We do not include <code>unicode_literals</code> in our recommendations as it is not a clear win due to implicit default codec conversion consequences it introduces in many places within Python 2.7. Most code is better off with explicit use of <code>b''</code> and <code>u''</code> bytes and unicode string literals as necessary.</p>"},{"location":"python/best_practices/styleguide/#the-six-future-and-past-libraries","title":"The six, future, and past libraries","text":"<p>When your project needs to actively support use under both Python 2 and 3, use the six, future, and past libraries as you see fit. They exist to make your code cleaner and life easier.</p>"},{"location":"python/best_practices/styleguide/#221-type-annotated-code","title":"2.21 Type Annotated Code","text":"<p>You can annotate Python 3 code with type hints according to PEP-484, and type-check the code at build time with a type checking tool like pytype.</p> <p>Type annotations can be in the source or in a stub pyi file. Whenever possible, annotations should be in the source. Use pyi files for third-party or extension modules.</p>"},{"location":"python/best_practices/styleguide/#2211-definition","title":"2.21.1 Definition","text":"<p>Type annotations (or \"type hints\") are for function or method arguments and return values:</p> <pre><code>def func(a: int) -&gt; List[int]:\n</code></pre> <p>You can also declare the type of a variable using similar PEP-526 syntax:</p> <pre><code>a: SomeType = some_func()\n</code></pre> <p>Or by using a type comment in code that must support legacy Python versions.</p> <pre><code>a = some_func()  # type: SomeType\n</code></pre>"},{"location":"python/best_practices/styleguide/#2212-pros","title":"2.21.2 Pros","text":"<p>Type annotations improve the readability and maintainability of your code. The type checker will convert many runtime errors to build-time errors, and reduce your ability to use Power Features.</p>"},{"location":"python/best_practices/styleguide/#2213-cons","title":"2.21.3 Cons","text":"<p>You will have to keep the type declarations up to date. You might see type errors that you think are valid code. Use of a type checker may reduce your ability to use Power Features.</p>"},{"location":"python/best_practices/styleguide/#2214-decision","title":"2.21.4 Decision","text":"<p>You are strongly encouraged to enable Python type analysis when updating code. When adding or modifying public APIs, include type annotations and enable checking via pytype in the build system. As static analysis is relatively new to Python, we acknowledge that undesired side-effects (such as wrongly inferred types) may prevent adoption by some projects. In those situations, authors are encouraged to add a comment with a TODO or link to a bug describing the issue(s) currently preventing type annotation adoption in the BUILD file or in the code itself as appropriate.</p>"},{"location":"python/best_practices/styleguide/#3-python-style-rules","title":"3 Python Style Rules","text":""},{"location":"python/best_practices/styleguide/#31-semicolons","title":"3.1 Semicolons","text":"<p>Do not terminate your lines with semicolons, and do not use semicolons to put two statements on the same line.</p>"},{"location":"python/best_practices/styleguide/#32-line-length","title":"3.2 Line length","text":"<p>Maximum line length is 120 characters.</p> <p>Explicit exceptions to the 120 character limit:</p> <ul> <li>Long import statements.</li> <li>URLs, pathnames, or long flags in comments.</li> <li>Long string module level constants not containing whitespace that would be     inconvenient to split across lines such as URLs or pathnames.</li> <li>Pylint disable comments. (e.g.: <code># pylint: disable=invalid-name</code>)</li> </ul> <p>Do not use backslash line continuation except for <code>with</code> statements requiring three or more context managers.</p> <p>Make use of Python's implicit line joining inside parentheses, brackets and braces. If necessary, you can add an extra pair of parentheses around an expression.</p> <pre><code>Yes: foo_bar(self, width, height, color='black', design=None, x='foo',\n             emphasis=None, highlight=0)\n\n     if (width == 0 and height == 0 and\n         color == 'red' and emphasis == 'strong'):\n</code></pre> <p>When a literal string won't fit on a single line, use parentheses for implicit line joining.</p> <pre><code>x = ('This will build a very long long '\n     'long long long long long long string')\n</code></pre> <p>Within comments, put long URLs on their own line if necessary.</p> <pre><code>Yes:  # See details at\n      # http://www.example.com/us/developer/documentation/api/content/v2.0/csv_file_name_extension_full_specification.html\n</code></pre> <pre><code>No:  # See details at\n     # http://www.example.com/us/developer/documentation/api/content/\\\n     # v2.0/csv_file_name_extension_full_specification.html\n</code></pre> <p>It is permissible to use backslash continuation when defining a <code>with</code> statement whose expressions span three or more lines. For two lines of expressions, use a nested <code>with</code> statement:</p> <pre><code>Yes:  with very_long_first_expression_function() as spam, \\\n           very_long_second_expression_function() as beans, \\\n           third_thing() as eggs:\n          place_order(eggs, beans, spam, beans)\n</code></pre> <pre><code>No:  with VeryLongFirstExpressionFunction() as spam, \\\n          VeryLongSecondExpressionFunction() as beans:\n       PlaceOrder(eggs, beans, spam, beans)\n</code></pre> <pre><code>Yes:  with very_long_first_expression_function() as spam:\n          with very_long_second_expression_function() as beans:\n              place_order(beans, spam)\n</code></pre> <p>Make note of the indentation of the elements in the line continuation examples above; see the indentation section for explanation.</p> <p>In all other cases where a line exceeds 80 characters, and the yapf auto-formatter does not help bring the line below the limit, the line is allowed to exceed this maximum.</p>"},{"location":"python/best_practices/styleguide/#33-parentheses","title":"3.3 Parentheses","text":"<p>Use parentheses sparingly.</p> <p>It is fine, though not required, to use parentheses around tuples. Do not use them in return statements or conditional statements unless using parentheses for implied line continuation or to indicate a tuple.</p> <pre><code>Yes: if foo:\n         bar()\n     while x:\n         x = bar()\n     if x and y:\n         bar()\n     if not x:\n         bar()\n     # For a 1 item tuple the ()s are more visually obvious than the comma.\n     onesie = (foo,)\n     return foo\n     return spam, beans\n     return (spam, beans)\n     for (x, y) in dict.items(): ...\n</code></pre> <pre><code>No:  if (x):\n         bar()\n     if not(x):\n         bar()\n     return (foo)\n</code></pre>"},{"location":"python/best_practices/styleguide/#34-indentation","title":"3.4 Indentation","text":"<p>Indent your code blocks with 4 spaces.</p> <p>Never use tabs or mix tabs and spaces. In cases of implied line continuation, you should align wrapped elements either vertically, as per the examples in the line length section; or using a hanging indent of 4 spaces, in which case there should be nothing after the open parenthesis or bracket on the first line.</p> <pre><code>Yes:   # Aligned with opening delimiter\n       foo = long_function_name(var_one, var_two,\n                                var_three, var_four)\n       meal = (spam,\n               beans)\n\n       # Aligned with opening delimiter in a dictionary\n       foo = {\n           long_dictionary_key: value1 +\n                                value2,\n           ...\n       }\n\n       # 4-space hanging indent; nothing on first line\n       foo = long_function_name(\n           var_one, var_two, var_three,\n           var_four)\n       meal = (\n           spam,\n           beans)\n\n       # 4-space hanging indent in a dictionary\n       foo = {\n           long_dictionary_key:\n               long_dictionary_value,\n           ...\n       }\n</code></pre> <pre><code>No:    # Stuff on first line forbidden\n       foo = long_function_name(var_one, var_two,\n           var_three, var_four)\n       meal = (spam,\n           beans)\n\n       # 2-space hanging indent forbidden\n       foo = long_function_name(\n         var_one, var_two, var_three,\n         var_four)\n\n       # No hanging indent in a dictionary\n       foo = {\n           long_dictionary_key:\n           long_dictionary_value,\n           ...\n       }\n</code></pre>"},{"location":"python/best_practices/styleguide/#341-trailing-commas-in-sequences-of-items","title":"3.4.1 Trailing commas in sequences of items?","text":"<p>Trailing commas in sequences of items are recommended only when the closing container token <code>]</code>, <code>)</code>, or <code>}</code> does not appear on the same line as the final element. The presence of a trailing comma is also used as a hint to our Python code auto-formatter YAPF to direct it to auto-format the container of items to one item per line when the <code>,</code> after the final element is present.</p> <pre><code>Yes:   golomb3 = [0, 1, 3]\nYes:   golomb4 = [\n           0,\n           1,\n           4,\n           6,\n       ]\n</code></pre> <pre><code>No:    golomb4 = [\n           0,\n           1,\n           4,\n           6\n       ]\n</code></pre>"},{"location":"python/best_practices/styleguide/#35-blank-lines","title":"3.5 Blank Lines","text":"<p>Two blank lines between top-level definitions, be they function or class definitions. One blank line between method definitions and between the <code>class</code> line and the first method. No blank line following a <code>def</code> line. Use single blank lines as you judge appropriate within functions or methods.</p>"},{"location":"python/best_practices/styleguide/#36-whitespace","title":"3.6 Whitespace","text":"<p>Follow standard typographic rules for the use of spaces around punctuation.</p> <p>No whitespace inside parentheses, brackets or braces.</p> <pre><code>Yes: spam(ham[1], {eggs: 2}, [])\n</code></pre> <pre><code>No:  spam( ham[ 1 ], { eggs: 2 }, [ ] )\n</code></pre> <p>No whitespace before a comma, semicolon, or colon. Do use whitespace after a comma, semicolon, or colon, except at the end of the line.</p> <pre><code>Yes: if x == 4:\n         print(x, y)\n     x, y = y, x\n</code></pre> <pre><code>No:  if x == 4 :\n         print(x , y)\n     x , y = y , x\n</code></pre> <p>No whitespace before the open paren/bracket that starts an argument list, indexing or slicing.</p> <pre><code>Yes: spam(1)\n</code></pre> <pre><code>No:  spam (1)\n</code></pre> <pre><code>Yes: dict['key'] = list[index]\n</code></pre> <pre><code>No:  dict ['key'] = list [index]\n</code></pre> <p>No trailing whitespace.</p> <p>Surround binary operators with a single space on either side for assignment (<code>=</code>), comparisons (<code>==, &lt;, &gt;, !=, &lt;&gt;, &lt;=, &gt;=, in, not in, is, is not</code>), and Booleans (<code>and, or, not</code>). Use your better judgment for the insertion of spaces around arithmetic operators (<code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, <code>//</code>, <code>%</code>, <code>**</code>, <code>@</code>).</p> <pre><code>Yes: x == 1\n</code></pre> <pre><code>No:  x&lt;1\n</code></pre> <p>Never use spaces around <code>=</code> when passing keyword arguments or defining a default parameter value, with one exception: when a type annotation is present, do use spaces around the <code>=</code> for the default parameter value.</p> <pre><code>Yes: def complex(real, imag=0.0): return Magic(r=real, i=imag)\nYes: def complex(real, imag: float = 0.0): return Magic(r=real, i=imag)\n</code></pre> <pre><code>No:  def complex(real, imag = 0.0): return Magic(r = real, i = imag)\nNo:  def complex(real, imag: float=0.0): return Magic(r = real, i = imag)\n</code></pre> <p>Don't use spaces to vertically align tokens on consecutive lines, since it becomes a maintenance burden (applies to <code>:</code>, <code>#</code>, <code>=</code>, etc.):</p> <pre><code>Yes:\n  foo = 1000  # comment\n  long_name = 2  # comment that should not be aligned\n\n  dictionary = {\n      'foo': 1,\n      'long_name': 2,\n  }\n</code></pre> <pre><code>No:\n  foo       = 1000  # comment\n  long_name = 2     # comment that should not be aligned\n\n  dictionary = {\n      'foo'      : 1,\n      'long_name': 2,\n  }\n</code></pre>"},{"location":"python/best_practices/styleguide/#37-shebang-line","title":"3.7 Shebang Line","text":"<p>Most <code>.py</code> files do not need to start with a <code>#!</code> line. Start the main file of a program with <code>#!/usr/bin/env python3</code> (to support virtualenvs) or <code>#!/usr/bin/python3</code> per PEP-394.</p> <p>This line is used by the kernel to find the Python interpreter, but is ignored by Python when importing modules. It is only necessary on a file intended to be executed directly.</p>"},{"location":"python/best_practices/styleguide/#38-comments-and-docstrings","title":"3.8 Comments and Docstrings","text":"<p>Be sure to use the right style for module, function, method docstrings and inline comments.</p>"},{"location":"python/best_practices/styleguide/#381-docstrings","title":"3.8.1 Docstrings","text":"<p>Python uses docstrings to document code. A docstring is a string that is the first statement in a package, module, class or function. These strings can be extracted automatically through the <code>__doc__</code> member of the object and are used by <code>pydoc</code>. (Try running <code>pydoc</code> on your module to see how it looks.) Always use the three double-quote <code>\"\"\"</code> format for docstrings (per PEP 257). A docstring should be organized as a summary line (one physical line not exceeding 80 characters) terminated by a period, question mark, or exclamation point. When writing more (encouraged), this must be followed by a blank line, followed by the rest of the docstring starting at the same cursor position as the first quote of the first line. There are more formatting guidelines for docstrings below.</p>"},{"location":"python/best_practices/styleguide/#382-modules","title":"3.8.2 Modules","text":"<p>Every file should contain license boilerplate. Choose the appropriate boilerplate for the license used by the project (for example, Apache 2.0, BSD, LGPL, GPL)</p> <p>Files should start with a docstring describing the contents and usage of the module.</p> <pre><code>\"\"\"A one line summary of the module or program, terminated by a period.\n\nLeave one blank line.  The rest of this docstring should contain an\noverall description of the module or program.  Optionally, it may also\ncontain a brief description of exported classes and functions and/or usage\nexamples.\n\n  Typical usage example:\n\n  foo = ClassFoo()\n  bar = foo.FunctionBar()\n\"\"\"\n</code></pre>"},{"location":"python/best_practices/styleguide/#383-functions-and-methods","title":"3.8.3 Functions and Methods","text":"<p>In this section, \"function\" means a method, function, or generator.</p> <p>A function must have a docstring, unless it meets all of the following criteria:</p> <ul> <li>not externally visible</li> <li>very short</li> <li>obvious</li> </ul> <p>A docstring should give enough information to write a call to the function without reading the function's code. The docstring should be descriptive-style (<code>\"\"\"Fetches rows from a Bigtable.\"\"\"</code>) rather than imperative-style (<code>\"\"\"Fetch rows from a Bigtable.\"\"\"</code>), except for <code>@property</code> data descriptors, which should use the same style as attributes. A docstring should describe the function's calling syntax and its semantics, not its implementation. For tricky code, comments alongside the code are more appropriate than using docstrings.</p> <p>A method that overrides a method from a base class may have a simple docstring sending the reader to its overridden method's docstring, such as <code>\"\"\"See base class.\"\"\"</code>. The rationale is that there is no need to repeat in many places documentation that is already present in the base method's docstring. However, if the overriding method's behavior is substantially different from the overridden method, or details need to be provided (e.g., documenting additional side effects), a docstring with at least those differences is required on the overriding method.</p> <p>Certain aspects of a function should be documented in special sections, listed below. Each section begins with a heading line, which ends with a colon. All sections other than the heading should maintain a hanging indent of two or four spaces (be consistent within a file). These sections can be omitted in cases where the function's name and signature are informative enough that it can be aptly described using a one-line docstring.</p> <p>Args :   List each parameter by name. A description should follow the name, and be     separated by a colon followed by either a space or newline. If the     description is too long to fit on a single 80-character line, use a hanging     indent of 2 or 4 spaces more than the parameter name (be consistent with the     rest of the docstrings in the file). The description should include required     type(s) if the code does not contain a corresponding type annotation. If a     function accepts <code>*foo</code> (variable length argument lists) and/or <code>**bar</code>     (arbitrary keyword arguments), they should be listed as <code>*foo</code> and <code>**bar</code>.</p> <p>Returns: (or Yields: for generators) :   Describe the type and semantics of the return value. If the function only     returns None, this section is not required. It may also be omitted if the     docstring starts with Returns or Yields (e.g. <code>\"\"\"Returns row from Bigtable     as a tuple of strings.\"\"\"</code>) and the opening sentence is sufficient to     describe return value.</p> <p>Raises:   List all exceptions that are relevant to the interface followed by a     description. Use a similar exception name + colon + space or newline and     hanging indent style as described in Args:. You should not document     exceptions that get raised if the API specified in the docstring is violated     (because this would paradoxically make behavior under violation of the API     part of the API).</p> <pre><code>def fetch_smalltable_rows(table_handle: smalltable.Table,\n                          keys: Sequence[Union[bytes, str]],\n                          require_all_keys: bool = False,\n) -&gt; Mapping[bytes, Tuple[str]]:\n    \"\"\"Fetches rows from a Smalltable.\n\n    Retrieves rows pertaining to the given keys from the Table instance\n    represented by table_handle.  String keys will be UTF-8 encoded.\n\n    Args:\n        table_handle: An open smalltable.Table instance.\n        keys: A sequence of strings representing the key of each table\n          row to fetch.  String keys will be UTF-8 encoded.\n        require_all_keys: Optional; If require_all_keys is True only\n          rows with values set for all keys will be returned.\n\n    Returns:\n        A dict mapping keys to the corresponding table row data\n        fetched. Each row is represented as a tuple of strings. For\n        example:\n\n        {b'Serak': ('Rigel VII', 'Preparer'),\n         b'Zim': ('Irk', 'Invader'),\n         b'Lrrr': ('Omicron Persei 8', 'Emperor')}\n\n        Returned keys are always bytes.  If a key from the keys argument is\n        missing from the dictionary, then that row was not found in the\n        table (and require_all_keys must have been False).\n\n    Raises:\n        IOError: An error occurred accessing the smalltable.\n    \"\"\"\n</code></pre> <p>Similarly, this variation on <code>Args:</code> with a line break is also allowed:</p> <pre><code>def fetch_smalltable_rows(table_handle: smalltable.Table,\n                          keys: Sequence[Union[bytes, str]],\n                          require_all_keys: bool = False,\n) -&gt; Mapping[bytes, Tuple[str]]:\n    \"\"\"Fetches rows from a Smalltable.\n\n    Retrieves rows pertaining to the given keys from the Table instance\n    represented by table_handle.  String keys will be UTF-8 encoded.\n\n    Args:\n      table_handle:\n        An open smalltable.Table instance.\n      keys:\n        A sequence of strings representing the key of each table row to\n        fetch.  String keys will be UTF-8 encoded.\n      require_all_keys:\n        Optional; If require_all_keys is True only rows with values set\n        for all keys will be returned.\n\n    Returns:\n      A dict mapping keys to the corresponding table row data\n      fetched. Each row is represented as a tuple of strings. For\n      example:\n\n      {b'Serak': ('Rigel VII', 'Preparer'),\n       b'Zim': ('Irk', 'Invader'),\n       b'Lrrr': ('Omicron Persei 8', 'Emperor')}\n\n      Returned keys are always bytes.  If a key from the keys argument is\n      missing from the dictionary, then that row was not found in the\n      table (and require_all_keys must have been False).\n\n    Raises:\n      IOError: An error occurred accessing the smalltable.\n    \"\"\"\n</code></pre>"},{"location":"python/best_practices/styleguide/#384-classes","title":"3.8.4 Classes","text":"<p>Classes should have a docstring below the class definition describing the class. If your class has public attributes, they should be documented here in an <code>Attributes</code> section and follow the same formatting as a function's <code>Args</code> section.</p> <pre><code>class SampleClass:\n    \"\"\"Summary of class here.\n\n    Longer class information....\n    Longer class information....\n\n    Attributes:\n        likes_spam: A boolean indicating if we like SPAM or not.\n        eggs: An integer count of the eggs we have laid.\n    \"\"\"\n\n    def __init__(self, likes_spam=False):\n        \"\"\"Inits SampleClass with blah.\"\"\"\n        self.likes_spam = likes_spam\n        self.eggs = 0\n\n    def public_method(self):\n        \"\"\"Performs operation blah.\"\"\"\n</code></pre>"},{"location":"python/best_practices/styleguide/#385-block-and-inline-comments","title":"3.8.5 Block and Inline Comments","text":"<p>The final place to have comments is in tricky parts of the code. If you're going to have to explain it at the next code review, you should comment it now. Complicated operations get a few lines of comments before the operations commence. Non-obvious ones get comments at the end of the line.</p> <pre><code># We use a weighted dictionary search to find out where i is in\n# the array.  We extrapolate position based on the largest num\n# in the array and the array size and then do binary search to\n# get the exact number.\n\nif i &amp; (i-1) == 0:  # True if i is 0 or a power of 2.\n</code></pre> <p>To improve legibility, these comments should start at least 2 spaces away from the code with the comment character <code>#</code>, followed by at least one space before the text of the comment itself.</p> <p>On the other hand, never describe the code. Assume the person reading the code knows Python (though not what you're trying to do) better than you do.</p> <pre><code># BAD COMMENT: Now go through the b array and make sure whenever i occurs\n# the next element is i+1\n</code></pre>"},{"location":"python/best_practices/styleguide/#386-punctuation-spelling-and-grammar","title":"3.8.6 Punctuation, Spelling, and Grammar","text":"<p>Pay attention to punctuation, spelling, and grammar; it is easier to read well-written comments than badly written ones.</p> <p>Comments should be as readable as narrative text, with proper capitalization and punctuation. In many cases, complete sentences are more readable than sentence fragments. Shorter comments, such as comments at the end of a line of code, can sometimes be less formal, but you should be consistent with your style.</p> <p>Although it can be frustrating to have a code reviewer point out that you are using a comma when you should be using a semicolon, it is very important that source code maintain a high level of clarity and readability. Proper punctuation, spelling, and grammar help with that goal.</p>"},{"location":"python/best_practices/styleguide/#310-strings","title":"3.10 Strings","text":"<p>Use an f-string, the <code>%</code> operator, or the <code>format</code> method for formatting strings, even when the parameters are all strings. Use your best judgment to decide between <code>+</code> and <code>%</code> (or <code>format</code>) though. Do not use <code>%</code> or the <code>format</code> method for pure concatenation.</p> <pre><code>Yes: x = a + b\n     x = '%s, %s!' % (imperative, expletive)\n     x = '{}, {}'.format(first, second)\n     x = 'name: %s; score: %d' % (name, n)\n     x = 'name: {}; score: {}'.format(name, n)\n     x = f'name: {name}; score: {n}'\n</code></pre> <pre><code>No: x = '%s%s' % (a, b)  # use + in this case\n    x = '{}{}'.format(a, b)  # use + in this case\n    x = first + ', ' + second\n    x = 'name: ' + name + '; score: ' + str(n)\n</code></pre> <p>Avoid using the <code>+</code> and <code>+=</code> operators to accumulate a string within a loop. In some conditions, accumulating a string with addition can lead to quadratic rather than linear running time. Although common accumulations of this sort may be optimized on CPython, that is an implementation detail. The conditions under which an optimization applies are not easy to predict and may change. Instead, add each substring to a list and <code>''.join</code> the list after the loop terminates, or write each substring to an <code>io.StringIO</code> buffer. These techniques consistently have amortized-linear run time complexity.</p> <pre><code>Yes: items = ['&lt;table&gt;']\n     for last_name, first_name in employee_list:\n         items.append('&lt;tr&gt;&lt;td&gt;%s, %s&lt;/td&gt;&lt;/tr&gt;' % (last_name, first_name))\n     items.append('&lt;/table&gt;')\n     employee_table = ''.join(items)\n</code></pre> <pre><code>No: employee_table = '&lt;table&gt;'\n    for last_name, first_name in employee_list:\n        employee_table += '&lt;tr&gt;&lt;td&gt;%s, %s&lt;/td&gt;&lt;/tr&gt;' % (last_name, first_name)\n    employee_table += '&lt;/table&gt;'\n</code></pre> <p>Be consistent with your choice of string quote character within a file. Pick <code>'</code> or <code>\"</code> and stick with it. It is okay to use the other quote character on a string to avoid the need to <code>\\\\</code> escape within the string.</p> <pre><code>Yes:\n  Python('Why are you hiding your eyes?')\n  Gollum(\"I'm scared of lint errors.\")\n  Narrator('\"Good!\" thought a happy Python reviewer.')\n</code></pre> <pre><code>No:\n  Python(\"Why are you hiding your eyes?\")\n  Gollum('The lint. It burns. It burns us.')\n  Gollum(\"Always the great lint. Watching. Watching.\")\n</code></pre> <p>Prefer <code>\"\"\"</code> for multi-line strings rather than <code>'''</code>. Projects may choose to use <code>'''</code> for all non-docstring multi-line strings if and only if they also use <code>'</code> for regular strings. Docstrings must use <code>\"\"\"</code> regardless.</p> <p>Multi-line strings do not flow with the indentation of the rest of the program. If you need to avoid embedding extra space in the string, use either concatenated single-line strings or a multi-line string with <code>textwrap.dedent()</code> to remove the initial space on each line:</p> <pre><code>  No:\n  long_string = \"\"\"This is pretty ugly.\nDon't do this.\n\"\"\"\n</code></pre> <pre><code>  Yes:\n  long_string = \"\"\"This is fine if your use case can accept\n      extraneous leading spaces.\"\"\"\n</code></pre> <pre><code>  Yes:\n  long_string = (\"And this is fine if you cannot accept\\n\" +\n                 \"extraneous leading spaces.\")\n</code></pre> <pre><code>  Yes:\n  long_string = (\"And this too is fine if you cannot accept\\n\"\n                 \"extraneous leading spaces.\")\n</code></pre> <pre><code>  Yes:\n  import textwrap\n\n  long_string = textwrap.dedent(\"\"\"\\\n      This is also fine, because textwrap.dedent()\n      will collapse common leading spaces in each line.\"\"\")\n</code></pre>"},{"location":"python/best_practices/styleguide/#3101-logging","title":"3.10.1 Logging","text":"<p>For logging functions that expect a pattern-string (with %-placeholders) as their first argument: Always call them with a string literal (not an f-string!) as their first argument with pattern-parameters as subsequent arguments. Some logging implementations collect the unexpanded pattern-string as a queryable field. It also prevents spending time rendering a message that no logger is configured to output.</p> <pre><code>  Yes:\n  import tensorflow as tf\n  logger = tf.get_logger()\n  logger.info('TensorFlow Version is: %s', tf.__version__)\n</code></pre> <pre><code>  Yes:\n  import os\n  from absl import logging\n\n  logging.info('Current $PAGER is: %s', os.getenv('PAGER', default=''))\n\n  homedir = os.getenv('HOME')\n  if homedir is None or not os.access(homedir, os.W_OK):\n    logging.error('Cannot write to home directory, $HOME=%r', homedir)\n</code></pre> <pre><code>  No:\n  import os\n  from absl import logging\n\n  logging.info('Current $PAGER is:')\n  logging.info(os.getenv('PAGER', default=''))\n\n  homedir = os.getenv('HOME')\n  if homedir is None or not os.access(homedir, os.W_OK):\n    logging.error(f'Cannot write to home directory, $HOME={homedir!r}')\n</code></pre>"},{"location":"python/best_practices/styleguide/#3102-error-messages","title":"3.10.2 Error Messages","text":"<p>Error messages (such as: message strings on exceptions like <code>ValueError</code>, or messages shown to the user) should follow three guidelines:</p> <ol> <li> <p>The message needs to precisely match the actual error condition.</p> </li> <li> <p>Interpolated pieces need to always be clearly identifiable as such.</p> </li> <li> <p>They should allow simple automated processing (e.g. grepping).</p> </li> </ol> <pre><code>  Yes:\n  if not 0 &lt;= p &lt;= 1:\n    raise ValueError(f'Not a probability: {p!r}')\n\n  try:\n    os.rmdir(workdir)\n  except OSError as error:\n    logging.warning('Could not remove directory (reason: %r): %r',\n                    error, workdir)\n</code></pre> <pre><code>  No:\n  if p &lt; 0 or p &gt; 1:  # PROBLEM: also false for float('nan')!\n    raise ValueError(f'Not a probability: {p!r}')\n\n  try:\n    os.rmdir(workdir)\n  except OSError:\n    # PROBLEM: Message makes an assumption that might not be true:\n    # Deletion might have failed for some other reason, misleading\n    # whoever has to debug this.\n    logging.warning('Directory already was deleted: %s', workdir)\n\n  try:\n    os.rmdir(workdir)\n  except OSError:\n    # PROBLEM: The message is harder to grep for than necessary, and\n    # not universally non-confusing for all possible values of `workdir`.\n    # Imagine someone calling a library function with such code\n    # using a name such as workdir = 'deleted'. The warning would read:\n    # \"The deleted directory could not be deleted.\"\n    logging.warning('The %s directory could not be deleted.', workdir)\n</code></pre>"},{"location":"python/best_practices/styleguide/#311-files-and-sockets","title":"3.11 Files and Sockets","text":"<p>Explicitly close files and sockets when done with them.</p> <p>Leaving files, sockets or other file-like objects open unnecessarily has many downsides:</p> <ul> <li>They may consume limited system resources, such as file descriptors. Code     that deals with many such objects may exhaust those resources unnecessarily     if they're not returned to the system promptly after use.</li> <li>Holding files open may prevent other actions such as moving or deleting     them.</li> <li>Files and sockets that are shared throughout a program may inadvertently be     read from or written to after logically being closed. If they are actually     closed, attempts to read or write from them will throw exceptions, making     the problem known sooner.</li> </ul> <p>Furthermore, while files and sockets are automatically closed when the file object is destructed, tying the lifetime of the file object to the state of the file is poor practice:</p> <ul> <li>There are no guarantees as to when the runtime will actually run the file's     destructor. Different Python implementations use different memory management     techniques, such as delayed garbage collection, which may increase the     object's lifetime arbitrarily and indefinitely.</li> <li>Unexpected references to the file, e.g. in globals or exception tracebacks,     may keep it around longer than intended.</li> </ul> <p>The preferred way to manage files is using the <code>with</code> statement:</p> <pre><code>with open(\"hello.txt\") as hello_file:\n    for line in hello_file:\n        print(line)\n</code></pre> <p>For file-like objects that do not support the <code>with</code> statement, use <code>contextlib.closing()</code>:</p> <pre><code>import contextlib\n\nwith contextlib.closing(urllib.urlopen(\"http://www.python.org/\")) as front_page:\n    for line in front_page:\n        print(line)\n</code></pre>"},{"location":"python/best_practices/styleguide/#312-todo-comments","title":"3.12 TODO Comments","text":"<p>TODO comments should not be merged to master, you can use them while developing your branch.</p> <p>If the TODO is out of the scope of the Ticket a new one should be created. If it is in the scope of the ticket it should be implemented.</p>"},{"location":"python/best_practices/styleguide/#313-imports-formatting","title":"3.13 Imports formatting","text":"<p>Imports should be on separate lines; there are exceptions for <code>typing</code> imports.</p> <p>E.g.:</p> <pre><code>Yes: import os\n     import sys\n     from typing import Mapping, Sequence\n</code></pre> <pre><code>No:  import os, sys\n</code></pre> <p>Imports are always put at the top of the file, just after any module comments and docstrings and before module globals and constants. Imports should be grouped from most generic to least generic:</p> <ol> <li> <p>Python future import statements. For example:</p> <pre><code>from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n</code></pre> <p>See above for more information about those.</p> </li> <li> <p>Python standard library imports. For example:</p> <pre><code>import sys\n</code></pre> </li> <li> <p>third-party module     or package imports. For example:</p> <pre><code>import tensorflow as tf\n</code></pre> </li> <li> <p>Code repository     sub-package imports. For example:</p> <pre><code>from otherproject.ai import mind\n</code></pre> </li> <li> <p>Deprecated: application-specific imports that are part of the same     top level     sub-package as this file. For example:</p> <pre><code>from myproject.backend.hgwells import time_machine\n</code></pre> <p>You may find older Google Python Style code doing this, but it is no longer required. New code is encouraged not to bother with this. Simply treat application-specific sub-package imports the same as other sub-package imports.</p> </li> </ol> <p>Within each grouping, imports should be sorted lexicographically, ignoring case, according to each module's full package path (the <code>path</code> in <code>from path import ...</code>). Code may optionally place a blank line between import sections.</p> <pre><code>import collections\nimport queue\nimport sys\n\nfrom absl import app\nfrom absl import flags\nimport bs4\nimport cryptography\nimport tensorflow as tf\n\nfrom book.genres import scifi\nfrom myproject.backend import huxley\nfrom myproject.backend.hgwells import time_machine\nfrom myproject.backend.state_machine import main_loop\nfrom otherproject.ai import body\nfrom otherproject.ai import mind\nfrom otherproject.ai import soul\n\n# Older style code may have these imports down here instead:\n#from myproject.backend.hgwells import time_machine\n#from myproject.backend.state_machine import main_loop\n</code></pre>"},{"location":"python/best_practices/styleguide/#314-statements","title":"3.14 Statements","text":"<p>Generally only one statement per line.</p> <p>However, you may put the result of a test on the same line as the test only if the entire statement fits on one line. In particular, you can never do so with <code>try</code>/<code>except</code> since the <code>try</code> and <code>except</code> can't both fit on the same line, and you can only do so with an <code>if</code> if there is no <code>else</code>.</p> <pre><code>Yes:\n\n  if foo: bar(foo)\n</code></pre> <pre><code>No:\n\n  if foo: bar(foo)\n  else:   baz(foo)\n\n  try:               bar(foo)\n  except ValueError: baz(foo)\n\n  try:\n      bar(foo)\n  except ValueError: baz(foo)\n</code></pre>"},{"location":"python/best_practices/styleguide/#315-accessors","title":"3.15 Accessors","text":"<p>If an accessor function would be trivial, you should use public variables instead of accessor functions to avoid the extra cost of function calls in Python. When more functionality is added you can use <code>property</code> to keep the syntax consistent.</p> <p>On the other hand, if access is more complex, or the cost of accessing the variable is significant, you should use function calls (following the Naming guidelines) such as <code>get_foo()</code> and <code>set_foo()</code>. If the past behavior allowed access through a property, do not bind the new accessor functions to the property. Any code still attempting to access the variable by the old method should break visibly so they are made aware of the change in complexity.</p>"},{"location":"python/best_practices/styleguide/#316-naming","title":"3.16 Naming","text":"<p><code>module_name</code>, <code>package_name</code>, <code>ClassName</code>, <code>method_name</code>, <code>ExceptionName</code>, <code>function_name</code>, <code>GLOBAL_CONSTANT_NAME</code>, <code>global_var_name</code>, <code>instance_var_name</code>, <code>function_parameter_name</code>, <code>local_var_name</code>.</p> <p>Function names, variable names, and filenames should be descriptive; eschew abbreviation. In particular, do not use abbreviations that are ambiguous or unfamiliar to readers outside your project, and do not abbreviate by deleting letters within a word.</p> <p>Always use a <code>.py</code> filename extension. Never use dashes.</p>"},{"location":"python/best_practices/styleguide/#3161-names-to-avoid","title":"3.16.1 Names to Avoid","text":"<ul> <li> <p>single character names, except for specifically allowed cases:</p> </li> <li> <p>counters or iterators (e.g. <code>i</code>, <code>j</code>, <code>k</code>, <code>v</code>, et al.)</p> </li> <li><code>e</code> as an exception identifier in <code>try/except</code> statements.</li> <li> <p><code>f</code> as a file handle in <code>with</code> statements</p> <p>Please be mindful not to abuse single-character naming. Generally speaking, descriptiveness should be proportional to the name's scope of visibility. For example, <code>i</code> might be a fine name for 5-line code block but within multiple nested scopes, it is likely too vague.</p> </li> <li> <p>dashes (<code>-</code>) in any package/module name</p> </li> <li> <p><code>__double_leading_and_trailing_underscore__</code> names (reserved by Python)</p> </li> <li> <p>offensive terms</p> </li> <li> <p>names that needlessly include the type of the variable (for example:     <code>id_to_name_dict</code>)</p> </li> </ul>"},{"location":"python/best_practices/styleguide/#3162-naming-conventions","title":"3.16.2 Naming Conventions","text":"<ul> <li> <p>\"Internal\" means internal to a module, or protected or private within a     class.</p> </li> <li> <p>Prepending a single underscore (<code>_</code>) has some support for protecting module     variables and functions (linters will flag protected member access). While     prepending a double underscore (<code>__</code> aka \"dunder\") to an instance variable     or method effectively makes the variable or method private to its class     (using name mangling); we discourage its use as it impacts readability and     testability, and isn't really private.</p> </li> <li> <p>Place related classes and top-level functions together in a     module.     Unlike Java, there is no need to limit yourself to one class per module.</p> </li> <li> <p>Use CapWords for class names, but lower_with_under.py for module names.     Although there are some old modules named CapWords.py, this is now     discouraged because it's confusing when the module happens to be named after     a class. (\"wait -- did I write <code>import StringIO</code> or <code>from StringIO import     StringIO</code>?\")</p> </li> <li> <p>Underscores may appear in unittest method names starting with <code>test</code> to     separate logical components of the name, even if those components use     CapWords. One possible pattern is <code>test&lt;MethodUnderTest&gt;_&lt;state&gt;</code>; for     example <code>testPop_EmptyStack</code> is okay. There is no One Correct Way to name     test methods.</p> </li> </ul>"},{"location":"python/best_practices/styleguide/#3163-file-naming","title":"3.16.3 File Naming","text":"<p>Python filenames must have a <code>.py</code> extension and must not contain dashes (<code>-</code>). This allows them to be imported and unittested. If you want an executable to be accessible without the extension, use a symbolic link or a simple bash wrapper containing <code>exec \"$0.py\" \"$@\"</code>.</p>"},{"location":"python/best_practices/styleguide/#3164-guidelines-derived-from-guidos-recommendations","title":"3.16.4 Guidelines derived from Guido's Recommendations","text":"Type Public Internal Packages <code>lower_with_under</code> Modules <code>lower_with_under</code> <code>_lower_with_under</code> Classes <code>CapWords</code> <code>_CapWords</code> Exceptions <code>CapWords</code> Functions <code>lower_with_under()</code> <code>_lower_with_under()</code> Global/Class Constants <code>CAPS_WITH_UNDER</code> <code>_CAPS_WITH_UNDER</code> Global/Class Variables <code>lower_with_under</code> <code>_lower_with_under</code> Instance Variables <code>lower_with_under</code> <code>_lower_with_under</code>(protected) Method Names <code>lower_with_under()</code> <code>_lower_with_under()</code>(protected) Function/Method Parameters <code>lower_with_under</code> Local Variables <code>lower_with_under</code>"},{"location":"python/best_practices/styleguide/#317-main","title":"3.17 Main","text":"<p>In Python, <code>pydoc</code> as well as unit tests require modules to be importable. If a file is meant to be used as an executable, its main functionality should be in a <code>main()</code> function, and your code should always check <code>if __name__ == '__main__'</code> before executing your main program, so that it is not executed when the module is imported.</p> <p>When using absl, use <code>app.run</code>:</p> <pre><code>from absl import app\n...\n\ndef main(argv):\n    # process non-flag arguments\n    ...\n\nif __name__ == '__main__':\n    app.run(main)\n</code></pre> <p>Otherwise, use:</p> <pre><code>def main():\n    ...\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p>All code at the top level will be executed when the module is imported. Be careful not to call functions, create objects, or perform other operations that should not be executed when the file is being <code>pydoc</code>ed.</p>"},{"location":"python/best_practices/styleguide/#318-function-length","title":"3.18 Function length","text":"<p>Prefer small and focused functions.</p> <p>We recognize that long functions are sometimes appropriate, so no hard limit is placed on function length. If a function exceeds about 40 lines, think about whether it can be broken up without harming the structure of the program.</p> <p>Even if your long function works perfectly now, someone modifying it in a few months may add new behavior. This could result in bugs that are hard to find. Keeping your functions short and simple makes it easier for other people to read and modify your code.</p> <p>You could find long and complicated functions when working with some code. Do not be intimidated by modifying existing code: if working with such a function proves to be difficult, you find that errors are hard to debug, or you want to use a piece of it in several different contexts, consider breaking up the function into smaller and more manageable pieces.</p>"},{"location":"python/best_practices/styleguide/#319-type-annotations","title":"3.19 Type Annotations","text":""},{"location":"python/best_practices/styleguide/#3191-general-rules","title":"3.19.1 General Rules","text":"<ul> <li>Familiarize yourself with     PEP-484.</li> <li>In methods, only annotate <code>self</code>, or <code>cls</code> if it is necessary for proper     type information. e.g., <code>@classmethod def create(cls: Type[T]) -&gt; T: return     cls()</code></li> <li>If any other variable or a returned type should not be expressed, use <code>Any</code>.</li> <li>You are not required to annotate all the functions in a module.</li> <li>At least annotate your public APIs.</li> <li>Use judgment to get to a good balance between safety and clarity on the         one hand, and flexibility on the other.</li> <li>Annotate code that is prone to type-related errors (previous bugs or         complexity).</li> <li>Annotate code that is hard to understand.</li> <li>Annotate code as it becomes stable from a types perspective. In many         cases, you can annotate all the functions in mature code without losing         too much flexibility.</li> </ul>"},{"location":"python/best_practices/styleguide/#3192-line-breaking","title":"3.19.2 Line Breaking","text":"<p>Try to follow the existing indentation rules.</p> <p>After annotating, many function signatures will become \"one parameter per line\".</p> <pre><code>def my_method(self,\n              first_var: int,\n              second_var: Foo,\n              third_var: Optional[Bar]) -&gt; int:\n  ...\n</code></pre> <p>Always prefer breaking between variables, and not, for example, between variable names and type annotations. However, if everything fits on the same line, go for it.</p> <pre><code>def my_method(self, first_var: int) -&gt; int:\n  ...\n</code></pre> <p>If the combination of the function name, the last parameter, and the return type is too long, indent by 4 in a new line.</p> <pre><code>def my_method(\n    self, first_var: int) -&gt; Tuple[MyLongType1, MyLongType1]:\n  ...\n</code></pre> <p>When the return type does not fit on the same line as the last parameter, the preferred way is to indent the parameters by 4 on a new line and align the closing parenthesis with the <code>def</code>.</p> <pre><code>Yes:\ndef my_method(\n    self, other_arg: Optional[MyLongType]\n) -&gt; Dict[OtherLongType, MyLongType]:\n  ...\n</code></pre> <p><code>pylint</code> allows you to move the closing parenthesis to a new line and align with the opening one, but this is less readable.</p> <pre><code>No:\ndef my_method(self,\n              other_arg: Optional[MyLongType]\n             ) -&gt; Dict[OtherLongType, MyLongType]:\n  ...\n</code></pre> <p>As in the examples above, prefer not to break types. However, sometimes they are too long to be on a single line (try to keep sub-types unbroken).</p> <pre><code>def my_method(\n    self,\n    first_var: Tuple[List[MyLongType1],\n                     List[MyLongType2]],\n    second_var: List[Dict[\n        MyLongType3, MyLongType4]]) -&gt; None:\n  ...\n</code></pre> <p>If a single name and type is too long, consider using an alias for the type. The last resort is to break after the colon and indent by 4.</p> <pre><code>Yes:\ndef my_function(\n    long_variable_name:\n        long_module_name.LongTypeName,\n) -&gt; None:\n  ...\n</code></pre> <pre><code>No:\ndef my_function(\n    long_variable_name: long_module_name.\n        LongTypeName,\n) -&gt; None:\n  ...\n</code></pre>"},{"location":"python/best_practices/styleguide/#3193-forward-declarations","title":"3.19.3 Forward Declarations","text":"<p>If you need to use a class name from the same module that is not yet defined -- for example, if you need the class inside the class declaration, or if you use a class that is defined below -- use a string for the class name.</p> <pre><code>class MyClass:\n\n  def __init__(self,\n               stack: List[\"MyClass\"]) -&gt; None:\n</code></pre>"},{"location":"python/best_practices/styleguide/#3194-default-values","title":"3.19.4 Default Values","text":"<p>As per PEP-008, use spaces around the <code>=</code> only for arguments that have both a type annotation and a default value.</p> <pre><code>Yes:\ndef func(a: int = 0) -&gt; int:\n  ...\n</code></pre> <pre><code>No:\ndef func(a:int=0) -&gt; int:\n  ...\n</code></pre>"},{"location":"python/best_practices/styleguide/#3195-nonetype","title":"3.19.5 NoneType","text":"<p>In the Python type system, <code>NoneType</code> is a \"first class\" type, and for typing purposes, <code>None</code> is an alias for <code>NoneType</code>. If an argument can be <code>None</code>, it has to be declared! You can use <code>Union</code>, but if there is only one other type, use <code>Optional</code>.</p> <p>Use explicit <code>Optional</code> instead of implicit <code>Optional</code>. Earlier versions of PEP 484 allowed <code>a: Text = None</code> to be interpreted as <code>a: Optional[Text] = None</code>, but that is no longer the preferred behavior.</p> <pre><code>Yes:\ndef func(a: Optional[Text], b: Optional[Text] = None) -&gt; Text:\n  ...\ndef multiple_nullable_union(a: Union[None, Text, int]) -&gt; Text\n  ...\n</code></pre> <pre><code>No:\ndef nullable_union(a: Union[None, Text]) -&gt; Text:\n  ...\ndef implicit_optional(a: Text = None) -&gt; Text:\n  ...\n</code></pre>"},{"location":"python/best_practices/styleguide/#3196-type-aliases","title":"3.19.6 Type Aliases","text":"<p>You can declare aliases of complex types. The name of an alias should be CapWorded. If the alias is used only in this module, it should be _Private.</p> <p>For example, if the name of the module together with the name of the type is too long:</p> <pre><code>_ShortName = module_with_long_name.TypeWithLongName\nComplexMap = Mapping[Text, List[Tuple[int, int]]]\n</code></pre> <p>Other examples are complex nested types and multiple return variables from a function (as a tuple).</p>"},{"location":"python/best_practices/styleguide/#3197-ignoring-types","title":"3.19.7 Ignoring Types","text":"<p>You can disable type checking on a line with the special comment <code># type: ignore</code>.</p> <p><code>pytype</code> has a disable option for specific errors (similar to lint):</p> <pre><code># pytype: disable=attribute-error\n</code></pre>"},{"location":"python/best_practices/styleguide/#3198-typing-variables","title":"3.19.8 Typing Variables","text":"<p>If an internal variable has a type that is hard or impossible to infer, you can specify its type in a couple ways.</p> <p>Type Comments:   Use a <code># type:</code> comment on the end of the line</p> <pre><code>a = SomeUndecoratedFunction()  # type: Foo\n</code></pre> <p>Annotated Assignments:   Use a colon and type between the variable name and value, as with function     arguments.</p> <pre><code>a: Foo = SomeUndecoratedFunction()\n</code></pre>"},{"location":"python/best_practices/styleguide/#3199-tuples-vs-lists","title":"3.19.9 Tuples vs Lists","text":"<p>Typed lists can only contain objects of a single type. Typed tuples can either have a single repeated type or a set number of elements with different types. The latter is commonly used as the return type from a function.</p> <pre><code>a = [1, 2, 3]  # type: List[int]\nb = (1, 2, 3)  # type: Tuple[int, ...]\nc = (1, \"2\", 3.5)  # type: Tuple[int, Text, float]\n</code></pre>"},{"location":"python/best_practices/styleguide/#31910-typevars","title":"3.19.10 TypeVars","text":"<p>The Python type system has generics. The factory function <code>TypeVar</code> is a common way to use them.</p> <p>Example:</p> <pre><code>from typing import List, TypeVar\nT = TypeVar(\"T\")\n...\ndef next(l: List[T]) -&gt; T:\n  return l.pop()\n</code></pre> <p>A TypeVar can be constrained:</p> <pre><code>AddableType = TypeVar(\"AddableType\", int, float, Text)\ndef add(a: AddableType, b: AddableType) -&gt; AddableType:\n  return a + b\n</code></pre> <p>A common predefined type variable in the <code>typing</code> module is <code>AnyStr</code>. Use it for multiple annotations that can be <code>bytes</code> or <code>unicode</code> and must all be the same type.</p> <pre><code>from typing import AnyStr\ndef check_length(x: AnyStr) -&gt; AnyStr:\n  if len(x) &lt;= 42:\n    return x\n  raise ValueError()\n</code></pre>"},{"location":"python/best_practices/styleguide/#31911-string-types","title":"3.19.11 String types","text":"<p>The proper type for annotating strings depends on what versions of Python the code is intended for.</p> <p>For Python 3 only code, prefer to use <code>str</code>. <code>Text</code> is also acceptable. Be consistent in using one or the other.</p> <p>For Python 2 compatible code, use <code>Text</code>. In some rare cases, <code>str</code> may make sense; typically to aid compatibility when the return types aren't the same between the two Python versions. Avoid using <code>unicode</code>: it doesn't exist in Python 3.</p> <p>The reason this discrepancy exists is because <code>str</code> means different things depending on the Python version.</p> <pre><code>No:\ndef py2_code(x: str) -&gt; unicode:\n  ...\n</code></pre> <p>For code that deals with binary data, use <code>bytes</code>.</p> <pre><code>def deals_with_binary_data(x: bytes) -&gt; bytes:\n  ...\n</code></pre> <p>For Python 2 compatible code that processes text data (<code>str</code> or <code>unicode</code> in Python 2, <code>str</code> in Python 3), use <code>Text</code>. For Python 3 only code that process text data, prefer <code>str</code>.</p> <pre><code>from typing import Text\n...\ndef py2_compatible(x: Text) -&gt; Text:\n  ...\ndef py3_only(x: str) -&gt; str:\n  ...\n</code></pre> <p>If the type can be either bytes or text, use <code>Union</code>, with the appropriate text type.</p> <pre><code>from typing import Text, Union\n...\ndef py2_compatible(x: Union[bytes, Text]) -&gt; Union[bytes, Text]:\n  ...\ndef py3_only(x: Union[bytes, str]) -&gt; Union[bytes, str]:\n  ...\n</code></pre> <p>If all the string types of a function are always the same, for example if the return type is the same as the argument type in the code above, use AnyStr.</p> <p>Writing it like this will simplify the process of porting the code to Python 3.</p>"},{"location":"python/best_practices/styleguide/#31912-imports-for-typing","title":"3.19.12 Imports For Typing","text":"<p>For classes from the <code>typing</code> module, always import the class itself. You are explicitly allowed to import multiple specific classes on one line from the <code>typing</code> module. Ex:</p> <pre><code>from typing import Any, Dict, Optional\n</code></pre> <p>Given that this way of importing from <code>typing</code> adds items to the local namespace, any names in <code>typing</code> should be treated similarly to keywords, and not be defined in your Python code, typed or not. If there is a collision between a type and an existing name in a module, import it using <code>import x as y</code>.</p> <pre><code>from typing import Any as AnyType\n</code></pre>"},{"location":"python/best_practices/styleguide/#31913-conditional-imports","title":"3.19.13 Conditional Imports","text":"<p>Use conditional imports only in exceptional cases where the additional imports needed for type checking must be avoided at runtime. This pattern is discouraged; alternatives such as refactoring the code to allow top level imports should be preferred.</p> <p>Imports that are needed only for type annotations can be placed within an <code>if TYPE_CHECKING:</code> block.</p> <ul> <li>Conditionally imported types need to be referenced as strings, to be forward     compatible with Python 3.6 where the annotation expressions are actually     evaluated.</li> <li>Only entities that are used solely for typing should be defined here; this     includes aliases. Otherwise it will be a runtime error, as the module will     not be imported at runtime.</li> <li>The block should be right after all the normal imports.</li> <li>There should be no empty lines in the typing imports list.</li> <li>Sort this list as if it were a regular imports list.</li> </ul> <pre><code>import typing\nif typing.TYPE_CHECKING:\n  import sketch\ndef f(x: \"sketch.Sketch\"): ...\n</code></pre>"},{"location":"python/best_practices/styleguide/#31914-circular-dependencies","title":"3.19.14 Circular Dependencies","text":"<p>Circular dependencies that are caused by typing are code smells. Such code is a good candidate for refactoring. Although technically it is possible to keep circular dependencies, various build systems will not let you do so because each module has to depend on the other.</p> <p>Replace modules that create circular dependency imports with <code>Any</code>. Set an alias with a meaningful name, and use the real type name from this module (any attribute of Any is Any). Alias definitions should be separated from the last import by one line.</p> <pre><code>from typing import Any\n\nsome_mod = Any  # some_mod.py imports this module.\n...\n\ndef my_method(self, var: \"some_mod.SomeType\") -&gt; None:\n  ...\n</code></pre>"},{"location":"python/best_practices/styleguide/#31915-generics","title":"3.19.15 Generics","text":"<p>When annotating, prefer to specify type parameters for generic types; otherwise, the generics' parameters will be assumed to be <code>Any</code>.</p> <pre><code>def get_names(employee_ids: List[int]) -&gt; Dict[int, Any]:\n  ...\n</code></pre> <pre><code># These are both interpreted as get_names(employee_ids: List[Any]) -&gt; Dict[Any, Any]\ndef get_names(employee_ids: list) -&gt; Dict:\n  ...\n\ndef get_names(employee_ids: List) -&gt; Dict:\n  ...\n</code></pre> <p>If the best type parameter for a generic is <code>Any</code>, make it explicit, but remember that in many cases <code>TypeVar</code> might be more appropriate:</p> <pre><code>def get_names(employee_ids: List[Any]) -&gt; Dict[Any, Text]:\n  \"\"\"Returns a mapping from employee ID to employee name for given IDs.\"\"\"\n</code></pre> <pre><code>T = TypeVar('T')\ndef get_names(employee_ids: List[T]) -&gt; Dict[T, Text]:\n  \"\"\"Returns a mapping from employee ID to employee name for given IDs.\"\"\"\n</code></pre>"},{"location":"python/best_practices/styleguide/#4-parting-words","title":"4 Parting Words","text":"<p>BE CONSISTENT.</p> <p>If you're editing code, take a few minutes to look at the code around you and determine its style. If they use spaces around all their arithmetic operators, you should too. If their comments have little boxes of hash marks around them, make your comments have little boxes of hash marks around them too.</p> <p>The point of having style guidelines is to have a common vocabulary of coding so people can concentrate on what you're saying rather than on how you're saying it. We present global style rules here so people know the vocabulary, but local style is also important. If code you add to a file looks drastically different from the existing code around it, it throws readers out of their rhythm when they go to read it. Avoid this.</p>"},{"location":"python/best_practices/zen_of_python/","title":"Zen of Python","text":"<p>Available in the shell</p> <p><code>import this</code></p> <ol> <li>Beautiful is better than ugly.</li> <li>Explicit is better than implicit.</li> <li>Simple is better than complex.</li> <li>Complex is better than complicated.</li> <li>Flat is better than nested.</li> <li>Sparse is better than dense.</li> <li>Readability counts.</li> <li>Special cases aren't special enough to break the rules.</li> <li>Although practicality beats purity.</li> <li>Errors should never pass silently.</li> <li>Unless explicitly silenced.</li> <li>In the face of ambiguity, refuse the temptation to guess.</li> <li>There should be one-- and preferably only one --obvious way to do it.</li> <li>Although that way may not be obvious at first unless you're Dutch.</li> <li>Now is better than never.</li> <li>Although never is often better than right now.</li> <li>If the implementation is hard to explain, it's a bad idea.</li> <li>If the implementation is easy to explain, it may be a good idea.</li> <li>Namespaces are one honking great idea -- let's do more of those!</li> </ol>"},{"location":"python/machine_learning/1_introduction/","title":"Introduction","text":""},{"location":"python/machine_learning/1_introduction/#what-is-it","title":"What is it?","text":"<ul> <li> <p>Subfield of Artificial Intelligence</p> </li> <li> <p>Systems to automatically learn and improve from experience, without being     explicitly programmed</p> </li> <li> <p>Algorithms (models) that can interpret and learn from complex data,     identify patterns, and make predictions or decisions based on it</p> </li> <li> <p>Usage: disease identification, financial projections, image recognition,     speech recognition, natural language processing, fraud detection, etc.</p> </li> </ul> <p> </p> Machine Learning Subfield <pre><code>flowchart LR\n    subgraph Traditional Programming\n        data--&gt;Machine\n        rules--&gt;Machine\n        Machine--&gt;output\n    end</code></pre> <pre><code>flowchart LR\n    subgraph Machine Learning\n        data--&gt;Machine\n        output--&gt;Machine\n        Machine--&gt;model\n    end</code></pre>"},{"location":"python/machine_learning/1_introduction/#types-of-machine-learning","title":"Types of Machine Learning","text":"<p>There are several machine learning algorithms that enables to build complex models. These algorithms can be grouped into a certain category depending on its learning process.</p> <ul> <li> <p>Supervised Learning: uses labeled data (expected output already known) to train the models.     The learning process finds the best way to map the inputs to the respective outputs.</p> </li> <li> <p>Unsupervised Learning: uses unlabeled data (doesn't include an output variable) to train the models.     The model discovers patterns and features in the input data.</p> </li> <li> <p>Semi-Supervised Learning: mix between supervised and unsupervised learning.     Only some of the ouput is known.</p> </li> <li> <p>Reinforcement Learning: follows trial and error to get the desired result.     Trains the machine to take the most suitable action at a given moment, and it learns from     the rewards.</p> </li> </ul> <pre><code>flowchart BT\n    A[Supervised Learning]---E[Machine Learning]\n    B[Unsupervised Learning]---E[Machine Learning]\n    C[Semi-Supervised Learning]---E[Machine Learning]\n    D[Reinforcement Learning]---E[Machine Learning]\n    F[Classification]---A[Supervised Learning]\n    G[Regression]---A[Supervised Learning]\n    H[Clustering]---B[Unsupervised Learning]</code></pre> <p>Within the same machine learning, we can also categorize the different problems, depending on what the machine learning algorithm is trying to predict.</p> <ul> <li>Classification: assign class labels to inputs (Ex: classify emails as spam or \"not spam\")</li> <li>Regression: assign numeric value to inputs (Ex: product price prediction)</li> <li>Clustering: divide input data into clusters (Ex: group together users with same patterns)</li> </ul> <p>          Classification and Regression. Adapted from \"Regression vs Classification in Machine Learning\".          Retrieved from here.     </p> <p>          Clustering. Adapted from \"Clustering in Machine Learning\" by Surya Priy.          Retrieved from here.     </p> <p>          Reinforcement. Adapted from \"Reinforcement Learning 101\" by Shweta Bhatt.          Retrieved from here.     </p>"},{"location":"python/machine_learning/1_introduction/#lifecycle","title":"Lifecycle","text":"<p>The Machine Learning lifecycle involves several steps. This is not characterized by being a linear process since it is common to jump to previous steps in the process, based on conclusions drawn in future phases. The major steps are the following:</p> <ol> <li> <p>Gathering data - identify the different data sources, data collection, data integration</p> </li> <li> <p>Data pre-processing - cleaning and converting raw data into a useable format    (Ex: missing values, duplicate records, invalid data, noise)</p> </li> <li> <p>Data Analysis - study the relationships contained in the different variables,    with vision in which could be a suitable model</p> </li> <li> <p>Model Selection - train and evaluate several machine learning algorithms/pipelines</p> </li> <li> <p>Test Model - final evaluation of the candidate model in a different dataset</p> </li> <li> <p>Deployment - deploy the model in the real world system! So it can be consumed by other applications</p> </li> </ol> <p>          Machine Learning Process. Adapted from \"Machine Learning Process And Scenarios\" by Akhil Mittal.         Retrieved from here.     </p>"},{"location":"python/machine_learning/1_introduction/#families-of-algorithms","title":"Families of Algorithms","text":"<p>Machine Learning algorithms can be classified into different families, depending on nature of the learning process. The four major families are the following:</p> <ul> <li>Information-based: concepts from Information Theory to train the models (Ex: Decision Trees)</li> <li>Similarity-based: measuring similarity between past and forthcoming occurrences (Ex: K-Nearest Neighbor)</li> <li>Probability-based: measuring how likely is that some event will occur (Ex: Bayesian Network)</li> <li>Error-based: minimizing the total error through a set of training instances (Ex: Linear Regression)</li> </ul>"},{"location":"python/machine_learning/2_regression/","title":"Regression","text":"<p>Regression, along with classification, are the most common machine learning techniques. The difference between them is that in regression the algorithms are used to predict continuous outcome. Regression machine learning models live in the group of supervised learning where the output variable (dependent variable) is known, and numeric. The goal is to understand and model the relationship between the dependent variable (what we want to predict), and one or more independent variables.</p> <pre><code>flowchart BT\n    A[Supervised Learning]---E[Machine Learning]\n    B[Unsupervised Learning]---E[Machine Learning]\n    C[Semi-Supervised Learning]---E[Machine Learning]\n    D[Reinforcement Learning]---E[Machine Learning]\n    F[Classification]---A[Supervised Learning]\n    G[Regression]---A[Supervised Learning]\n    H[Clustering]---B[Unsupervised Learning]\n    style G stroke:#f66,stroke-width:4px</code></pre>"},{"location":"python/machine_learning/2_regression/#linear-regression","title":"Linear Regression","text":"<ul> <li> <p>Statistical method used to model the relationship between a dependent variable and   one or more independent variables.</p> </li> <li> <p>Assumes that the relationship between the dependent variable and the   independent variable(s) is linear.</p> </li> </ul> <p>Training: find the line of best fit that minimizes the sum of squared differences between the predicted values and the actual values of the dependent variable.</p> <p>The equation of the hyperplane (n independent variables) is given by:</p> <p>y = b0 + b1X1 + b2X2 + ... + bn*Xn</p> <p>In the simplest form (only one independent variable), the equation is the same as the straight line equation.</p> <p>y = b0 + b1X1</p> <p>          Linear Regression Model. Adapted from \"Linear Regression in Machine Learning\u201d.          Retrieved from here.     </p> <p>Before using linear regression model, make sure that the data follow these assumptions:</p> <ol> <li> <p>The variables should be measured at a continuous level.</p> </li> <li> <p>Relationship between the dependent variable and the independent variable(s) is linear    (scatter plot to visualize).</p> </li> <li> <p>The observations and variables should be independent of each other.</p> </li> <li> <p>Your data should have no significant outliers.</p> </li> <li> <p>The residuals (errors) of the best-fit regression line follow normal distribution.</p> </li> </ol>"},{"location":"python/machine_learning/2_regression/#polynomial-regression","title":"Polynomial Regression","text":"<p>Polynomial Regression is very similar to Linear Regression. The only difference is that it transforms the input data to include non-linear terms. It is described by a degree, which is the highest power computed from the original input. The amount of additional terms will consequently depend on the degree.</p> <p>A dataset with a single feature X</p> <pre><code>X = [\n    [X1],\n    [X2],\n    [X3],\n]\n</code></pre> <p>using a polynomial of second degree would be transformed to:</p> <pre><code>X = [\n    [X1, X1\u00b2],\n    [X2, X2\u00b2],\n    [X3, X3\u00b2],\n]\n</code></pre> <p>          Linear vs Polynomial Regression. Adapted from \"Why -Polynomial Regression and not Linear Regression?\u201d by Tamil Selvi.          Retrieved from here.     </p> <p>The training process would be exactly the same: find the coefficients for each of the features that minimizes the square error. In the described example, since we would have 2 coefficients instead of 1, it would define a parable instead of a straight line.</p> <p>Key factors to consider:</p> <ul> <li>Include nonlinear terms such as \ud835\udc65\u00b2.</li> <li>Often used when the relationship between the variables cannot be accurately described by a linear model.</li> <li>Plot the data (Ex: scatter plot) and check if a linear model is appropriate.</li> </ul>"},{"location":"python/machine_learning/3_evaluation/","title":"Model Evaluation","text":"<p>Before deploying a model to be consumed by several applications, we need to know what is the expected performance that it will achieve. After training a model using the collected and transformed dataset, concrete metrics have to be calculated so we can have a vision of how well the algorithm can model the data that we have on hand.</p> <p>Depending on the type of the problem (supervised, unsupervised, classification, regression), there are different metrics to evaluate. In supervised learning, we have available target values that we can use to compare to what was predicted by the model. However, the metrics are different between classification and regression problems since in the first one the target variable is not continuous.</p> <p>Another topic worth mentioning, that can lead to misleading conclusions, is what data should be used to evaluate. If we train the model with the entire dataset, make predictions over the same dataset, and compare these predictions with the actual values, we are not verifying how the model behaves with unseen data samples. That's why before speaking about evaluation metrics, we have to jump into data split strategies, to be able to separate the data into different sets depending on their use.</p>"},{"location":"python/machine_learning/3_evaluation/#data-split-strategies","title":"Data Split Strategies","text":"<p>A data split strategy is a way of splitting the data into different subsets, so each subset is used with a different goal. This will prevent us from using the entire dataset for training, tuning , and evaluating which can give false conclusions.</p> <p>The most used strategies are:</p> <ul> <li>train-test split</li> <li>cross-validation</li> <li>leave one out</li> </ul>"},{"location":"python/machine_learning/3_evaluation/#train-test-split","title":"Train-test split","text":"<p>The idea behind the train-test split, as the name suggests, is to split the entire dataset in different sets, usually the following:</p> <ul> <li>training set (usually ~70%): data samples used to train the model</li> <li>validation set (usually ~15%): data samples used to tune the model   (hyperparameters, architecture)</li> <li>testing set (usually ~15%): data samples used to evaluate the model</li> </ul> <p>          Train-Test Split. Adapted from \"Train/Test Split and Cross Validation \u2013 A Python Tutorial\u201d by Greg Bland.          Retrieved from here.     </p>"},{"location":"python/machine_learning/3_evaluation/#cross-validation","title":"Cross-validation","text":"<p>Cross-validation, and leave one out are strategies used usually when the dataset size is not huge, since they are computationally expensive. The idea is to divide the entire dataset into K folds. The model will be trained using N - 1 folds and evaluated on the remaining fold. These will be performed K (number of folds) times, so the advantage is that the data coverage will be higher (more reliable evaluation).</p> <p>          Cross Validation. Adapted from \"Cross-validation: evaluating estimator performance\u201d.          Retrieved from here.     </p>"},{"location":"python/machine_learning/3_evaluation/#leave-one-out","title":"Leave one out","text":"<p>Leave one out is usually applied on small datasets. It is a particular case of cross-validation, it is equivalent to using cross-validation with N folds, given that N is the number of samples in the dataset. This means that the model will be trained and evaluated N times, and the prediction for evaluation will be performed on a single sample at a time.</p>"},{"location":"python/machine_learning/3_evaluation/#to-be-aware","title":"To be aware","text":"<p>It is worth mentioning two additional topics:</p> <ul> <li> <p>temporal data: when dealing with temporal data, it is   important to sort before applying the split, because   it is considered \"cheating\" if the sort operation   is not applied. Imagine having past and future samples   in the training set of a sample in the test set. This   would not reflect the real usage of the model;</p> </li> <li> <p>stratification: in classification problems, it is important   to maintain the class distribution along the different sets,   so we can avoid having, for example, a test set with samples   of a single class.</p> </li> </ul>"},{"location":"python/machine_learning/3_evaluation/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>Evaluation metrics can be divided into regression and classification since the output is different depending on the type of the problem. However, the goal of these metrics is to evaluate how well the model can make predictions. It receives as input the actuals (y) and the predicted values for the same samples (\u0177).</p>"},{"location":"python/machine_learning/3_evaluation/#regression","title":"Regression","text":"<p>In regression, the actuals and the predicted values are continuous. The following metrics are usually used to evaluate this kind of models:</p> <ul> <li>Mean Absolute Error (MAE)</li> </ul> <p>MAE is the easiest metric to interpret. It represents how much the predicted value deviates from the actual value. n is the number of samples (size of y and \u0177).</p> <p><code>MAE = (1/n) * \u03a3|yi - \u0177i|</code></p> <ul> <li>Mean Square Error (MSE)</li> </ul> <p>MSE is very similar to MAE. The difference is that it is squared instead of using the absolute difference. This will penalize higher errors. It is usually used as a loss function in some of the models (i.e Neural Network).</p> <p><code>MSE = (1/n) * \u03a3(yi - \u0177i)\u00b2</code></p> <ul> <li>Root Mean Square Error (RMSE)</li> </ul> <p>RMSE is the root of MSE. This facilitates the interpretation since it converts the unit from squared to the original unit.</p> <p><code>RMSE = sqrt((1/n) * \u03a3(yi - \u0177i)\u00b2)</code></p> <ul> <li>R-Squared Score (R2)</li> </ul> <p>R2 is a metric that represents how much variance of the data was successfully modeled. It indicates how well the model could represent the data and can establish a relationship between the independent variables and the dependent variable.</p> <p>SSres is the residual sum of squares, and SStot is the total variance contained in the target output y. This metric is within the range of 0.0 to 1.0, being 1.0 a perfect model, able to represent the entire variance. It can also have negative values in cases where there is a terrible relationship between the target and independent variables.</p> <p><code>R2 = 1 - (SSres / SStot)</code></p>"},{"location":"python/machine_learning/3_evaluation/#useful-plots","title":"Useful Plots","text":"<p>Visualization sometimes can help taking conclusions about the obtained results. The listed metrics in the previous section are good to have a clear and specific evaluation of the model. However, some useful plots can be visualized to complement the achieved numeric results.</p>"},{"location":"python/machine_learning/3_evaluation/#regression_1","title":"Regression","text":"<p>The first plot is the actuals vs predicted values. Ideally, all the points should be laid in the diagonal line, which represents that the predicted value is equal to the actual value. This plot gives an overall idea of the achieved predictions and how far they are from the actual value.</p> <p>          Actual vs Predicted plot.          Retrieved from here.     </p> <p>The second one is similar, however, the residual value substitutes one of the previous variables (actuals or predictions). Ideally, the points should be near 0.0 which means no error. The goal of this plot is to conclude the error for each range of values and visualize possible trends that the errors may show. This indicates that there is some pattern contained in the data that the model was not able to cover.</p> <p>          Residuals vs Predicted plot. Adapted from \"Residual plot\".         Retrieved from here.     </p> <p>The last one is the histogram of residuals. This allows the conclusion of where are most errors contained. Additionally, it is also important to conclude the distribution of the errors. For example, a Linear Regression assumes that the errors are normally distributed.</p> <p>          Residuals Histogram. Adapted from \"Residual Plot Analysis\u201d.          Retrieved from here.     </p>"},{"location":"python/machine_learning/4_regression_first_contact/","title":"Hands on Regression - First Contact","text":"In\u00a0[\u00a0]: Copied! <pre>from sklearn.datasets import make_regression\n\nX, y = make_regression(n_samples=100, n_features=1, noise=1, random_state=0)\n\nX[:5], y[:5]\n</pre> from sklearn.datasets import make_regression  X, y = make_regression(n_samples=100, n_features=1, noise=1, random_state=0)  X[:5], y[:5] Out[\u00a0]: <pre>(array([[-0.35955316],\n        [ 0.97663904],\n        [ 0.40234164],\n        [-0.81314628],\n        [-0.88778575]]),\n array([-15.71144661,  39.38978203,  16.50379768, -32.65326103,\n        -37.43638625]))</pre> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nplt.scatter(X.flatten(), y, alpha=0.7)\nplt.title(\"Independent/Dependent Variables Relationship\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\n</pre> import matplotlib.pyplot as plt  plt.scatter(X.flatten(), y, alpha=0.7) plt.title(\"Independent/Dependent Variables Relationship\") plt.xlabel(\"X\") plt.ylabel(\"Y\") Out[\u00a0]: <pre>Text(0, 0.5, 'Y')</pre> In\u00a0[\u00a0]: Copied! <pre>from sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\nmodel.fit(X, y)\n\nmodel.coef_, model.intercept_\n</pre> from sklearn.linear_model import LinearRegression  model = LinearRegression() model.fit(X, y)  model.coef_, model.intercept_ Out[\u00a0]: <pre>(array([42.4088974]), -0.081418182703072)</pre> In\u00a0[\u00a0]: Copied! <pre>y_pred = model.predict(X)\n\nplt.scatter(X.flatten(), y, alpha=0.7)\nplt.title(\"Line Generated in the Training Process\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\n\nplt.plot(X, y_pred, color=\"tab:orange\")\n</pre> y_pred = model.predict(X)  plt.scatter(X.flatten(), y, alpha=0.7) plt.title(\"Line Generated in the Training Process\") plt.xlabel(\"X\") plt.ylabel(\"Y\")  plt.plot(X, y_pred, color=\"tab:orange\") Out[\u00a0]: <pre>[&lt;matplotlib.lines.Line2D at 0x7f09493d3430&gt;]</pre> In\u00a0[\u00a0]: Copied! <pre>from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=0\n)\n\nX_train.shape, X_test.shape\n</pre> from sklearn.model_selection import train_test_split  X_train, X_test, y_train, y_test = train_test_split(     X, y, test_size=0.2, random_state=0 )  X_train.shape, X_test.shape Out[\u00a0]: <pre>((80, 1), (20, 1))</pre> In\u00a0[\u00a0]: Copied! <pre>from sklearn.model_selection import KFold\n\nkfold = KFold(n_splits=5)\n\nfor i, (train_indexes, test_indexes) in enumerate(kfold.split(X)):\n    print(f\"Fold {i}\")\n    print(f\"\\t Test: {test_indexes}\")\n</pre> from sklearn.model_selection import KFold  kfold = KFold(n_splits=5)  for i, (train_indexes, test_indexes) in enumerate(kfold.split(X)):     print(f\"Fold {i}\")     print(f\"\\t Test: {test_indexes}\")  <pre>Fold 0\n\t Test: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\nFold 1\n\t Test: [20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39]\nFold 2\n\t Test: [40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59]\nFold 3\n\t Test: [60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79]\nFold 4\n\t Test: [80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99]\n</pre> In\u00a0[\u00a0]: Copied! <pre>from sklearn.model_selection import LeaveOneOut\n\nloo = LeaveOneOut()\nfor train_indexes, test_indexes in loo.split(X):\n    print(f\"Test: {test_indexes}\")\n\n    if test_indexes[0] &gt; 19:\n      print(\"...\")\n      break\n</pre> from sklearn.model_selection import LeaveOneOut  loo = LeaveOneOut() for train_indexes, test_indexes in loo.split(X):     print(f\"Test: {test_indexes}\")      if test_indexes[0] &gt; 19:       print(\"...\")       break  <pre>Test: [0]\nTest: [1]\nTest: [2]\nTest: [3]\nTest: [4]\nTest: [5]\nTest: [6]\nTest: [7]\nTest: [8]\nTest: [9]\nTest: [10]\nTest: [11]\nTest: [12]\nTest: [13]\nTest: [14]\nTest: [15]\nTest: [16]\nTest: [17]\nTest: [18]\nTest: [19]\nTest: [20]\n...\n</pre> In\u00a0[\u00a0]: Copied! <pre>from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\nkfold = KFold(n_splits=5)\n\nfor i, (train_indexes, test_indexes) in enumerate(kfold.split(X)):\n    X_train = X[train_indexes]\n    X_test = X[test_indexes]\n    y_train = y[train_indexes]\n    y_test = y[test_indexes]\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    mae = mean_absolute_error(y_test, y_pred)\n    mse = mean_squared_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n    rmse = mse ** 0.5\n\n    print(f\"Fold {i}\")\n    print(f\"\\t MAE={mae}\")\n    print(f\"\\t MSE={mse}\")\n    print(f\"\\t R2={r2}\")\n    print(f\"\\t RMSE={rmse}\")\n</pre> from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score  kfold = KFold(n_splits=5)  for i, (train_indexes, test_indexes) in enumerate(kfold.split(X)):     X_train = X[train_indexes]     X_test = X[test_indexes]     y_train = y[train_indexes]     y_test = y[test_indexes]      model = LinearRegression()     model.fit(X_train, y_train)      y_pred = model.predict(X_test)     mae = mean_absolute_error(y_test, y_pred)     mse = mean_squared_error(y_test, y_pred)     r2 = r2_score(y_test, y_pred)     rmse = mse ** 0.5      print(f\"Fold {i}\")     print(f\"\\t MAE={mae}\")     print(f\"\\t MSE={mse}\")     print(f\"\\t R2={r2}\")     print(f\"\\t RMSE={rmse}\")  <pre>Fold 0\n\t MAE=0.8621552298978855\n\t MSE=1.0962047425997308\n\t R2=0.9991740317272837\n\t RMSE=1.046997966855586\nFold 1\n\t MAE=1.1204936504484526\n\t MSE=1.7660868415050117\n\t R2=0.9987579014443851\n\t RMSE=1.3289420008055324\nFold 2\n\t MAE=1.0036298987412464\n\t MSE=1.5132931891492074\n\t R2=0.9993053057530135\n\t RMSE=1.2301598226040418\nFold 3\n\t MAE=0.8524516012025206\n\t MSE=1.0506789465771977\n\t R2=0.9996546320170226\n\t RMSE=1.0250263150657146\nFold 4\n\t MAE=0.703414249405674\n\t MSE=0.7691013896506624\n\t R2=0.9990726778559803\n\t RMSE=0.8769842584965037\n</pre> In\u00a0[\u00a0]: Copied! <pre>from sklearn.model_selection import cross_val_predict\n\nmodel = LinearRegression()\ny_pred = cross_val_predict(model, X, y, cv=5)\n\nmae = mean_absolute_error(y, y_pred)\nmse = mean_squared_error(y, y_pred)\nr2 = r2_score(y, y_pred)\nrmse = mse ** 0.5\n\nprint(f\"MAE={mae}\")\nprint(f\"MSE={mse}\")\nprint(f\"R2={r2}\")\nprint(f\"RMSE={rmse}\")\n</pre> from sklearn.model_selection import cross_val_predict  model = LinearRegression() y_pred = cross_val_predict(model, X, y, cv=5)  mae = mean_absolute_error(y, y_pred) mse = mean_squared_error(y, y_pred) r2 = r2_score(y, y_pred) rmse = mse ** 0.5  print(f\"MAE={mae}\") print(f\"MSE={mse}\") print(f\"R2={r2}\") print(f\"RMSE={rmse}\") <pre>MAE=0.9084289259391557\nMSE=1.2390730218963621\nR2=0.9993222148719967\nRMSE=1.1131365692925383\n</pre> In\u00a0[\u00a0]: Copied! <pre>plt.scatter(y_pred, y, alpha=0.5)\nplt.title(\"Actuals vs Predicted\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.gca().axline((0, 0), slope=1, color=\"tab:gray\", linestyle=\"--\")\n</pre> plt.scatter(y_pred, y, alpha=0.5) plt.title(\"Actuals vs Predicted\") plt.xlabel(\"Predicted\") plt.ylabel(\"Actual\") plt.gca().axline((0, 0), slope=1, color=\"tab:gray\", linestyle=\"--\") Out[\u00a0]: <pre>&lt;matplotlib.lines._AxLine at 0x7f094960b670&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>residuals = y - y_pred\n\nplt.hist(residuals)\nplt.title(\"Residuals Histogram\")\nplt.xlabel(\"Residual\")\nplt.ylabel(\"Count\")\n</pre> residuals = y - y_pred  plt.hist(residuals) plt.title(\"Residuals Histogram\") plt.xlabel(\"Residual\") plt.ylabel(\"Count\") Out[\u00a0]: <pre>Text(0, 0.5, 'Count')</pre> In\u00a0[\u00a0]: Copied! <pre>plt.scatter(y, residuals, alpha=0.5)\nplt.title(\"Residuals vs Actuals\")\nplt.xlabel(\"Actual\")\nplt.ylabel(\"Residual\")\nplt.gca().axline((0, 0), slope=0, color=\"tab:gray\", linestyle=\"--\")\n</pre> plt.scatter(y, residuals, alpha=0.5) plt.title(\"Residuals vs Actuals\") plt.xlabel(\"Actual\") plt.ylabel(\"Residual\") plt.gca().axline((0, 0), slope=0, color=\"tab:gray\", linestyle=\"--\") Out[\u00a0]: <pre>&lt;matplotlib.lines._AxLine at 0x7f09490a3a30&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>from sklearn.preprocessing import PolynomialFeatures\n\npoly = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly.fit_transform(X)\ny_squared = y ** 2\n\nX[:5], X_poly[:5]\n</pre> from sklearn.preprocessing import PolynomialFeatures  poly = PolynomialFeatures(degree=2, include_bias=False) X_poly = poly.fit_transform(X) y_squared = y ** 2  X[:5], X_poly[:5] Out[\u00a0]: <pre>(array([[-0.35955316],\n        [ 0.97663904],\n        [ 0.40234164],\n        [-0.81314628],\n        [-0.88778575]]),\n array([[-0.35955316,  0.12927848],\n        [ 0.97663904,  0.95382381],\n        [ 0.40234164,  0.1618788 ],\n        [-0.81314628,  0.66120688],\n        [-0.88778575,  0.78816353]]))</pre> In\u00a0[\u00a0]: Copied! <pre>linear_model = LinearRegression()\nlinear_model.fit(X, y_squared)\n\npoly_model = LinearRegression()\npoly_model.fit(X_poly, y_squared)\n</pre> linear_model = LinearRegression() linear_model.fit(X, y_squared)  poly_model = LinearRegression() poly_model.fit(X_poly, y_squared) Out[\u00a0]: <pre>LinearRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression<pre>LinearRegression()</pre> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n\ny_pred_linear = linear_model.predict(X)\ny_pred_poly = poly_model.predict(X_poly)\n\nsorted_indexes = np.argsort(X.flatten())\nX_sorted = X.flatten()[sorted_indexes]\n\nplt.scatter(X_sorted, y_squared[sorted_indexes], alpha=0.5, label=\"actuals\")\nplt.title(\"Linear vs Polynomial\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\n\nplt.plot(\n    X_sorted, \n    y_pred_linear[sorted_indexes], \n    \"--\", \n    color=\"tab:orange\", \n    label=\"linear model\",\n)\nplt.plot(\n    X_sorted, \n    y_pred_poly[sorted_indexes], \n    \"--\", \n    color=\"tab:green\", \n    label=\"polynomial model\",\n)\nplt.legend()\n</pre> import numpy as np  y_pred_linear = linear_model.predict(X) y_pred_poly = poly_model.predict(X_poly)  sorted_indexes = np.argsort(X.flatten()) X_sorted = X.flatten()[sorted_indexes]  plt.scatter(X_sorted, y_squared[sorted_indexes], alpha=0.5, label=\"actuals\") plt.title(\"Linear vs Polynomial\") plt.xlabel(\"X\") plt.ylabel(\"Y\")  plt.plot(     X_sorted,      y_pred_linear[sorted_indexes],      \"--\",      color=\"tab:orange\",      label=\"linear model\", ) plt.plot(     X_sorted,      y_pred_poly[sorted_indexes],      \"--\",      color=\"tab:green\",      label=\"polynomial model\", ) plt.legend() Out[\u00a0]: <pre>&lt;matplotlib.legend.Legend at 0x7f0948edee50&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>y_pred = cross_val_predict(linear_model, X, y_squared, cv=5)\n\nmae = mean_absolute_error(y_squared, y_pred)\nmse = mean_squared_error(y_squared, y_pred)\nr2 = r2_score(y_squared, y_pred)\nrmse = mse ** 0.5\n\nprint(\"Linear Model Evaluation:\")\nprint(f\"MAE={mae}\")\nprint(f\"MSE={mse}\")\nprint(f\"R2={r2}\")\nprint(f\"RMSE={rmse}\")\n</pre> y_pred = cross_val_predict(linear_model, X, y_squared, cv=5)  mae = mean_absolute_error(y_squared, y_pred) mse = mean_squared_error(y_squared, y_pred) r2 = r2_score(y_squared, y_pred) rmse = mse ** 0.5  print(\"Linear Model Evaluation:\") print(f\"MAE={mae}\") print(f\"MSE={mse}\") print(f\"R2={r2}\") print(f\"RMSE={rmse}\") <pre>Linear Model Evaluation:\nMAE=1812.4334097918857\nMSE=5966835.355196196\nR2=-0.0863241551310745\nRMSE=2442.7106572814137\n</pre> In\u00a0[\u00a0]: Copied! <pre>y_pred = cross_val_predict(model, X_poly, y_squared, cv=5)\n\nmae = mean_absolute_error(y_squared, y_pred)\nmse = mean_squared_error(y_squared, y_pred)\nr2 = r2_score(y_squared, y_pred)\nrmse = mse ** 0.5\n\nprint(\"Polynomial Model Evaluation:\")\nprint(f\"MAE={mae}\")\nprint(f\"MSE={mse}\")\nprint(f\"R2={r2}\")\nprint(f\"RMSE={rmse}\")\n</pre> y_pred = cross_val_predict(model, X_poly, y_squared, cv=5)  mae = mean_absolute_error(y_squared, y_pred) mse = mean_squared_error(y_squared, y_pred) r2 = r2_score(y_squared, y_pred) rmse = mse ** 0.5  print(\"Polynomial Model Evaluation:\") print(f\"MAE={mae}\") print(f\"MSE={mse}\") print(f\"R2={r2}\") print(f\"RMSE={rmse}\") <pre>Polynomial Model Evaluation:\nMAE=58.4698071861382\nMSE=7429.651964598892\nR2=0.9986473549355888\nRMSE=86.19542890779587\n</pre>"},{"location":"python/machine_learning/4_regression_first_contact/#hands-on-regression-first-contact","title":"Hands on Regression - First Contact\u00b6","text":"<p>This notebook is intended to introduce the problem of regression. It includes familiarization with a linear and polynomial regression model, as well as evaluation techniques and metrics.</p> <p>https://docs.google.com/presentation/d/1VCNsLJbL_c8IM30P4sj0hayBbk_j8_OVlyoMnHtIX54/edit?usp=sharing</p>"},{"location":"python/machine_learning/4_regression_first_contact/#part-1-load-and-visualize-dummy-data","title":"Part 1 - Load and Visualize dummy data\u00b6","text":"<p>In this first part, the goal is to generate data that is friendly to visualize and understand. For this purpose, the <code>make_regression</code> function is used, in order to generate instances (independent variables) associated with a dependent variable (what is intended to be predicted). Additionally, we visualize the generated data in order to see this same relationship.</p> <p>https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html</p> <p>https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html</p>"},{"location":"python/machine_learning/4_regression_first_contact/#part-2-data-split-strategies","title":"Part 2 - Data Split Strategies\u00b6","text":"<p>The data is usually divided into different subsets, each of which has its own particular purpose: training the model, fine-tuning possible hyper-parameters, testing and evaluating the model. There are several strategies to do this: random split, cross validation and leave one out. In this part, the goal is to apply this same division in the different forms for the purposes of illustration.</p> <p>https://scikit-learn.org/stable/modules/cross_validation.html</p>"},{"location":"python/machine_learning/4_regression_first_contact/#part-3-evaluation-metrics","title":"Part 3 - Evaluation Metrics\u00b6","text":"<p>What is the performance of the model? How good the model can make predictions? These are valid questions that have to be answer before deploying a model, and that are part of the model selection phase. To do so, clear metrics are calculated to evaluate the model to take conclusions about it. Some of the most used regression metrics include: Mean Absolute Error (MAE), Mean Square Error (MSE), Root Mean Square Error (RMSE), and R-Squared Score (R2). In this third part, we apply a cross validation strategy and the listed metrics are calculated.</p> <p>https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics</p>"},{"location":"python/machine_learning/4_regression_first_contact/#part-4-useful-plots","title":"Part 4 - Useful Plots\u00b6","text":"<p>Visualization can be useful and complementary to understand the results. In addition to calculated metrics, it can also be beneficial to observe the following plots: predicted vs actuals, residuals histogram, and actuals vs residuals.</p>"},{"location":"python/machine_learning/4_regression_first_contact/#part-5-polynomial-regression","title":"Part 5 - Polynomial Regression\u00b6","text":"<p>The relationship between the independent variables and the dependent variable is not always linear. In these cases, including polynomial terms may improve the performance. In this part, we will apply a polynomial regression and compare the results with was obtained in the previous sections.</p> <p>https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html</p>"},{"location":"python/machine_learning/synthethic_data/1_synthetic_data/","title":"Introduction","text":"<p>In this section we'll explore the generation of synthetic data using SDV library. The SDV offers multiple machine learning models ranging from classical statistical methods (Copulas) to deep learning methods (GANs). Synthetic Data is very important for a number of reasons:</p> <ul> <li>Software Testing</li> <li>Access Expansion</li> <li>Pilot New Products</li> <li>Augmented Data</li> <li>Plan scenarios</li> </ul>"},{"location":"python/machine_learning/synthethic_data/1_synthetic_data/#personal-identifiable-information-pii","title":"Personal Identifiable Information (PII)","text":"<p>One main issue with data is its sensitivity. So we can define data columns as PII - Personal Identifiable Information - or non-PII. According to the University of Pittsburgh:</p> <p>Personally Identifiable Information (PII) includes:</p> <ol> <li>Any information that can be used to distinguish or trace an individual\u2019s identity, such as name, social security number, date and place of birth, mother\u2019s maiden name, or biometric records; and</li> <li>Any other information that is linked or linkable to an individual, such as medical, educational, financial, and employment information.</li> </ol> <p>Examples of PII include, but are not limited to:</p> <ul> <li>Name: full name, maiden name, mother\u2019s maiden name, or alias</li> <li>Personal identification numbers: social security number (SSN), passport number, driver\u2019s license number, taxpayer identification number, patient identification number, financial account number, or credit card number</li> <li>Personal address information: street address, or email address</li> <li>Personal telephone numbers</li> <li>Personal characteristics: photographic images (particularly of face or other identifying characteristics), fingerprints, or handwriting</li> <li>Biometric data: retina scans, voice signatures, or facial geometry</li> <li>Information identifying personally owned property: VIN number or title number</li> <li>Asset information: Internet Protocol (IP) or Media Access Control (MAC) addresses that consistently link to a particular person</li> </ul> <p>Source</p>"},{"location":"python/machine_learning/synthethic_data/1_synthetic_data/#requirements","title":"Requirements","text":"<p>To install the SDV library we should be working with Python &gt;= 3.10 and &lt; 3.11. For the analysis done on this library we used Python 3.10.9. <pre><code>pip install sdv\n</code></pre></p>"},{"location":"python/machine_learning/synthethic_data/1_synthetic_data/#flow","title":"Flow","text":"<p>Usually the flow of generating synthetic data is the following: <pre><code>flowchart TD\n    subgraph Synthetic Data Flow\n        ld[Load Data]\n        cm[Create Metadata]\n        em[Edit Metadata]\n        cs[Create Synthesizer]\n        ts[Train Synthesizer]\n        gsd[Generate Synthetic Data]\n        ld--&gt;cm\n        cm--&gt;em\n        em--&gt;cs\n        cs--&gt;ts\n        ts--&gt;gsd\n    end</code></pre></p>"},{"location":"python/machine_learning/synthethic_data/1_synthetic_data/#load-data","title":"Load Data","text":"<p>At first we need to load the data. If it's stored in a csv format we can use the built-in load_csvs method. This method reads all the csv's available in that particular folder.</p> <pre><code>from sdv.datasets.local import load_csvs\n\n# assume that my_folder contains 1 CSV file named 'guests.csv'\ndatasets = load_csvs(folder_name='my_folder/')\n</code></pre> <p>Since SDV uses Pandas' dataframe under the hood, we can use it directly to load the data.</p> <pre><code>import pandas as pd\n\ndata = pd.read_excel('file://localhost/path/to/table.xlsx')\n</code></pre>"},{"location":"python/machine_learning/synthethic_data/1_synthetic_data/#create-metadata","title":"Create Metadata","text":"<p>Metadata is an object which contains the skeleton of our data, mainly types of columns, keys, etc. On a second step we should create this metadata based on the data loaded previously. For this we have a method in SDV library called detect_from_dataframe.  At this stage we must tell SDV whether we are trying to generate a multi table or single table metadata.  <pre><code>from sdv.metadata import SingleTableMetadata\n\nmetadata = SingleTableMetadata()\nmetadata.detect_from_dataframe(data=data)\n</code></pre></p>"},{"location":"python/machine_learning/synthethic_data/1_synthetic_data/#edit-metadata","title":"Edit Metadata","text":"<p>After this, a metadata object is created. It's strongly advised that this object should be checked and edited if need be. Usually the auto detection only gets the types and some may be wrong. For example, if a column flag has the values 0 or 1 it will say it is numerical even though the correct type is most probably boolean. To edit we should use the methods update_column, set_primary_key and add_alternate_keys.</p> <p>For these methods we have a parameter called sdtype which sets the type of the column. These types are provided from the Faker Python Library. The most common ones are the following:</p> <ul> <li>Boolean: Sdtype boolean describes columns that contain TRUE or FALSE values and may contain some missing data.</li> <li>Categorical: Sdtype categorical describes columns that contain distinct categories. The defining aspect of a categorical column is that only the values that appear in the real data are valid. The categories may be ordered or unordered.</li> <li>Datetime: Sdtype datetime describes columns that indicate a point of time. This can be at any granularity: to the nearest day, minute, second or even nanosecond. Typically, the datetime will be represented as a string.</li> <li>Numerical: Sdtype numerical describes data with numbers. The defining aspect of numerical data is that there is an order and you can apply a variety of mathematical computations to the values (average, sum, etc.) The actual values may follow a specific format, such as being rounded to 2 decimal digits and remaining between min/max bounds.</li> <li>ID: Sdtype id describes columns that are used to identify rows (eg. as a primary or foreign key). ID columns do not have any other mathematical or special meanings. Typically, an ID column follows a particular structure, for example being exactly 8 digits long with a - in the middle. </li> </ul> <p>Also, note that this object can be saved or loaded locally.</p> <pre><code>metadata.update_column(\n    column_name='room_type',\n    sdtype='categorical')\n\nmetadata.set_primary_key(column_name='guest_email')\n\nmetadata.add_alternate_keys(column_names=['credit_card_number'])\n\nmetadata.save_to_json(filepath='my_metadata_v1.json')\n\n# Needs to import SingleTableMetadata\nmetadata_obj = SingleTableMetadata.load_from_dict(metadata_dict)\n</code></pre>"},{"location":"python/machine_learning/synthethic_data/1_synthetic_data/#create-synthesizer","title":"Create Synthesizer","text":"<p>The synthesizer is the tool that uses machine learning to understand your data and create synthetic data based on it. There are several different models of synthesizers. For single tables we have:</p> <ul> <li>GaussianCopulaSynthesizer</li> <li>Fast ML Preset which is a preset that uses the GaussianCopulaSynthesizer in background.</li> <li>CTGANSynthesizer</li> <li>TVAESynthesizer</li> <li>CopulaGANSynthesizer</li> </ul> <p>For multi tables we have: - HMASynthesizer   (note that we can set the synthesizer used for each table on a multitable synthesizer)</p> <pre><code>from sdv.single_table import GaussianCopulaSynthesizer\n\nsynthesizer = GaussianCouplaSynthesizer(metadata)\n</code></pre>"},{"location":"python/machine_learning/synthethic_data/1_synthetic_data/#train-synthesizer","title":"Train Synthesizer","text":"<p>To train the synthesizer we use the method fit. <pre><code>synthesizer.fit(real_data)\n</code></pre> You can also save and load the synthesizer.</p>"},{"location":"python/machine_learning/synthethic_data/1_synthetic_data/#generate-synthetic-data","title":"Generate Synthetic Data","text":"<p>To generate synthetic data with our synthesizer we should use the method sample. <pre><code>synthetic_data = synthesizer.sample(num_rows=100)\n</code></pre> It's also possible to generate conditional synthetic data. <pre><code>from sdv.sampling import Condition\n\nsuite_guests_with_rewards = Condition(\n    num_rows=250,\n    column_values={'room_type': 'SUITE', 'has_rewards': True}\n)\n\nsuite_guests_without_rewards = Condition(\n    num_rows=250,\n    column_values={'room_type': 'SUITE', 'has_rewards': False}\n)\n\nsynthetic_data = custom_synthesizer.sample_from_conditions(\n    conditions=[suite_guests_with_rewards, suite_guests_without_rewards],\n    output_file_path='synthetic_simulated_scenario.csv'\n)\n</code></pre></p> <p>Now that we know the flow of usage of the SDV library, it's important to mention that all of these steps have more options to them not shown here. For those you can check the official documentation.</p>"},{"location":"python/machine_learning/synthethic_data/1_synthetic_data/#evaluation","title":"Evaluation","text":"<p>One other tool that the SDV library provides is an evaluation module which we can use to compare the newly synthetic data with the real data. This is very helpful in order to check the quality of our synthetic data and decide whether to use it or not.</p>"},{"location":"python/machine_learning/synthethic_data/1_synthetic_data/#metrics","title":"Metrics","text":"<p>To study the performance of this library methods (fit - to train - and sample - to create data) we used a Jupyter Notebook on VSCode using a M1 MacBook Air with 16GB of RAM. The results are the following:</p> Synthesizer model Method Num of rows used/generated Time Gaussian - FastML fit 5000 0.1s Gaussian - FastML sample 100000 0.9s Gaussian - FastML sample 1000000 8.0s Gaussian fit 5000 1.2s Gaussian sample 100000 1.9s Gaussian sample 1000000 17.4s CTGAN fit 5000 1min 21s CTGAN sample 100000 2.3s CTGAN sample 1000000 22.1s TVAE fit 5000 28.1s TVAE sample 100000 1.3s TVAE sample 1000000 13s CopulaGAN fit 5000 1min 31s CopulaGAN sample 100000 2.8s CopulaGAN sample 1000000 28.3s"},{"location":"python/machine_learning/synthethic_data/1_synthetic_data/#notebooks","title":"Notebooks","text":"<p>There are 2 notebooks available that showcase the functionality of the SDV library for the single table and multi table workflow.</p>"},{"location":"python/machine_learning/synthethic_data/2_single_table/","title":"Single Table Synthetic Data","text":"In\u00a0[1]: Copied! <pre>from sdv.datasets.local import load_csvs\n\ntry:\n  datasets = load_csvs(folder_name='content/')\nexcept ValueError:\n  print('You have not uploaded any csv files. Using some demo data instead.')\n</pre> from sdv.datasets.local import load_csvs  try:   datasets = load_csvs(folder_name='content/') except ValueError:   print('You have not uploaded any csv files. Using some demo data instead.') <p>Then, we access the strokes table and display the first 20 rows.</p> In\u00a0[2]: Copied! <pre>print(datasets.keys())\n\nstrokes_table = datasets['strokes']\nstrokes_table.head(20)\n</pre> print(datasets.keys())  strokes_table = datasets['strokes'] strokes_table.head(20) <pre>dict_keys(['people', 'strokes'])\n</pre> Out[2]: id person_id gender age hypertension heart_disease ever_married work_type Residence_type avg_glucose_level bmi smoking_status stroke 0 4799 3205 Female 79 0 0 Yes Self-employed Urban 79.03 11.3 Unknown 0 1 4798 59993 Male 40 0 0 Yes Private Rural 60.96 11.5 never smoked 0 2 4797 20364 Female 4 0 0 No children Urban 107.25 12.0 Unknown 0 3 4796 45893 Female 8 0 0 No children Urban 106.51 12.3 Unknown 0 4 4795 52859 Female 4 0 0 No children Urban 61.54 13.2 Unknown 0 5 4794 4789 Male 8 0 0 No children Rural 91.54 13.4 Unknown 0 6 4793 25391 Female 10 0 0 No children Rural 69.84 13.7 Unknown 0 7 4792 48435 Female 2 0 0 No children Rural 155.14 13.7 Unknown 0 8 4791 60926 Male 5 0 0 No children Urban 79.89 13.8 Unknown 0 9 4790 6107 Female 5 0 0 No children Urban 77.88 13.8 Unknown 0 10 4789 24736 Female 4 0 0 No children Urban 94.27 14.0 Unknown 0 11 4788 28309 Female 67 0 0 Yes Private Urban 82.09 14.1 never smoked 0 12 4787 32560 Female 8 0 0 No children Rural 87.92 14.1 Unknown 0 13 4786 52447 Female 3 0 0 No children Rural 131.81 14.1 Unknown 0 14 4785 59762 Male 61 0 0 Yes Private Urban 227.98 14.2 Unknown 0 15 4784 18352 Female 3 0 0 No children Rural 108.32 14.2 Unknown 0 16 4783 72701 Male 2 0 0 No children Rural 112.66 14.2 Unknown 0 17 4782 51162 Female 11 0 0 No children Rural 122.75 14.3 Unknown 0 18 4781 33876 Male 10 0 0 No children Urban 87.09 14.3 Unknown 0 19 4780 61672 Female 11 0 0 No children Urban 69.68 14.4 Unknown 0 In\u00a0[4]: Copied! <pre>from sdv.metadata import SingleTableMetadata\n\nmetadata = SingleTableMetadata()\nmetadata.detect_from_dataframe(\n    data=strokes_table\n)\n\nprint('Auto detected data:\\n')\nprint(metadata)\n</pre> from sdv.metadata import SingleTableMetadata  metadata = SingleTableMetadata() metadata.detect_from_dataframe(     data=strokes_table )  print('Auto detected data:\\n') print(metadata) <pre>Auto detected data:\n\n{\n    \"METADATA_SPEC_VERSION\": \"SINGLE_TABLE_V1\",\n    \"columns\": {\n        \"id\": {\n            \"sdtype\": \"numerical\"\n        },\n        \"person_id\": {\n            \"sdtype\": \"numerical\"\n        },\n        \"gender\": {\n            \"sdtype\": \"categorical\"\n        },\n        \"age\": {\n            \"sdtype\": \"numerical\"\n        },\n        \"hypertension\": {\n            \"sdtype\": \"numerical\"\n        },\n        \"heart_disease\": {\n            \"sdtype\": \"numerical\"\n        },\n        \"ever_married\": {\n            \"sdtype\": \"categorical\"\n        },\n        \"work_type\": {\n            \"sdtype\": \"categorical\"\n        },\n        \"Residence_type\": {\n            \"sdtype\": \"categorical\"\n        },\n        \"avg_glucose_level\": {\n            \"sdtype\": \"numerical\"\n        },\n        \"bmi\": {\n            \"sdtype\": \"numerical\"\n        },\n        \"smoking_status\": {\n            \"sdtype\": \"categorical\"\n        },\n        \"stroke\": {\n            \"sdtype\": \"numerical\"\n        }\n    }\n}\n</pre> In\u00a0[5]: Copied! <pre>metadata.update_column(\n    column_name='id',\n    sdtype='id'\n)\n\nmetadata.update_column(\n    column_name='age',\n    sdtype='numerical',\n    computer_representation=\"Int64\"\n)\n\nmetadata.update_column(\n    column_name='bmi',\n    sdtype='numerical',\n    computer_representation=\"Float\"\n)\n\nmetadata.update_column(\n    column_name='stroke',\n    sdtype='categorical',\n)\n\nmetadata.update_column(\n    column_name='hypertension',\n    sdtype='categorical',\n)\n\nmetadata.update_column(\n    column_name='heart_disease',\n    sdtype='categorical',\n)\n\nprint(metadata)\n</pre> metadata.update_column(     column_name='id',     sdtype='id' )  metadata.update_column(     column_name='age',     sdtype='numerical',     computer_representation=\"Int64\" )  metadata.update_column(     column_name='bmi',     sdtype='numerical',     computer_representation=\"Float\" )  metadata.update_column(     column_name='stroke',     sdtype='categorical', )  metadata.update_column(     column_name='hypertension',     sdtype='categorical', )  metadata.update_column(     column_name='heart_disease',     sdtype='categorical', )  print(metadata) <pre>{\n    \"METADATA_SPEC_VERSION\": \"SINGLE_TABLE_V1\",\n    \"columns\": {\n        \"id\": {\n            \"sdtype\": \"id\"\n        },\n        \"person_id\": {\n            \"sdtype\": \"numerical\"\n        },\n        \"gender\": {\n            \"sdtype\": \"categorical\"\n        },\n        \"age\": {\n            \"sdtype\": \"numerical\",\n            \"computer_representation\": \"Int64\"\n        },\n        \"hypertension\": {\n            \"sdtype\": \"categorical\"\n        },\n        \"heart_disease\": {\n            \"sdtype\": \"categorical\"\n        },\n        \"ever_married\": {\n            \"sdtype\": \"categorical\"\n        },\n        \"work_type\": {\n            \"sdtype\": \"categorical\"\n        },\n        \"Residence_type\": {\n            \"sdtype\": \"categorical\"\n        },\n        \"avg_glucose_level\": {\n            \"sdtype\": \"numerical\"\n        },\n        \"bmi\": {\n            \"sdtype\": \"numerical\",\n            \"computer_representation\": \"Float\"\n        },\n        \"smoking_status\": {\n            \"sdtype\": \"categorical\"\n        },\n        \"stroke\": {\n            \"sdtype\": \"categorical\"\n        }\n    }\n}\n</pre> In\u00a0[6]: Copied! <pre>metadata.update_column(\n    column_name='age',\n    pii=True\n)\n</pre> metadata.update_column(     column_name='age',     pii=True ) <pre>\n---------------------------------------------------------------------------\nInvalidMetadataError                      Traceback (most recent call last)\nCell In[6], line 1\n----&gt; 1 metadata.update_column(\n      2     column_name='age',\n      3     pii=True\n      4 )\n\nFile ~/Library/Caches/pypoetry/virtualenvs/synthetic-data-EqHpLbmO-py3.10/lib/python3.10/site-packages/sdv/metadata/single_table.py:228, in SingleTableMetadata.update_column(self, column_name, **kwargs)\n    226     sdtype = self.columns[column_name]['sdtype']\n    227     _kwargs['sdtype'] = sdtype\n--&gt; 228 self._validate_column(column_name, sdtype, **kwargs)\n    229 self.columns[column_name] = _kwargs\n\nFile ~/Library/Caches/pypoetry/virtualenvs/synthetic-data-EqHpLbmO-py3.10/lib/python3.10/site-packages/sdv/metadata/single_table.py:149, in SingleTableMetadata._validate_column(self, column_name, sdtype, **kwargs)\n    147 def _validate_column(self, column_name, sdtype, **kwargs):\n    148     self._validate_sdtype(sdtype)\n--&gt; 149     self._validate_unexpected_kwargs(column_name, sdtype, **kwargs)\n    150     if sdtype == 'categorical':\n    151         self._validate_categorical(column_name, **kwargs)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/synthetic-data-EqHpLbmO-py3.10/lib/python3.10/site-packages/sdv/metadata/single_table.py:134, in SingleTableMetadata._validate_unexpected_kwargs(self, column_name, sdtype, **kwargs)\n    132 unexpected_kwargs = sorted(unexpected_kwargs)\n    133 unexpected_kwargs = ', '.join(unexpected_kwargs)\n--&gt; 134 raise InvalidMetadataError(\n    135     f\"Invalid values '({unexpected_kwargs})' for {sdtype} column '{column_name}'.\")\n\nInvalidMetadataError: Invalid values '(pii)' for numerical column 'age'.</pre> <p>As you can see above, we then tried to say that Age is a PII column (Personally Identifiable Information) however that's not the case. Only as seen on the Synthetic Data - Introduction.</p> <p>If you have a numerical value that is sensitive and needs to be protected but is not a PII, there are a few options you can consider:</p> <ul> <li>Anonymization: Modify the numerical values to remove any direct or indirect identifiers. For example, you can apply techniques such as generalization, suppression, or randomization to de-identify the sensitive values. This approach allows you to retain the statistical properties of the data while protecting individual privacy.</li> <li>Tokenization or Encoding: If the sensitive numerical values represent categorical or discrete data, you can consider tokenizing or encoding them. This involves replacing the original values with unique tokens or numeric representations, ensuring that the sensitive information cannot be directly inferred.</li> <li>Aggregation or Binning: If the specific values of the numerical data are not crucial and the main focus is on preserving statistical properties, you can aggregate or group the values into ranges or bins. For example, you can convert age values into age groups (e.g., 20-30, 31-40) or income values into income brackets. This approach can help to maintain the overall distribution while adding a level of privacy protection.</li> <li>Differential Privacy: Differential privacy is a concept that provides a rigorous mathematical framework for privacy protection. It involves injecting noise into the data or query responses in a controlled manner, ensuring that the privacy of individual data points is preserved. Differential privacy techniques can be applied to numerical data to protect sensitive information.</li> </ul> <p>When handling sensitive numerical data, it's essential to comply with relevant privacy regulations and consider the specific requirements of your use case. Consult with legal and privacy experts to ensure that the chosen approach aligns with applicable laws and regulations, and adequately protects the privacy of individuals.</p> <p>Since age is not a PII column and neither are any of the other columns of this table, we'll test this PII setting on our multitable notebook once we have a people table with personal information.</p> <p>We then proceeded to identify the primary key column. In this case it was id.</p> In\u00a0[7]: Copied! <pre>metadata.set_primary_key(column_name='id')\nprint(metadata)\n</pre> metadata.set_primary_key(column_name='id') print(metadata) <pre>{\n    \"METADATA_SPEC_VERSION\": \"SINGLE_TABLE_V1\",\n    \"columns\": {\n        \"id\": {\n            \"sdtype\": \"id\"\n        },\n        \"person_id\": {\n            \"sdtype\": \"numerical\"\n        },\n        \"gender\": {\n            \"sdtype\": \"categorical\"\n        },\n        \"age\": {\n            \"sdtype\": \"numerical\",\n            \"computer_representation\": \"Int64\"\n        },\n        \"hypertension\": {\n            \"sdtype\": \"categorical\"\n        },\n        \"heart_disease\": {\n            \"sdtype\": \"categorical\"\n        },\n        \"ever_married\": {\n            \"sdtype\": \"categorical\"\n        },\n        \"work_type\": {\n            \"sdtype\": \"categorical\"\n        },\n        \"Residence_type\": {\n            \"sdtype\": \"categorical\"\n        },\n        \"avg_glucose_level\": {\n            \"sdtype\": \"numerical\"\n        },\n        \"bmi\": {\n            \"sdtype\": \"numerical\",\n            \"computer_representation\": \"Float\"\n        },\n        \"smoking_status\": {\n            \"sdtype\": \"categorical\"\n        },\n        \"stroke\": {\n            \"sdtype\": \"categorical\"\n        }\n    },\n    \"primary_key\": \"id\"\n}\n</pre> In\u00a0[9]: Copied! <pre>from sdv.lite import SingleTablePreset\n\nsynthesizer = SingleTablePreset(metadata, name='FAST_ML')\nsynthesizer.fit(strokes_table)\n\nsynthetic_data = synthesizer.sample(num_rows=5000)\nsynthetic_data\n</pre> from sdv.lite import SingleTablePreset  synthesizer = SingleTablePreset(metadata, name='FAST_ML') synthesizer.fit(strokes_table)  synthetic_data = synthesizer.sample(num_rows=5000) synthetic_data Out[9]: id person_id gender age hypertension heart_disease ever_married work_type Residence_type avg_glucose_level bmi smoking_status stroke 0 0 18483 Female 42 0 0 Yes Private Rural 138.949724 34.448243 Unknown 0 1 1 40268 Female 49 0 0 Yes Self-employed Rural 106.397658 41.427455 smokes 0 2 2 39773 Male 72 0 0 Yes Private Urban 146.924737 36.470968 smokes 0 3 3 29169 Female 31 0 0 No Private Rural 137.243082 20.695570 never smoked 0 4 4 23725 Female 8 0 0 Yes Self-employed Rural 127.135125 37.828633 Unknown 0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... 4995 4995 31770 Female 12 0 0 Yes Private Urban 80.745171 31.964878 Unknown 0 4996 4996 47371 Male 48 0 0 Yes Private Rural 133.521052 27.476454 formerly smoked 0 4997 4997 59809 Male 82 0 0 Yes Private Rural 127.595544 39.734157 Unknown 0 4998 4998 62847 Female 31 0 0 No Private Rural 110.706109 38.073594 never smoked 0 4999 4999 61172 Female 31 0 0 Yes Private Urban 133.384555 30.610890 never smoked 0 <p>5000 rows \u00d7 13 columns</p> <p>We can also condition the data we are interested in. In the case bellow we created a table with 250 rows with only people who have had strokes. However, conditional sampling sometimes does not work. To try and solve that problem you can resort to the troubleshooting section on the documentation.</p> In\u00a0[10]: Copied! <pre>from sdv.sampling import Condition\n\nwith_stroke = Condition(\n    num_rows=250,\n    column_values={'stroke': 1}\n)\n\nsynthetic_data_with_stroke = synthesizer.sample_from_conditions(\n    conditions=[with_stroke],\n)\n\nsynthetic_data_with_stroke\n</pre> from sdv.sampling import Condition  with_stroke = Condition(     num_rows=250,     column_values={'stroke': 1} )  synthetic_data_with_stroke = synthesizer.sample_from_conditions(     conditions=[with_stroke], )  synthetic_data_with_stroke  <pre>Sampling conditions: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 250/250 [00:00&lt;00:00, 4936.73it/s]\n</pre> Out[10]: id person_id gender age hypertension heart_disease ever_married work_type Residence_type avg_glucose_level bmi smoking_status stroke 0 5000 77 Male 60 0 0 Yes Private Rural 171.181426 26.095976 never smoked 1 1 5001 14831 Female 40 0 0 No Self-employed Urban 55.120000 20.928930 Unknown 1 2 5002 18073 Male 36 0 0 Yes Self-employed Rural 148.844070 39.191698 never smoked 1 3 5003 47225 Male 41 0 0 No Private Rural 105.012975 18.172498 never smoked 1 4 5004 34256 Female 60 0 0 Yes Private Rural 184.591717 41.743158 Unknown 1 ... ... ... ... ... ... ... ... ... ... ... ... ... ... 245 5245 55172 Male 59 0 0 No Private Urban 125.669804 27.232660 never smoked 1 246 5246 42483 Male 28 0 0 Yes Private Rural 148.901177 26.345644 never smoked 1 247 5247 5554 Female 2 0 0 Yes Self-employed Rural 55.120000 36.977610 smokes 1 248 5248 63486 Female 82 0 0 Yes Private Rural 151.333343 28.523270 never smoked 1 249 5249 48875 Female 61 0 0 Yes Private Rural 177.863760 41.056034 never smoked 1 <p>250 rows \u00d7 13 columns</p> In\u00a0[11]: Copied! <pre>from sdv.evaluation.single_table import evaluate_quality\n\nquality_report = evaluate_quality(\n    real_data=strokes_table,\n    synthetic_data=synthetic_data,\n    metadata=metadata)\n</pre> from sdv.evaluation.single_table import evaluate_quality  quality_report = evaluate_quality(     real_data=strokes_table,     synthetic_data=synthetic_data,     metadata=metadata) <pre>Creating report: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00&lt;00:00, 10.81it/s]\n</pre> <pre>\nOverall Quality Score: 89.33%\n\nProperties:\nColumn Shapes: 92.66%\nColumn Pair Trends: 86.01%\n</pre> In\u00a0[10]: Copied! <pre>from sdv.evaluation.single_table import run_diagnostic\n\ndiagnostic_report = run_diagnostic(\n    real_data=strokes_table,\n    synthetic_data=synthetic_data,\n    metadata=metadata)\n</pre> from sdv.evaluation.single_table import run_diagnostic  diagnostic_report = run_diagnostic(     real_data=strokes_table,     synthetic_data=synthetic_data,     metadata=metadata) <pre>Creating report: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:15&lt;00:00,  3.89s/it]</pre> <pre>\nDiagnosticResults:\n\nSUCCESS:\n\u2713 The synthetic data covers over 90% of the categories present in the real data\n\u2713 Over 90% of the synthetic rows are not copies of the real data\n\u2713 The synthetic data follows over 90% of the min/max boundaries set by the real data\n\nWARNING:\n! The synthetic data is missing more than 10% of the numerical ranges present in the real data\n</pre> <pre>\n</pre> In\u00a0[11]: Copied! <pre>diagnostic_report.get_results()\n</pre> diagnostic_report.get_results() Out[11]: <pre>{'SUCCESS': ['The synthetic data covers over 90% of the categories present in the real data',\n  'Over 90% of the synthetic rows are not copies of the real data',\n  'The synthetic data follows over 90% of the min/max boundaries set by the real data'],\n 'WARNING': ['The synthetic data is missing more than 10% of the numerical ranges present in the real data'],\n 'DANGER': []}</pre> In\u00a0[12]: Copied! <pre>diagnostic_report.get_properties()\n</pre> diagnostic_report.get_properties() Out[12]: <pre>{'Coverage': 0.9592604702286464, 'Synthesis': 1.0, 'Boundaries': 1.0}</pre> In\u00a0[13]: Copied! <pre>diagnostic_report.get_details(property_name='Coverage')\n</pre> diagnostic_report.get_details(property_name='Coverage') Out[13]: Column Metric Diagnostic Score 0 person_id RangeCoverage 1.000000 1 age RangeCoverage 1.000000 2 avg_glucose_level RangeCoverage 1.000000 3 bmi RangeCoverage 0.511126 4 gender CategoryCoverage 1.000000 5 hypertension CategoryCoverage 1.000000 6 heart_disease CategoryCoverage 1.000000 7 ever_married CategoryCoverage 1.000000 8 work_type CategoryCoverage 1.000000 9 Residence_type CategoryCoverage 1.000000 10 smoking_status CategoryCoverage 1.000000 11 stroke CategoryCoverage 1.000000 <p>It also allows you to visualize that comparison.</p> In\u00a0[14]: Copied! <pre>from sdv.evaluation.single_table import get_column_pair_plot\n\nfig = get_column_pair_plot(\n    real_data=strokes_table,\n    synthetic_data=synthetic_data,\n    column_names=['age', 'bmi'],\n    metadata=metadata)\n    \nfig.show()\n</pre> from sdv.evaluation.single_table import get_column_pair_plot  fig = get_column_pair_plot(     real_data=strokes_table,     synthetic_data=synthetic_data,     column_names=['age', 'bmi'],     metadata=metadata)      fig.show() In\u00a0[15]: Copied! <pre>from sdv.evaluation.single_table import get_column_plot\n\nfig = get_column_plot(\n    real_data=strokes_table,\n    synthetic_data=synthetic_data,\n    column_name='bmi',\n    metadata=metadata\n)\n    \nfig.show()\n</pre> from sdv.evaluation.single_table import get_column_plot  fig = get_column_plot(     real_data=strokes_table,     synthetic_data=synthetic_data,     column_name='bmi',     metadata=metadata )      fig.show() In\u00a0[16]: Copied! <pre>from sdv.single_table import GaussianCopulaSynthesizer\n\nmy_constraint = {\n    'constraint_class': 'ScalarRange',\n    'constraint_parameters': {\n        'column_name': 'bmi',\n        'low_value': 11.3,\n        'high_value': 97.6,\n        'strict_boundaries': False\n    }\n}\n\nmy_constraint2 = {\n    'constraint_class': 'ScalarRange',\n    'constraint_parameters': {\n        'column_name': 'avg_glucose_level',\n        'low_value': 55.12,\n        'high_value': 271.74,\n        'strict_boundaries': False\n    }\n}\n\nsynthesizer = GaussianCopulaSynthesizer(metadata, numerical_distributions={\"bmi\":\"truncnorm\", \"avg_glucose_level\":\"truncnorm\"})\n\nsynthesizer.add_constraints(constraints=[\n    my_constraint\n])\n\nsynthesizer.add_constraints(constraints=[\n    my_constraint2\n])\n\nsynthesizer.fit(strokes_table)\n\nsynthetic_data_2 = synthesizer.sample(num_rows=4000)\n</pre> from sdv.single_table import GaussianCopulaSynthesizer  my_constraint = {     'constraint_class': 'ScalarRange',     'constraint_parameters': {         'column_name': 'bmi',         'low_value': 11.3,         'high_value': 97.6,         'strict_boundaries': False     } }  my_constraint2 = {     'constraint_class': 'ScalarRange',     'constraint_parameters': {         'column_name': 'avg_glucose_level',         'low_value': 55.12,         'high_value': 271.74,         'strict_boundaries': False     } }  synthesizer = GaussianCopulaSynthesizer(metadata, numerical_distributions={\"bmi\":\"truncnorm\", \"avg_glucose_level\":\"truncnorm\"})  synthesizer.add_constraints(constraints=[     my_constraint ])  synthesizer.add_constraints(constraints=[     my_constraint2 ])  synthesizer.fit(strokes_table)  synthetic_data_2 = synthesizer.sample(num_rows=4000) <pre>Sampling rows: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4000/4000 [00:00&lt;00:00, 28692.59it/s]\n</pre> In\u00a0[17]: Copied! <pre>from sdv.evaluation.single_table import evaluate_quality\n\nquality_report = evaluate_quality(\n    real_data=strokes_table,\n    synthetic_data=synthetic_data_2,\n    metadata=metadata)\n</pre> from sdv.evaluation.single_table import evaluate_quality  quality_report = evaluate_quality(     real_data=strokes_table,     synthetic_data=synthetic_data_2,     metadata=metadata) <pre>Creating report: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00&lt;00:00, 13.10it/s]\n</pre> <pre>\nOverall Quality Score: 88.48%\n\nProperties:\nColumn Shapes: 91.81%\nColumn Pair Trends: 85.16%\n</pre> In\u00a0[18]: Copied! <pre>from sdv.evaluation.single_table import run_diagnostic\n\ndiagnostic_report = run_diagnostic(\n    real_data=strokes_table,\n    synthetic_data=synthetic_data_2,\n    metadata=metadata)\n\nsynthesizer.get_learned_distributions()\n</pre> from sdv.evaluation.single_table import run_diagnostic  diagnostic_report = run_diagnostic(     real_data=strokes_table,     synthetic_data=synthetic_data_2,     metadata=metadata)  synthesizer.get_learned_distributions() <pre>Creating report: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:12&lt;00:00,  3.23s/it]</pre> <pre>\nDiagnosticResults:\n\nSUCCESS:\n\u2713 The synthetic data covers over 90% of the numerical ranges present in the real data\n\u2713 The synthetic data covers over 90% of the categories present in the real data\n\u2713 Over 90% of the synthetic rows are not copies of the real data\n\u2713 The synthetic data follows over 90% of the min/max boundaries set by the real data\n</pre> <pre>\n</pre> Out[18]: <pre>{'person_id': {'distribution': 'beta',\n  'learned_parameters': {'loc': 80.25890325886454,\n   'scale': 72859.74109674116,\n   'a': 0.9969547367188392,\n   'b': 0.9716645550572578}},\n 'gender': {'distribution': 'beta',\n  'learned_parameters': {'loc': -2.5252690911627557e-05,\n   'scale': 2.429965774475864,\n   'a': 1.1674347481757146,\n   'b': 2.031096048032575}},\n 'age': {'distribution': 'beta',\n  'learned_parameters': {'loc': 1.2374170728627083,\n   'scale': 80.7625829271373,\n   'a': 0.9923267833484788,\n   'b': 0.8396338204631779}},\n 'hypertension': {'distribution': 'beta',\n  'learned_parameters': {'loc': -0.0015033493750864929,\n   'scale': 2.6734823331569064,\n   'a': 1.298928537009436,\n   'b': 4.583705567012164}},\n 'heart_disease': {'distribution': 'beta',\n  'learned_parameters': {'loc': -0.0011121211788436343,\n   'scale': 2.5118242709872747,\n   'a': 1.3920048676282983,\n   'b': 5.027246739017915}},\n 'ever_married': {'distribution': 'beta',\n  'learned_parameters': {'loc': 0.00015138900059352697,\n   'scale': 2.000541483317777,\n   'a': 0.9231752283513573,\n   'b': 1.2510098798236158}},\n 'work_type': {'distribution': 'beta',\n  'learned_parameters': {'loc': -0.19329558094923285,\n   'scale': 136617.51361846982,\n   'a': 3.08290050373147,\n   'b': 205131.79060185206}},\n 'Residence_type': {'distribution': 'beta',\n  'learned_parameters': {'loc': 0.0005564993489894335,\n   'scale': 1.9990261104896918,\n   'a': 0.9592938015075503,\n   'b': 0.9884101783243173}},\n 'smoking_status': {'distribution': 'beta',\n  'learned_parameters': {'loc': 0.00016896520737423191,\n   'scale': 4.015279314682994,\n   'a': 1.0265460598108,\n   'b': 1.3662274288516505}},\n 'stroke': {'distribution': 'beta',\n  'learned_parameters': {'loc': 0.000195975643300648,\n   'scale': 2.3818244739318652,\n   'a': 1.3553887073649618,\n   'b': 4.673247660622257}},\n 'bmi#11.3#97.6': {'distribution': 'beta',\n  'learned_parameters': {'loc': -16.755951983495105,\n   'scale': 51.377743910445545,\n   'a': 630.7390090100525,\n   'b': 1469.4336390811734}},\n 'avg_glucose_level#55.12#271.74': {'distribution': 'beta',\n  'learned_parameters': {'loc': -4.597277640616451,\n   'scale': 625222.6174629636,\n   'a': 9.019129645706993,\n   'b': 1728046.3185030504}}}</pre> In\u00a0[19]: Copied! <pre>diagnostic_report.get_details(property_name='Coverage')\n</pre> diagnostic_report.get_details(property_name='Coverage') Out[19]: Column Metric Diagnostic Score 0 person_id RangeCoverage 0.999671 1 age RangeCoverage 1.000000 2 avg_glucose_level RangeCoverage 1.000000 3 bmi RangeCoverage 0.631518 4 gender CategoryCoverage 1.000000 5 hypertension CategoryCoverage 1.000000 6 heart_disease CategoryCoverage 1.000000 7 ever_married CategoryCoverage 1.000000 8 work_type CategoryCoverage 1.000000 9 Residence_type CategoryCoverage 1.000000 10 smoking_status CategoryCoverage 1.000000 11 stroke CategoryCoverage 1.000000 In\u00a0[30]: Copied! <pre>quality_report.get_details(property_name='Column Shapes')\n</pre> quality_report.get_details(property_name='Column Shapes') Out[30]: Column Metric Quality Score 0 person_id KSComplement 0.987458 1 age KSComplement 0.912625 2 avg_glucose_level KSComplement 0.913750 3 bmi KSComplement 0.966333 4 gender TVComplement 0.954792 5 hypertension TVComplement 0.923458 6 heart_disease TVComplement 0.910125 7 ever_married TVComplement 0.924000 8 work_type TVComplement 0.706875 9 Residence_type TVComplement 0.996333 10 smoking_status TVComplement 0.902667 11 stroke TVComplement 0.918542"},{"location":"python/machine_learning/synthethic_data/2_single_table/#single-table-synthetic-data","title":"Single Table Synthetic Data\u00b6","text":""},{"location":"python/machine_learning/synthethic_data/2_single_table/#data-analysis","title":"Data analysis\u00b6","text":"<p>We'll be using one table which has strokes data per person. This table was extracted from kaggle. Each entry has an id and a person_id and some related data to their health and a stroke field which tells us whether this person has had a stroke or not. As you can see bellow all of the columns are in conformity with GDPR. Meaning you cannot identify a person by any single or combined columns. However it has sensitive data which is important for us to masquerade. So we'll be doing that.</p>"},{"location":"python/machine_learning/synthethic_data/2_single_table/#load-data","title":"Load Data\u00b6","text":"<p>First, we go to the content folder and get all csv files there.</p>"},{"location":"python/machine_learning/synthethic_data/2_single_table/#create-metadata","title":"Create metadata\u00b6","text":"<p>We then need to create the metadata object to be used when creating the synthesizer. SDV will detect some information from the table content but it may not be correct. It's always best to check the metadata and fix whatever needs to be fixed.</p>"},{"location":"python/machine_learning/synthethic_data/2_single_table/#edit-metadata","title":"Edit Metadata\u00b6","text":"<p>Bellow, we make a few changes such as:</p> <ul> <li>Change column id to type id;</li> <li>Change column age to a numerical type of integers;</li> <li>Change column bmi to a numerical type of floats;</li> <li>Change column stroke to a categorical column since the information is either 0 or 1;</li> <li>Change hypertension and heart_disease columns to categorical as well for the same reason.</li> </ul>"},{"location":"python/machine_learning/synthethic_data/2_single_table/#create-synthesizer","title":"Create Synthesizer\u00b6","text":"<p>Having created the metadata object we then needed to create the synthesizer which will be trained to generate the synthetic data. For a first attempt we used a GaussianCopulaSynthesizer with a FAST_ML preset. Since we are just generating data for one table we used the SingleTablePreset from SDV library. We then trained the synthesizer using the fit method and got the results bellow. We then use the sample method to get the synthetic data. The parameters for this method beside num_rows are:</p> <ul> <li>batch_size: An integer &gt;0, describing the number of rows to sample at a time. If you are sampling a large number of rows, setting a smaller batch size allows you to see and save incremental progress. Defaults to the same as num_rows.</li> <li>max_tries_per_batch: An integer &gt;0, describing the number of sampling attempts to make per batch. If you have included constraints, it may take multiple batches to create valid data. Defaults to 100.</li> <li>output_file_path: A string describing a CSV filepath for writing the synthetic data. Specify to None to skip writing to a file. Defaults to None.</li> </ul>"},{"location":"python/machine_learning/synthethic_data/2_single_table/#evaluation","title":"Evaluation\u00b6","text":"<p>The SDV library has a very powerful tool which allows you to evaluate the quality of the data generated by your synthesizer and also create a diagnostic report of that data.</p>"},{"location":"python/machine_learning/synthethic_data/2_single_table/#bmi-fix","title":"BMI Fix\u00b6","text":"<p>As you can see BMI was the column with worst range coverage and probably what's bringing the evaluation down. We can start by analysing what BMI really is. BMI is an index that is calculated through: $$weight/height^2$$ So we'll add some constraints so the values generated are within the same range as the original data. We'll also use the truncnorm distribution which basically is a normal distribution but within a range. The values used for low and high values on the constraints are the same as the low and high values on the original data.</p>"},{"location":"python/machine_learning/synthethic_data/2_single_table/#new-evaluation","title":"New Evaluation\u00b6","text":"<p>As you can see using a customized GaussianCopulaSynthesizer module to synthesize data was not enough to improve the quality of said data. We could also use Neural network-based synthesizers such as CTGAN, TVAE or CopulaGAN synthesizers But for now that analysis is not tested.</p>"},{"location":"python/machine_learning/synthethic_data/3_multi_table/","title":"Multi Table Synthetic Data","text":"In\u00a0[2]: Copied! <pre>from sdv.datasets.local import load_csvs\n\ntry:\n    datasets = load_csvs(folder_name='content/')\nexcept ValueError as e:\n    print(e)\n</pre> from sdv.datasets.local import load_csvs  try:     datasets = load_csvs(folder_name='content/') except ValueError as e:     print(e) <p>Then, we access both table and display the first 20 rows of the people table.</p> In\u00a0[3]: Copied! <pre>print(datasets.keys())\n\nstrokes_table = datasets['strokes']\npeople_table = datasets['people']\npeople_table.head(20)\n</pre> print(datasets.keys())  strokes_table = datasets['strokes'] people_table = datasets['people'] people_table.head(20) <pre>dict_keys(['people', 'strokes'])\n</pre> Out[3]: id name address city 0 56420 Marcelo Holmes 49 Walt Whitman Lane New York 1 51856 Aleena Hahn Apple Valley, CA 92307 Los Angeles 2 41097 Jocelyn Hancock 10 West Church St. Chicago 3 545 Marcel Underwood Hastings, MN 55033 Miami 4 37759 Jazlyn Davila 7444 South Pine Dr. Dallas 5 66333 Teagan Randall Malden, MA 02148 Houston 6 70670 Antony Graham 666 Windfall Dr. Philadelphia 7 20292 Evelyn Becker Niagara Falls, NY 14304 Atlanta 8 72784 Arturo Dillon 334 Grove Street Washington 9 65895 Alyssa Peters Moncks Corner, SC 29461 Boston 10 5131 Damarion Colon 58 Canterbury Street Phoenix 11 72911 Holden Mccarthy Lake Jackson, TX 77566 Detroit 12 1307 Colten Costa 345 East Brandywine St. Seattle 13 23047 Turner Mcdaniel Halethorpe, MD 21227 San Francisco 14 32604 Jett Knox 9 Eagle Dr. San Diego 15 63915 Talia Olson Maumee, OH 43537 Minneapolis 16 25405 Kate Hale 59 Vale St. Brooklyn 17 3590 Lilliana Warren Tuckerton, NJ 08087 Tampa 18 2898 Ramon Dillon 95 Cross Ave. Denver 19 60675 Ayanna Tyler Chardon, OH 44024 Queens In\u00a0[4]: Copied! <pre>from sdv.metadata import MultiTableMetadata\n\nmetadata = MultiTableMetadata()\n\nmetadata.detect_table_from_dataframe(\n    table_name='strokes',\n    data=strokes_table\n)\n\nmetadata.detect_table_from_dataframe(\n    table_name='people',\n    data=people_table\n)\n\nprint('Auto detected data:\\n')\nmetadata\n</pre> from sdv.metadata import MultiTableMetadata  metadata = MultiTableMetadata()  metadata.detect_table_from_dataframe(     table_name='strokes',     data=strokes_table )  metadata.detect_table_from_dataframe(     table_name='people',     data=people_table )  print('Auto detected data:\\n') metadata <pre>Auto detected data:\n\n</pre> Out[4]: <pre>{\n    \"tables\": {\n        \"strokes\": {\n            \"columns\": {\n                \"id\": {\n                    \"sdtype\": \"numerical\"\n                },\n                \"person_id\": {\n                    \"sdtype\": \"numerical\"\n                },\n                \"gender\": {\n                    \"sdtype\": \"categorical\"\n                },\n                \"age\": {\n                    \"sdtype\": \"numerical\"\n                },\n                \"hypertension\": {\n                    \"sdtype\": \"numerical\"\n                },\n                \"heart_disease\": {\n                    \"sdtype\": \"numerical\"\n                },\n                \"ever_married\": {\n                    \"sdtype\": \"categorical\"\n                },\n                \"work_type\": {\n                    \"sdtype\": \"categorical\"\n                },\n                \"Residence_type\": {\n                    \"sdtype\": \"categorical\"\n                },\n                \"avg_glucose_level\": {\n                    \"sdtype\": \"numerical\"\n                },\n                \"bmi\": {\n                    \"sdtype\": \"numerical\"\n                },\n                \"smoking_status\": {\n                    \"sdtype\": \"categorical\"\n                },\n                \"stroke\": {\n                    \"sdtype\": \"numerical\"\n                }\n            }\n        },\n        \"people\": {\n            \"columns\": {\n                \"id\": {\n                    \"sdtype\": \"numerical\"\n                },\n                \"name\": {\n                    \"sdtype\": \"categorical\"\n                },\n                \"address\": {\n                    \"sdtype\": \"categorical\"\n                },\n                \"city\": {\n                    \"sdtype\": \"categorical\"\n                }\n            }\n        }\n    },\n    \"relationships\": [],\n    \"METADATA_SPEC_VERSION\": \"MULTI_TABLE_V1\"\n}</pre> In\u00a0[5]: Copied! <pre>metadata.update_column(\n    table_name='strokes',\n    column_name='id',\n    sdtype='id'\n)\n\nmetadata.update_column(\n    table_name='strokes',\n    column_name='person_id',\n    sdtype='id'\n)\n\nmetadata.update_column(\n    table_name='strokes',\n    column_name='age',\n    sdtype='numerical',\n    computer_representation=\"Int64\"\n)\n\nmetadata.update_column(\n    table_name='strokes',\n    column_name='bmi',\n    sdtype='numerical',\n    computer_representation=\"Float\"\n)\n\nmetadata.update_column(\n    table_name='strokes',\n    column_name='stroke',\n    sdtype='categorical',\n)\n\nmetadata.update_column(\n    table_name='strokes',\n    column_name='hypertension',\n    sdtype='categorical',\n)\n\nmetadata.update_column(\n    table_name='strokes',\n    column_name='heart_disease',\n    sdtype='categorical',\n)\n\nprint(metadata)\n</pre> metadata.update_column(     table_name='strokes',     column_name='id',     sdtype='id' )  metadata.update_column(     table_name='strokes',     column_name='person_id',     sdtype='id' )  metadata.update_column(     table_name='strokes',     column_name='age',     sdtype='numerical',     computer_representation=\"Int64\" )  metadata.update_column(     table_name='strokes',     column_name='bmi',     sdtype='numerical',     computer_representation=\"Float\" )  metadata.update_column(     table_name='strokes',     column_name='stroke',     sdtype='categorical', )  metadata.update_column(     table_name='strokes',     column_name='hypertension',     sdtype='categorical', )  metadata.update_column(     table_name='strokes',     column_name='heart_disease',     sdtype='categorical', )  print(metadata) <pre>{\n    \"tables\": {\n        \"strokes\": {\n            \"columns\": {\n                \"id\": {\n                    \"sdtype\": \"id\"\n                },\n                \"person_id\": {\n                    \"sdtype\": \"id\"\n                },\n                \"gender\": {\n                    \"sdtype\": \"categorical\"\n                },\n                \"age\": {\n                    \"sdtype\": \"numerical\",\n                    \"computer_representation\": \"Int64\"\n                },\n                \"hypertension\": {\n                    \"sdtype\": \"categorical\"\n                },\n                \"heart_disease\": {\n                    \"sdtype\": \"categorical\"\n                },\n                \"ever_married\": {\n                    \"sdtype\": \"categorical\"\n                },\n                \"work_type\": {\n                    \"sdtype\": \"categorical\"\n                },\n                \"Residence_type\": {\n                    \"sdtype\": \"categorical\"\n                },\n                \"avg_glucose_level\": {\n                    \"sdtype\": \"numerical\"\n                },\n                \"bmi\": {\n                    \"sdtype\": \"numerical\",\n                    \"computer_representation\": \"Float\"\n                },\n                \"smoking_status\": {\n                    \"sdtype\": \"categorical\"\n                },\n                \"stroke\": {\n                    \"sdtype\": \"categorical\"\n                }\n            }\n        },\n        \"people\": {\n            \"columns\": {\n                \"id\": {\n                    \"sdtype\": \"numerical\"\n                },\n                \"name\": {\n                    \"sdtype\": \"categorical\"\n                },\n                \"address\": {\n                    \"sdtype\": \"categorical\"\n                },\n                \"city\": {\n                    \"sdtype\": \"categorical\"\n                }\n            }\n        }\n    },\n    \"relationships\": [],\n    \"METADATA_SPEC_VERSION\": \"MULTI_TABLE_V1\"\n}\n</pre> In\u00a0[6]: Copied! <pre>metadata.update_column(\n    table_name='people',\n    column_name='id',\n    sdtype='id'\n)\n\nmetadata.update_column(\n    table_name='people',\n    column_name='name',\n    sdtype='name',\n    pii=True\n)\n\nmetadata.update_column(\n    table_name='people',\n    column_name='address',\n    sdtype='address',\n    pii=True\n)\n\nmetadata.update_column(\n    table_name='people',\n    column_name='city',\n    sdtype='city',\n    pii=True\n)\n\nprint(metadata)\n</pre> metadata.update_column(     table_name='people',     column_name='id',     sdtype='id' )  metadata.update_column(     table_name='people',     column_name='name',     sdtype='name',     pii=True )  metadata.update_column(     table_name='people',     column_name='address',     sdtype='address',     pii=True )  metadata.update_column(     table_name='people',     column_name='city',     sdtype='city',     pii=True )  print(metadata) <pre>{\n    \"tables\": {\n        \"strokes\": {\n            \"columns\": {\n                \"id\": {\n                    \"sdtype\": \"id\"\n                },\n                \"person_id\": {\n                    \"sdtype\": \"id\"\n                },\n                \"gender\": {\n                    \"sdtype\": \"categorical\"\n                },\n                \"age\": {\n                    \"sdtype\": \"numerical\",\n                    \"computer_representation\": \"Int64\"\n                },\n                \"hypertension\": {\n                    \"sdtype\": \"categorical\"\n                },\n                \"heart_disease\": {\n                    \"sdtype\": \"categorical\"\n                },\n                \"ever_married\": {\n                    \"sdtype\": \"categorical\"\n                },\n                \"work_type\": {\n                    \"sdtype\": \"categorical\"\n                },\n                \"Residence_type\": {\n                    \"sdtype\": \"categorical\"\n                },\n                \"avg_glucose_level\": {\n                    \"sdtype\": \"numerical\"\n                },\n                \"bmi\": {\n                    \"sdtype\": \"numerical\",\n                    \"computer_representation\": \"Float\"\n                },\n                \"smoking_status\": {\n                    \"sdtype\": \"categorical\"\n                },\n                \"stroke\": {\n                    \"sdtype\": \"categorical\"\n                }\n            }\n        },\n        \"people\": {\n            \"columns\": {\n                \"id\": {\n                    \"sdtype\": \"id\"\n                },\n                \"name\": {\n                    \"sdtype\": \"name\",\n                    \"pii\": true\n                },\n                \"address\": {\n                    \"sdtype\": \"address\",\n                    \"pii\": true\n                },\n                \"city\": {\n                    \"sdtype\": \"city\",\n                    \"pii\": true\n                }\n            }\n        }\n    },\n    \"relationships\": [],\n    \"METADATA_SPEC_VERSION\": \"MULTI_TABLE_V1\"\n}\n</pre> <p>Then we need to connect the two tables</p> In\u00a0[7]: Copied! <pre>metadata.set_primary_key(\n    table_name='strokes',\n    column_name='id'\n)\n\nmetadata.set_primary_key(\n    table_name='people',\n    column_name='id'\n)\n\nmetadata.add_relationship(\n    parent_table_name='people',\n    child_table_name='strokes',\n    parent_primary_key='id',\n    child_foreign_key='person_id'\n)\n\nprint(metadata)\n</pre> metadata.set_primary_key(     table_name='strokes',     column_name='id' )  metadata.set_primary_key(     table_name='people',     column_name='id' )  metadata.add_relationship(     parent_table_name='people',     child_table_name='strokes',     parent_primary_key='id',     child_foreign_key='person_id' )  print(metadata) <pre>{\n    \"tables\": {\n        \"strokes\": {\n            \"primary_key\": \"id\",\n            \"columns\": {\n                \"id\": {\n                    \"sdtype\": \"id\"\n                },\n                \"person_id\": {\n                    \"sdtype\": \"id\"\n                },\n                \"gender\": {\n                    \"sdtype\": \"categorical\"\n                },\n                \"age\": {\n                    \"sdtype\": \"numerical\",\n                    \"computer_representation\": \"Int64\"\n                },\n                \"hypertension\": {\n                    \"sdtype\": \"categorical\"\n                },\n                \"heart_disease\": {\n                    \"sdtype\": \"categorical\"\n                },\n                \"ever_married\": {\n                    \"sdtype\": \"categorical\"\n                },\n                \"work_type\": {\n                    \"sdtype\": \"categorical\"\n                },\n                \"Residence_type\": {\n                    \"sdtype\": \"categorical\"\n                },\n                \"avg_glucose_level\": {\n                    \"sdtype\": \"numerical\"\n                },\n                \"bmi\": {\n                    \"sdtype\": \"numerical\",\n                    \"computer_representation\": \"Float\"\n                },\n                \"smoking_status\": {\n                    \"sdtype\": \"categorical\"\n                },\n                \"stroke\": {\n                    \"sdtype\": \"categorical\"\n                }\n            }\n        },\n        \"people\": {\n            \"primary_key\": \"id\",\n            \"columns\": {\n                \"id\": {\n                    \"sdtype\": \"id\"\n                },\n                \"name\": {\n                    \"sdtype\": \"name\",\n                    \"pii\": true\n                },\n                \"address\": {\n                    \"sdtype\": \"address\",\n                    \"pii\": true\n                },\n                \"city\": {\n                    \"sdtype\": \"city\",\n                    \"pii\": true\n                }\n            }\n        }\n    },\n    \"relationships\": [\n        {\n            \"parent_table_name\": \"people\",\n            \"child_table_name\": \"strokes\",\n            \"parent_primary_key\": \"id\",\n            \"child_foreign_key\": \"person_id\"\n        }\n    ],\n    \"METADATA_SPEC_VERSION\": \"MULTI_TABLE_V1\"\n}\n</pre> In\u00a0[8]: Copied! <pre>from sdv.multi_table import HMASynthesizer\n\nsynthesizer = HMASynthesizer(metadata)\nsynthesizer.fit(datasets)\n</pre> from sdv.multi_table import HMASynthesizer  synthesizer = HMASynthesizer(metadata) synthesizer.fit(datasets) <p>We cannot define number of rows when using multitable but we can define a scale:</p> <ul> <li>&lt;1 : Shrink the data by the specified percentage. For example, 0.9 will create synthetic data that is roughly 90% of the size of the original data.;</li> <li>=1 : Don't scale the data. The model will create synthetic data that is roughly the same size as the original data.;</li> <li>&gt;1 : Scale the data by the specified factor. For example, 2.5 will create synthetic data that is roughly  2.5x the size of the original data.;</li> </ul> In\u00a0[9]: Copied! <pre>synthetic_data = synthesizer.sample(\n    scale=2.5\n)\n\nsynthetic_data\n</pre> synthetic_data = synthesizer.sample(     scale=2.5 )  synthetic_data Out[9]: <pre>{'people':           id                name   \n 0          0       Brent Collins  \\\n 1          1         Carlos Mata   \n 2          2  Mrs. Crystal Blair   \n 3          3    Nathaniel Murphy   \n 4          4    Shannon Mitchell   \n ...      ...                 ...   \n 11995  11995     Misty Dominguez   \n 11996  11996       Roberto Brown   \n 11997  11997       Deborah Smith   \n 11998  11998       William Mason   \n 11999  11999          Todd Price   \n \n                                                  address                city  \n 0      6157 Clark Rest\\nSouth Christophershire, MT 67360        New Johnfurt  \n 1                       PSC 8737, Box 1740\\nAPO AA 35924           Jacobland  \n 2              922 Smith Union\\nPort Shawnbury, DE 51299           Holdenton  \n 3             3937 William Mount\\nCohenborough, KY 49299         West Mariah  \n 4              09073 Manning Vista\\nGravesport, MT 89179   South Thomashaven  \n ...                                                  ...                 ...  \n 11995             7945 Wright Ford\\nMartinbury, AK 83553          New Steven  \n 11996  335 Williams Mills Suite 707\\nSarahland, GA 05413         Garciashire  \n 11997           971 Timothy Coves\\nPort Daniel, UT 86478  West Calvinchester  \n 11998            499 Erica Drive\\nLake Carolyn, PA 58154         Morganmouth  \n 11999      13150 Pamela Walk\\nNorth Joshuaside, ND 17432     East Dianeville  \n \n [12000 rows x 4 columns],\n 'strokes':           id  person_id  gender  age  hypertension  heart_disease   \n 0          0          0    Male   41             0              0  \\\n 1          1          1  Female   22             0              0   \n 2          2          2    Male   42             1              1   \n 3          3          3    Male   71             0              0   \n 4          4          4  Female    2             0              0   \n ...      ...        ...     ...  ...           ...            ...   \n 11995  11995      11995   Other   52             0              0   \n 11996  11996      11996    Male   26             0              0   \n 11997  11997      11997  Female   40             0              0   \n 11998  11998      11998  Female   79             0              0   \n 11999  11999      11999    Male   59             0              0   \n \n       ever_married      work_type Residence_type  avg_glucose_level   bmi   \n 0               No       children          Rural              68.03  27.1  \\\n 1              Yes   Never_worked          Urban             189.96  22.5   \n 2               No       Govt_job          Rural              90.20  36.4   \n 3              Yes   Never_worked          Rural             175.26  31.3   \n 4               No   Never_worked          Rural              64.72  16.2   \n ...            ...            ...            ...                ...   ...   \n 11995           No   Never_worked          Urban              62.05  29.4   \n 11996          Yes       Govt_job          Urban             122.06  17.0   \n 11997          Yes        Private          Urban             187.24  39.3   \n 11998          Yes  Self-employed          Rural             147.39  27.7   \n 11999          Yes       Govt_job          Urban              71.11  33.2   \n \n         smoking_status  stroke  \n 0         never smoked       0  \n 1              Unknown       0  \n 2      formerly smoked       1  \n 3              Unknown       0  \n 4              Unknown       0  \n ...                ...     ...  \n 11995          Unknown       0  \n 11996          Unknown       0  \n 11997  formerly smoked       0  \n 11998     never smoked       0  \n 11999  formerly smoked       0  \n \n [12000 rows x 13 columns]}</pre> <p>If you search for this info on the original data you can see that it's not there since we marked name, address and city as PII.</p> Evaluation In\u00a0[9]: Copied! <pre>from sdv.evaluation.multi_table import evaluate_quality\n\nquality_report = evaluate_quality(\n    real_data=datasets,\n    synthetic_data=synthetic_data,\n    metadata=metadata)\n</pre> from sdv.evaluation.multi_table import evaluate_quality  quality_report = evaluate_quality(     real_data=datasets,     synthetic_data=synthetic_data,     metadata=metadata) <pre>Creating report: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00&lt;00:00, 15.89it/s]\n</pre> <pre>\nOverall Quality Score: 91.12%\n\nProperties:\nColumn Shapes: 90.35%\nColumn Pair Trends: 83.01%\nParent Child Relationships: 100.0%\n</pre> In\u00a0[10]: Copied! <pre>from sdv.evaluation.multi_table import run_diagnostic\n\ndiagnostic_report = run_diagnostic(\n    real_data=datasets,\n    synthetic_data=synthetic_data,\n    metadata=metadata)\n</pre> from sdv.evaluation.multi_table import run_diagnostic  diagnostic_report = run_diagnostic(     real_data=datasets,     synthetic_data=synthetic_data,     metadata=metadata) <pre>Creating report: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:28&lt;00:00,  7.15s/it]</pre> <pre>\nDiagnosticResults:\n\nSUCCESS:\n\u2713 The synthetic data covers over 90% of the numerical ranges present in the real data\n\u2713 The synthetic data covers over 90% of the categories present in the real data\n\u2713 Over 90% of the synthetic rows are not copies of the real data\n\u2713 The synthetic data follows over 90% of the min/max boundaries set by the real data\n</pre> <pre>\n</pre> In\u00a0[11]: Copied! <pre>diagnostic_report.get_results()\n</pre> diagnostic_report.get_results() Out[11]: <pre>{'SUCCESS': ['The synthetic data covers over 90% of the numerical ranges present in the real data',\n  'The synthetic data covers over 90% of the categories present in the real data',\n  'Over 90% of the synthetic rows are not copies of the real data',\n  'The synthetic data follows over 90% of the min/max boundaries set by the real data'],\n 'WARNING': [],\n 'DANGER': []}</pre> In\u00a0[12]: Copied! <pre>diagnostic_report.get_properties()\n</pre> diagnostic_report.get_properties() Out[12]: <pre>{'Coverage': 0.9733928657693466, 'Synthesis': 1.0, 'Boundaries': 1.0}</pre> In\u00a0[13]: Copied! <pre>from rdt.transformers.pii import AnonymizedFaker\n\nsynthesizer.update_transformers(table_name=\"people\", column_name_to_transformer={\n    'name': AnonymizedFaker(provider_name='person', function_name='name', locales=['fr_CA'])\n})\n\nsynthesizer.fit(datasets)\n</pre> from rdt.transformers.pii import AnonymizedFaker  synthesizer.update_transformers(table_name=\"people\", column_name_to_transformer={     'name': AnonymizedFaker(provider_name='person', function_name='name', locales=['fr_CA']) })  synthesizer.fit(datasets) <pre>/Users/vascopais/Library/Caches/pypoetry/virtualenvs/synthetic-data-EqHpLbmO-py3.10/lib/python3.10/site-packages/sdv/single_table/base.py:292: UserWarning: For this change to take effect, please refit the synthesizer using `fit`.\n  warnings.warn(msg, UserWarning)\n/Users/vascopais/Library/Caches/pypoetry/virtualenvs/synthetic-data-EqHpLbmO-py3.10/lib/python3.10/site-packages/sdv/single_table/base.py:292: UserWarning: For this change to take effect, please refit the synthesizer using `fit`.\n  warnings.warn(msg, UserWarning)\n/Users/vascopais/Library/Caches/pypoetry/virtualenvs/synthetic-data-EqHpLbmO-py3.10/lib/python3.10/site-packages/sdv/single_table/base.py:292: UserWarning: For this change to take effect, please refit the synthesizer using `fit`.\n  warnings.warn(msg, UserWarning)\n</pre> <p>Then we generate the data once again (this time a smaller sample). And we can see that the names on the people table are now french canadian names.</p> In\u00a0[14]: Copied! <pre>synthesizer.sample(scale=0.01)\n</pre> synthesizer.sample(scale=0.01) Out[14]: <pre>{'people':     id                         name   \n 0    0           Emmanuelle Poirier  \\\n 1    1                  Maude Blais   \n 2    2     Caroline Trottier-Lemire   \n 3    3                \u00c9douard Soucy   \n 4    4       Alexis-Emmanuel S\u00e9guin   \n 5    5             P\u00e9n\u00e9lope Gervais   \n 6    6   Doroth\u00e9e Marcoux-Larivi\u00e8re   \n 7    7                Julien Larose   \n 8    8              Mich\u00e8le Couture   \n 9    9              Susanne St-Onge   \n 10  10        Jeannine-Manon Daigle   \n 11  11       Martin Gingras-Provost   \n 12  12     Emmanuel Dionne-Rodrigue   \n 13  13               Mich\u00e8le B\u00e9rub\u00e9   \n 14  14               Josette B\u00e9rub\u00e9   \n 15  15      Juliette-Sylvie Bernier   \n 16  16               Pauline Nadeau   \n 17  17            Marcel L\u00e9tourneau   \n 18  18              Jacques Fortier   \n 19  19                 Louis Lepage   \n 20  20             C\u00e9line Arsenault   \n 21  21           Xavier-Jean Larose   \n 22  22       William B\u00e9dard-Germain   \n 23  23                 Maude Lepage   \n 24  24               Mathieu Chabot   \n 25  25  Thomas Champagne-Morissette   \n 26  26          Timoth\u00e9e Robitaille   \n 27  27             Robert Arsenault   \n 28  28           Henriette L\u00e9vesque   \n 29  29                   R\u00e9my Morin   \n 30  30                Nathan Blouin   \n 31  31        Alex Bernard-Rousseau   \n 32  32                 Henri Bisson   \n 33  33                Maxime Savard   \n 34  34             Roger Robitaille   \n 35  35          Yves B\u00e9land-Paradis   \n 36  36       Tristan Roberge-Dumont   \n 37  37                  No\u00ebl Dufour   \n 38  38                 Aurore Lebel   \n 39  39              Bertrand Lemire   \n 40  40      Thomas Lemieux-Fournier   \n 41  41             Bertrand Gilbert   \n 42  42              Nicolas Lussier   \n 43  43            Alexandre Lacroix   \n 44  44              Thomas Gauthier   \n 45  45             Olivia Beauchamp   \n 46  46                Jules Boucher   \n 47  47            Constance Leclerc   \n \n                                               address                 city  \n 0   6157 Clark Rest\\nSouth Christophershire, MT 67360         New Johnfurt  \n 1                    PSC 8737, Box 1740\\nAPO AA 35924            Jacobland  \n 2           922 Smith Union\\nPort Shawnbury, DE 51299            Holdenton  \n 3          3937 William Mount\\nCohenborough, KY 49299          West Mariah  \n 4           09073 Manning Vista\\nGravesport, MT 89179    South Thomashaven  \n 5   2619 White Fields Apt. 532\\nSouth Amandaport, ...        New Taraville  \n 6          23276 Billy Plains\\nMartinezberg, LA 87469         Michaelshire  \n 7    0704 Smith Walks Apt. 228\\nMillerburgh, WY 89418          Danielburgh  \n 8               521 Sara Street\\nHectorside, DC 24587          Robertsland  \n 9    174 Velasquez Court\\nEast Jenniferport, ID 40915        Marvinchester  \n 10               153 Yu Island\\nPalmermouth, MS 89535       South Kimberly  \n 11        48142 Timothy Summit\\nShannonview, MI 72357       Gutierrezshire  \n 12       6397 Melissa Circle\\nPort Benjamin, DE 42448             New Jack  \n 13          55144 Brooks Walk\\nSouth Andrew, OK 36891            Nelsonton  \n 14       9230 Garza Parks\\nPort Nataliefurt, TX 81751            Martinton  \n 15           61788 Freeman Mill\\nMillerland, WI 27888         Lake Jeffrey  \n 16        409 Hodges Street\\nParrishborough, ME 97469            Port Cody  \n 17             89649 Joseph Path\\nRickyland, NC 57649          West Andrea  \n 18  4685 Hawkins Haven Suite 592\\nRiveraberg, KS 0...           Port Tanya  \n 19    83913 Patricia Gardens\\nEricksonville, IN 08631        Andersonshire  \n 20          2086 Mahoney Unions\\nEricaburgh, WV 53247           Port Katie  \n 21  26457 Wendy Wells Apt. 903\\nNorth Megan, OH 70806         North Thomas  \n 22   33704 Tiffany Tunnel\\nLake Anthonyfort, TX 93085           East Sarah  \n 23            714 Wendy Estate\\nJoseborough, KY 03379             Jameston  \n 24                   Unit 5426 Box 7165\\nDPO AP 37872  West Jacquelinefurt  \n 25       9589 Elizabeth Springs\\nKevinville, MI 72107           East Jason  \n 26      336 Franklin Crossroad\\nMelissaberg, KY 57917         Lake Melissa  \n 27         5328 Lauren Valley\\nNew Ryanport, TN 14377     South Karenville  \n 28  106 Carney Views Suite 400\\nGarciamouth, CO 56012            Smithview  \n 29     599 Jason Ford Suite 531\\nEast Tammy, OK 38545        Hamiltonshire  \n 30  2642 Graham Plains Apt. 732\\nMullenchester, MN...         West Lindsey  \n 31      3418 Byrd Loop Suite 726\\nWardhaven, MO 11008  New Kathleenborough  \n 32  9012 Laura Viaduct Apt. 726\\nGrahamland, MI 86340     North Glendafurt  \n 33  2656 Mcmillan Wall\\nEast Douglasborough, TX 75230          Brandonside  \n 34             1191 David Rest\\nEast Lauren, MO 75586           Brownmouth  \n 35      0479 Jensen Alley\\nChristophermouth, MO 57533     East Matthewbury  \n 36                            USNS Ross\\nFPO AE 38922        East Jennifer  \n 37  533 Wang Junction Suite 546\\nLake Michaelshire...         East Richard  \n 38  115 Webb Springs Suite 300\\nAngelaville, FL 58789     Port Heatherfort  \n 39                   PSC 5539, Box 0143\\nAPO AA 19955         Melissashire  \n 40  69349 Lisa Mountains Apt. 851\\nRobinsonfurt, C...             Wangport  \n 41         76314 Hernandez Lock\\nHolmestown, MO 88896       South Jilltown  \n 42    390 John Orchard Apt. 594\\nRobertside, MO 87023         South Robert  \n 43                   Unit 8573 Box 9313\\nDPO AE 22469          Schultzbury  \n 44   144 Richard Fields Apt. 599\\nWeissside, MO 19466      Christopherview  \n 45        227 Doyle Islands\\nTimothychester, NM 14247            Calebview  \n 46  397 Thompson Springs Apt. 535\\nDestinymouth, N...   South Mitchellberg  \n 47   3376 Garrett Crescent\\nBenjaminborough, MO 03106        Austinborough  ,\n 'strokes':     id  person_id  gender  age  hypertension  heart_disease ever_married   \n 0    0          0    Male   59             0              0          Yes  \\\n 1    1          1    Male   36             0              0          Yes   \n 2    2          2    Male   33             0              0           No   \n 3    3          3  Female   39             0              0          Yes   \n 4    4          4    Male   65             0              0          Yes   \n 5    5          5  Female   72             0              0          Yes   \n 6    6          6    Male   65             0              0          Yes   \n 7    7          7    Male   49             0              1           No   \n 8    8          8  Female   75             0              0          Yes   \n 9    9          9  Female   37             0              0          Yes   \n 10  10         10    Male   36             0              0           No   \n 11  11         11    Male   58             0              0          Yes   \n 12  12         12  Female   60             0              0           No   \n 13  13         13    Male   48             0              0           No   \n 14  14         14    Male   78             1              1          Yes   \n 15  15         15    Male   63             0              0          Yes   \n 16  16         16    Male   11             1              0           No   \n 17  17         17  Female   13             0              0           No   \n 18  18         18  Female    3             0              0           No   \n 19  19         19  Female   35             0              0          Yes   \n 20  20         20  Female   18             0              0          Yes   \n 21  21         21  Female   47             0              0          Yes   \n 22  22         22    Male   33             1              0           No   \n 23  23         23    Male   17             0              1           No   \n 24  24         24  Female   52             0              0           No   \n 25  25         25    Male   72             0              0          Yes   \n 26  26         26    Male   35             0              0          Yes   \n 27  27         27    Male   61             0              0           No   \n 28  28         28  Female   71             0              0          Yes   \n 29  29         29  Female   76             0              0          Yes   \n 30  30         30   Other    2             0              0           No   \n 31  31         31    Male   30             1              1           No   \n 32  32         32  Female   29             0              0           No   \n 33  33         33  Female   81             1              0          Yes   \n 34  34         34    Male   10             0              1           No   \n 35  35         35    Male   68             0              0           No   \n 36  36         36  Female   21             0              0           No   \n 37  37         37  Female   44             0              0          Yes   \n 38  38         38  Female   64             0              0           No   \n 39  39         39  Female   37             0              0          Yes   \n 40  40         40   Other   63             0              0           No   \n 41  41         41  Female   35             0              0          Yes   \n 42  42         42    Male   11             0              0           No   \n 43  43         43    Male   27             0              0          Yes   \n 44  44         44  Female   82             0              0          Yes   \n 45  45         45  Female   70             0              0           No   \n 46  46         46  Female   81             0              0          Yes   \n 47  47         47   Other    7             0              0           No   \n \n         work_type Residence_type  avg_glucose_level   bmi   smoking_status   \n 0         Private          Rural             105.99  26.2     never smoked  \\\n 1        children          Urban             124.18  24.8  formerly smoked   \n 2         Private          Urban              56.59  21.5          Unknown   \n 3   Self-employed          Urban              92.89  22.2     never smoked   \n 4        children          Rural             149.89  31.9          Unknown   \n 5   Self-employed          Rural             134.48  23.2     never smoked   \n 6        children          Urban             139.36  34.5           smokes   \n 7        children          Rural             167.76  31.2  formerly smoked   \n 8        children          Urban             102.79  37.2     never smoked   \n 9         Private          Urban              87.34  35.7          Unknown   \n 10        Private          Urban              63.28  18.4     never smoked   \n 11       children          Rural             205.38  28.4          Unknown   \n 12        Private          Urban             132.14  23.3           smokes   \n 13  Self-employed          Urban              81.61  14.0     never smoked   \n 14       children          Rural             114.62  30.1           smokes   \n 15  Self-employed          Urban             137.29  37.7           smokes   \n 16   Never_worked          Urban             135.24  20.4          Unknown   \n 17        Private          Urban              87.55  24.0     never smoked   \n 18        Private          Rural              55.88  24.1          Unknown   \n 19  Self-employed          Urban              56.21  38.0     never smoked   \n 20       children          Urban              94.29  21.9          Unknown   \n 21        Private          Urban             147.00  36.8  formerly smoked   \n 22       children          Rural             127.40  23.7  formerly smoked   \n 23       Govt_job          Urban              57.79  19.4     never smoked   \n 24       children          Rural             121.97  38.2  formerly smoked   \n 25  Self-employed          Urban              57.00  15.8     never smoked   \n 26       children          Rural             212.33  54.7     never smoked   \n 27  Self-employed          Rural              88.19  35.0           smokes   \n 28        Private          Urban              98.38  32.7     never smoked   \n 29  Self-employed          Urban              73.53  32.4     never smoked   \n 30   Never_worked          Rural             182.09  21.4           smokes   \n 31       children          Rural             117.16  23.3           smokes   \n 32       Govt_job          Rural             121.17  22.7     never smoked   \n 33        Private          Rural             202.05  32.9     never smoked   \n 34  Self-employed          Urban              62.05  18.7          Unknown   \n 35        Private          Rural              71.35  32.8     never smoked   \n 36       children          Urban             171.71  26.4     never smoked   \n 37  Self-employed          Rural             123.41  29.3           smokes   \n 38        Private          Urban              78.75  21.8     never smoked   \n 39   Never_worked          Rural              69.86  19.8          Unknown   \n 40  Self-employed          Rural             126.36  23.7     never smoked   \n 41  Self-employed          Urban             155.57  46.4           smokes   \n 42        Private          Urban              75.05  22.3          Unknown   \n 43  Self-employed          Urban              77.96  30.5     never smoked   \n 44        Private          Urban             113.75  26.0  formerly smoked   \n 45  Self-employed          Urban             105.87  41.3           smokes   \n 46  Self-employed          Rural             220.69  29.4     never smoked   \n 47       children          Rural             103.11  23.4          Unknown   \n \n     stroke  \n 0        0  \n 1        0  \n 2        0  \n 3        0  \n 4        0  \n 5        0  \n 6        0  \n 7        0  \n 8        0  \n 9        0  \n 10       0  \n 11       0  \n 12       0  \n 13       0  \n 14       0  \n 15       0  \n 16       0  \n 17       0  \n 18       0  \n 19       0  \n 20       0  \n 21       0  \n 22       1  \n 23       1  \n 24       0  \n 25       0  \n 26       0  \n 27       0  \n 28       0  \n 29       0  \n 30       1  \n 31       1  \n 32       0  \n 33       0  \n 34       0  \n 35       1  \n 36       1  \n 37       0  \n 38       0  \n 39       0  \n 40       0  \n 41       0  \n 42       0  \n 43       0  \n 44       0  \n 45       0  \n 46       0  \n 47       0  }</pre>"},{"location":"python/machine_learning/synthethic_data/3_multi_table/#multi-table-synthetic-data","title":"Multi Table Synthetic Data\u00b6","text":""},{"location":"python/machine_learning/synthethic_data/3_multi_table/#data-analysis","title":"Data analysis\u00b6","text":"<p>For multitable we'll be using the strokes table used on the single table analysis and a people table. For the people table each entry has an ID which matches the person_id on the strokes table. This table also has some PII (Personal Identifiable Information) information which is important for us to masquerade.</p>"},{"location":"python/machine_learning/synthethic_data/3_multi_table/#load-data","title":"Load Data\u00b6","text":"<p>First, we go to the content folder and get all csv files there.</p>"},{"location":"python/machine_learning/synthethic_data/3_multi_table/#create-metadata","title":"Create metadata\u00b6","text":"<p>We then need to create the metadata object to be used when creating the synthesizer. SDV will detect some information from the table content but it may not be correct. It's always best to check the metadata and fix whatever needs to be fixed.</p>"},{"location":"python/machine_learning/synthethic_data/3_multi_table/#edit-metadata","title":"Edit Metadata\u00b6","text":""},{"location":"python/machine_learning/synthethic_data/3_multi_table/#strokes-table","title":"Strokes Table\u00b6","text":"<p>Bellow, we make a few changes such as:</p> <ul> <li>Change column id to type id;</li> <li>Change column age to a numerical type of integers;</li> <li>Change column bmi to a numerical type of floats;</li> <li>Change column stroke to a categorical column since the information is either 0 or 1;</li> <li>Change hypertension and heart_disease columns to categorical as well for the same reason.</li> </ul>"},{"location":"python/machine_learning/synthethic_data/3_multi_table/#people-table","title":"People Table\u00b6","text":"<p>Bellow, we make a few changes to people type such as:</p> <ul> <li>Change column id to type id;</li> <li>Change column name to type name and mark as PII;</li> <li>Change column address to type address and mark as PII;</li> <li>Change column city to type city and mark as PII.</li> </ul> <p>We need to add pii to the columns because they are personable identifiable information.</p>"},{"location":"python/machine_learning/synthethic_data/3_multi_table/#create-synthesizer","title":"Create Synthesizer\u00b6","text":"<p>Having created the metadata object we then needed to create the synthesizer which will be trained to generate the synthetic data. Here we use the only possible (excluding the enterprise edition) synthesizer - HMA. Note that you can configure which synthesizer each table uses.</p>"},{"location":"python/machine_learning/synthethic_data/3_multi_table/#pii-locales","title":"PII Locales\u00b6","text":"<p>It's also possible to generate data from certain languages on certain countries. For example if we want only french canadian names here's how to proceed.</p> <p>We create a AnonymizedFaker which receives the provider and function names from the Python Faker Library and a locales array with possible language_countries. And then we retrain the synthesizer for our changes to be learned.</p>"},{"location":"python/machine_learning/synthethic_data/3_multi_table/#conclusion","title":"Conclusion\u00b6","text":"<p>This time on the multi table generation we got a better coverage percentage on the quality report. This may be because of:</p> <ul> <li>Having 2.5x the synthetic data compared to the single table example;</li> <li>Using the GaussianCopula model for each table with HMA on multi table data generation vs. using the GaussianCopula model with FastML preset on single table data generation. The latter uses a normal distribution, no enforced rounding and the categorical transformer is a FrequencyEncoder instead of a LabelEncoder. More about these encoders on the source code.</li> </ul> <p>It's also important to note that even though the SDV library has a tool to evaluate the quality of the data created we should have a third party library to also test this quality in order to be as unbiased as possible.</p> <p>Now we know how to create synthetic data and how quick and secure it can be.</p>"},{"location":"python/testing/1_different_ways_of_testing/","title":"Different ways of testing","text":""},{"location":"python/testing/1_different_ways_of_testing/#manual-testing","title":"Manual Testing","text":""},{"location":"python/testing/1_different_ways_of_testing/#manual-exploratory-testing","title":"Manual Exploratory Testing","text":"<p>This type of testing is just an exploration of the application, relies on discovery, investigation and learning. It emphasizes personal freedom and responsibility of the individual tester. </p>"},{"location":"python/testing/1_different_ways_of_testing/#manual-testing_1","title":"Manual Testing","text":"<p>In this case, the test suit is a list of all the features, different scenarios, different input types and the expected results. All the validations are performed by you, the developer, which means that every time something changes, you need to go over your test suit list and ensure that you get what you'd expect.</p>"},{"location":"python/testing/1_different_ways_of_testing/#automated","title":"Automated","text":""},{"location":"python/testing/1_different_ways_of_testing/#unit-tests","title":"Unit Tests","text":"<p>Tests a particular component in your application, this is the smallest piece of code that can be logically isolated in a system, typically: a function, a subroutine, a method or property.</p>"},{"location":"python/testing/1_different_ways_of_testing/#integration-tests","title":"Integration Tests","text":"<p>Different domains, modules or components of your application are tested as a whole.</p>"},{"location":"python/testing/2_sample_ticket/","title":"Sample Ticket","text":"<p>Let's take a hypothetical ticket where we're required to create a very simple report generator.  </p>"},{"location":"python/testing/2_sample_ticket/#report-generator","title":"Report Generator","text":""},{"location":"python/testing/2_sample_ticket/#requirements","title":"Requirements","text":"<p>Develop the necessary functionality to create a report generator using Jinja2.  This report must take as parameters: </p> <ul> <li>title: string with the title of the report</li> <li>description: string with the description of the report</li> <li>data: string generated from a pandas DataFrame</li> </ul> <p>The template must follow the structure below</p> <pre><code>Report\n\n{{title}}\n\n{{description}}\n\n{{data}}\n</code></pre>"},{"location":"python/testing/2_sample_ticket/#acceptance-criteria","title":"Acceptance Criteria","text":"<ul> <li> <p>Must follow the following code principles</p> <ul> <li>Don't repeat yourself (DRY)</li> <li>Single-responsibility principle (SRP)</li> </ul> </li> <li> <p>Must include unit test</p> </li> </ul>"},{"location":"python/testing/2_sample_ticket/#sample-report","title":"Sample Report","text":"<pre><code>Report\n\nVery Important Report\n\nSample description\n\n   col1  col2\n0     1     3\n1     2     4\n</code></pre>"},{"location":"python/testing/2_sample_ticket/#initial-approach","title":"Initial approach","text":"<p>In our initial approach we're simple playing around with Jinja2, to make sure we can build what we're required. After a quick look into the Jinja2 documentation we come up with the following working example.</p> <pre><code>from jinja2.nativetypes import NativeEnvironment\nimport pandas as pd\n\nTEMPLATE = \"\"\"\nSample Report\n\n{{title}}\n\n{{description}}\n\n{{data}}\n\"\"\"\n\ntitle = \"Very Important Report\"\ndescription = \"Sample description\"\ndf = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n\nenv = NativeEnvironment()\ntemplate = env.from_string(TEMPLATE)\nreport = template.render(title=title, description=description, data=df.to_string())\nprint(report)\n</code></pre> <p>Which produces the following result </p> <pre><code>Report\n\nVery Important Report\n\nSample description\n\n   col1  col2\n0     1     3\n1     2     4\n</code></pre> <p>Even though it performs as expected, it doesn't follow the acceptance criteria.</p> <p> Must follow the Don't repeat yourself (DRY) code principle</p> <p> Must follow the Single-responsibility principle (SRP) code principle</p> <p> Must include unit test</p>"},{"location":"python/testing/2_sample_ticket/#turn-it-into-a-function","title":"Turn it into a function","text":"<p>After concluding that the acceptance criteria isn't met, we quickly conclude that a function might do the trick.</p> <pre><code>from jinja2.nativetypes import NativeEnvironment\nimport pandas as pd\n\nTEMPLATE = \"\"\"\nReport\n\n{{title}}\n\n{{description}}\n\n{{data}}\n\"\"\"\n\ntitle = \"Very Important Report\"\ndescription = \"Sample description\"\ndf = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n\ndef generate_report(title, description, data):\n    env = NativeEnvironment()\n    template = env.from_string(TEMPLATE)\n    return template.render(title=title, descritpion=description, data=data)\n\ngenerate_report(title, description, df.to_string())\n</code></pre> <p>Time to test it! Since Unitest is the default Unit testing framework in Python, we'll use it in this exercise.</p> <pre><code>import unittest\nfrom unittest import TestCase\n\nclass GenerateReportTest(TestCase):\n    def test_success(self):\n        title = \"sample title\"\n        description = \"sample description\"\n        data = \"42\"\n\n        report = generate_report(title, description, data)\n        self.assertTrue(title in report)\n        self.assertTrue(description in report)\n        self.assertTrue(data in report)\n\n    def test_failure(self):\n        generate_report(None, None, None)\n\nif __name__ == '__main__':\n    unittest.main()\n</code></pre> <p>The TestCase includes two tests, one for the case we consider to be a success and another for the case we consider to be a failure. </p> <p>The success asserts that whatever that was used to generate a report is present in it.</p> <p>On the other hand, the failure asserts by means of a successful execution that is possible to generate an empty report.</p>"},{"location":"python/testing/2_sample_ticket/#validations-galore","title":"Validations galore","text":"<p>Time to ensure that whatever report we generate is meaningful, which means we need to validate our three parameters.</p> <p>Since we need all three to exist, let's add the following snippet which breaks if at least one of them is <code>None</code>.</p> <pre><code>if title is None or description is None or data is None:\n    return\n</code></pre> <p>The updated generate report function. </p> <pre><code>def generate_report(title, description, data):\n    if title is None or description is None or data is None:\n        return\n    env = NativeEnvironment()\n    template = env.from_string(TEMPLATE)\n    return template.render(title=title, description=description, data=data)\n</code></pre> <p>The existing tests should pass as they are, however we should improve the failure test. Now we want to assert that if at least one of the parameters is <code>None</code>, the response is <code>None</code> as well.</p> <pre><code>import unittest\nfrom unittest import TestCase\n\nclass GenerateReportTest(TestCase):\n    def setUp(self):\n        self.title = \"sample title\"\n        self.description = \"sample description\"\n        self.data = \"42\"\n\n    def test_success(self):\n        report = generate_report(self.title, self.description, self.data)\n        self.assertTrue(self.title in report)\n        self.assertTrue(self.description in report)\n        self.assertTrue(self.data in report)\n\n    def test_failure_1_params(self):\n        self.assertIsNone(generate_report(self.title, None, self.data))\n\n    def test_failure_3_params(self):\n        self.assertIsNone(generate_report(None, None, None))\n\nif __name__ == '__main__':\n    unittest.main()\n</code></pre> <p>Since <code>title</code>, <code>description</code> and <code>data</code> are used in two different tests they're added to setUp method.</p> <p>After grouping the logic in a function, adding parameters validation and tests, it's time to revisit the Acceptance criteria. </p> <p> Must follow the Don't repeat yourself (DRY) code principle</p> <p> Must follow the Single-responsibility principle (SRP) code principle, the <code>generate_report</code> functions is doing all the work, the validation and the generation of the report.</p> <p> Must include unit test</p>"},{"location":"python/testing/2_sample_ticket/#set-responsibilities","title":"Set responsibilities","text":"<p>Instead of having a function for validation and another for generation, we're going to create a class to handle the generation of the report.</p> <pre><code>import pandas as pd\nfrom jinja2.nativetypes import NativeEnvironment\nfrom dataclasses import dataclass\n\nTEMPLATE = \"\"\"\nReport\n\n{{title}}\n\n{{description}}\n\n{{data}}\n\"\"\"\n\ntitle = \"Very Important Report\"\ndescription = \"Sample description\"\ndf = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n\n@dataclass\nclass GenerateReport:\n    title: str\n    description: str\n    data: str\n\n    def __post_init__(self):\n        self._validate_input()\n        self._get_template()\n\n    def _validate_input(self):\n        if self.title is None or self.description is None or self.data is None:\n            raise Exception(\"Parameters are missing\")\n\n    def _get_template(self):\n        env = NativeEnvironment()\n        self.template = env.from_string(TEMPLATE)\n\n    def __call__(self, *args, **kwargs):\n        return self.template.render(title=self.title, description=self.description, data=self.data)\n\ngenerate_report = GenerateReport(title, description, df.to_string())\nprint(generate_report())\n</code></pre> <p>Let's update the tests to follow up on the changes we've just performed.</p> <pre><code>import unittest\nfrom unittest import TestCase\n\nclass GenerateReportTest(TestCase):\n    def setUp(self):\n        self.title = \"sample title\"\n        self.description = \"sample description\"\n        self.data = \"42\"\n\n    def test_success(self):\n        report = GenerateReport(self.title, self.description, self.data)()\n        self.assertTrue(self.title in report)\n        self.assertTrue(self.description in report)\n        self.assertTrue(self.data in report)\n\n    def test_failure_1_params(self):\n        report = GenerateReport(self.title, None, self.data)()\n        self.assertIsNone(report)\n\n    def test_failure_3_params(self):\n        report = GenerateReport(None, None, None)()\n        self.assertIsNone(report)\n\nif __name__ == '__main__':\n    unittest.main()\n</code></pre> <p>However the tests are now failing</p> <pre><code>test_failure_1_params (__main__.GenerateReportTest) ... ERROR\ntest_failure_3_params (__main__.GenerateReportTest) ... ERROR\ntest_success (__main__.GenerateReportTest) ... ok\n</code></pre> <p>What changed? Previously we're just returning <code>None</code> </p> <pre><code>if title is None or description is None or data is None:\n    return\n</code></pre> <p>while now, we're raising and exception</p> <pre><code>def _validate_input(self):\n    if self.title is None or self.description is None or self.data is None:\n        raise Exception(\"Parameters are missing\")\n</code></pre> <p>Let's update the tests to follow on that update</p> <pre><code>import unittest\nfrom unittest import TestCase\n\nclass GenerateReportTest(TestCase):\n    def setUp(self):\n        self.title = \"sample title\"\n        self.description = \"sample description\"\n        self.data = \"42\"\n\n    def test_success(self):\n        report = GenerateReport(self.title, self.description, self.data)()\n        self.assertTrue(self.title in report)\n        self.assertTrue(self.description in report)\n        self.assertTrue(self.data in report)\n\n    def test_failure_1_params(self):\n        with self.assertRaises(Exception) as cm:\n            GenerateReport(self.title, None, self.data)()\n\n        self.assertEqual(cm.exception.args[0], \"Parameters are missing\")\n\n    def test_failure_3_params(self):\n        with self.assertRaises(Exception) as cm:\n            GenerateReport(None, None, None)()\n\n        self.assertEqual(cm.exception.args[0], \"Parameters are missing\")\n\nif __name__ == '__main__':\n    unittest.main()\n</code></pre> <p>What if the report variables change? How many changes are necessary to perform in our code?</p> <p>Now we're clearly following the single-responsibility principle, but is there room for improvement? If something fails in the validation, we don't know for sure what failed so let's add that.</p> <p>Instead of having a very simple validation, such as</p> <pre><code>def _validate_input(self):\n    if self.title is None or self.description is None or self.data is None:\n        raise Exception(f\"Parameters are missing\")\n</code></pre> <p>What's changing:</p> <ul> <li>we're replacing all the class parameters by a dictionary</li> <li>we're setting all the expected template variables to have a reference of what is expected </li> </ul> <pre><code>data: dict\nTEMPLATE_VARIABLES: ClassVar[set] = {\"title\", \"description\", \"data\"}\n\ndef _validate_input(self):\n    missing = set(self.data.keys()) ^ self.TEMPLATE_VARIABLES\n    if missing:\n        raise Exception(f\"The following keys are missing: {missing}\")\n</code></pre> <p>Here's our updated class for the report generator.</p> <pre><code>import pandas as pd\n\nfrom jinja2.nativetypes import NativeEnvironment\nfrom typing import ClassVar\nfrom dataclasses import dataclass\n\nTEMPLATE = \"\"\"\nReport\n\n{{title}}\n\n{{description}}\n\n{{data}}\n\"\"\"\n\ntitle = \"Very Important Report\"\ndescription = \"Sample description\"\ndf = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n\ndata = {\"title\": title, \"description\": description, \"data\": df.to_string()}\n\n@dataclass\nclass GenerateReport:\n    data: dict\n    TEMPLATE_VARIABLES: ClassVar[set] = {\"title\", \"description\", \"data\"}\n\n    def __post_init__(self):\n        self._validate_input()\n        self._get_template()\n\n    def _validate_input(self):\n        missing = set(self.data.keys()) ^ self.TEMPLATE_VARIABLES\n        if missing:\n            raise Exception(f\"The following keys are missing: {missing}\")\n\n    def _get_template(self):\n        env = NativeEnvironment()\n        self.template = env.from_string(TEMPLATE)\n\n    def __call__(self, *args, **kwargs):\n        return self.template.render(**self.data)\n\nreport = GenerateReport(data)()\nprint(report)\n</code></pre> <p>And let's add a test for it </p> <pre><code>import unittest\nfrom unittest import TestCase\n\nclass GenerateReportTest(TestCase):\n    def setUp(self):\n        self.title = \"sample title\"\n        self.description = \"sample description\"\n        self.data = \"42\"\n\n        self.template_data = {\"title\": self.title,\n        \"description\": self.description,\n        \"data\": self.data}\n\n    def test_success(self):\n        report = GenerateReport(self.template_data)()\n        self.assertTrue(self.title in report)\n        self.assertTrue(self.description in report)\n        self.assertTrue(self.data in report)\n\n    def test_failure_1_params(self):\n        self.template_data.pop('description')\n        with self.assertRaises(Exception) as cm:\n            GenerateReport(self.template_data)()\n\n        self.assertEqual(\n            cm.exception.args[0],\n            \"The following keys are missing: {'description'}\")\n\n    def test_failure_3_params(self):\n        with self.assertRaises(Exception) as cm:\n            GenerateReport({})()\n\n        self.assertEqual(\n            cm.exception.args[0],\n            \"The following keys are missing: {'title', 'description', 'data'}\")\n\nif __name__ == '__main__':\n    unittest.main()\n</code></pre>"},{"location":"python/testing/3_unittest/","title":"Unittest","text":"<p>this document was inspired/copied from of https://docs.python.org/3/library/unittest.html and  https://docs.djangoproject.com/en/4.1/topics/testing/overview/</p> <p>The unittest unit testing framework was originally inspired by JUnit and has a similar flavor as major unit testing  frameworks in other languages. It supports test automation, sharing of setup and shutdown code for tests, aggregation  of tests into collections, and independence of the tests from the reporting framework.</p> <p>Unittest supports some important concepts in an object-oriented way:</p> <ul> <li>test fixture: A test fixture represents the preparation needed to perform one or more tests, and any associated cleanup actions. This may involve, for example, creating temporary or proxy databases, directories, or starting a server process.</li> <li>test case: A test case is the individual unit of testing. It checks for a specific response to a particular set of inputs. unittest provides a base class, TestCase, which may be used to create new test cases.</li> <li>test suite: A test suite is a collection of test cases, test suites, or both. It is used to aggregate tests that should be executed together.</li> <li>test runner: A test runner is a component which orchestrates the execution of tests and provides the outcome to the user. The runner may use a graphical interface, a textual interface, or return a special value to indicate the results of executing the tests.</li> </ul>"},{"location":"python/testing/3_unittest/#anatomy-of-a-test","title":"Anatomy of a test","text":"<pre><code>import unittest\n\nclass TestStringMethods(unittest.TestCase):  # A testcase is created by \n                                             # subclassing unittest.TestCase\n\n    # The individual tests are defined with methods whose names start with the \n    # letters test. This naming convention informs the test runner about which methods \n    # represent tests.\n    def test_upper(self):  \n        self.assertEqual('foo'.upper(), 'FOO')\n\n    def test_isupper(self):\n        self.assertTrue('FOO'.isupper())\n        self.assertFalse('Foo'.isupper())\n\n    def test_split(self):\n        s = 'hello world'\n        self.assertEqual(s.split(), ['hello', 'world'])\n        # check that s.split fails when the separator is not a string\n        with self.assertRaises(TypeError):\n            s.split(2)\n\nif __name__ == '__main__':\n    # provides a command-line interface to the test script\n    unittest.main()\n</code></pre>"},{"location":"python/testing/3_unittest/#fixture","title":"Fixture","text":"<p>A working environment that has a setUp() and tearDown() the testing code is called a test fixture. </p> <p>A new TestCase instance is created as a unique test fixture used to execute each individual test method. Thus  setUp(), tearDown(), and init() will be called once per test.</p> <pre><code>import unittest\n\nclass WidgetTestCase(unittest.TestCase):\n    def setUp(self):\n        self.widget = Widget('The widget')\n\n    def tearDown(self):\n        self.widget.dispose()\n</code></pre> <p>If the setUp() method raises an exception while the test is running, the framework will consider the test to have suffered an error, and the test method will not be executed. If setUp() succeeded, tearDown() will be run  whether the test method succeeded or not.</p> <p>By default, setUp() and tearDown() do nothing.</p>"},{"location":"python/testing/3_unittest/#command-line","title":"Command Line","text":"<p>The unittest module can be used from the command line to run tests from modules, classes or even individual test methods:</p> <pre><code>python -m unittest test_module1 test_module2\npython -m unittest test_module.TestClass\npython -m unittest test_module.TestClass.test_method\n</code></pre> <p>You can pass in a list with any combination of module names, and fully qualified class or method names.</p> <p>Test modules can be specified by file path as well:</p> <pre><code>python -m unittest tests/test_something.py\n</code></pre> <p>This allows you to use the shell filename completion to specify the test module. The file specified must still be importable as a module. The path is converted to a module name by removing the \u2018.py\u2019 and converting path separators into \u2018.\u2019. If you want to execute a test file that isn\u2019t importable as a module you should execute the file directly instead.</p> <p>You can run tests with more detail (higher verbosity) by passing in the -v flag:</p> <pre><code>python -m unittest -v test_module\n</code></pre> <p>When executed without arguments Test Discovery is started:</p> <pre><code>python -m unittest\n</code></pre> <p>For a list of all the command-line options:</p> <pre><code>python -m unittest -h\n</code></pre>"},{"location":"python/testing/3_unittest/#assertions","title":"Assertions","text":"<p>The TestCase class provides several assert methods to check for and report failures.</p> Method Checks that assertEqual(a, b) a == b assertNotEqual(a, b) a != b assertTrue(x) bool(x) is True assertFalse(x) bool(x) is False assertIs(a, b) a is b assertIsNot(a, b) a is not b assertIsNone(x) x is None assertIsNotNone(x) x is not None assertIn(a, b) a in b assertNotIn(a, b) a not in b assertIsInstance(a, b) isinstance(a, b) assertNotIsInstance(a, b) not isinstance(a, b) assertRaises(exc, fun, args, *kwds) fun(args, *kwds) raises exc assertRaisesRegex(exc, r, fun, args, *kwds) fun(args, *kwds) raises exc and the message matches regex r assertWarns(warn, fun, args, *kwds) fun(args, *kwds) raises warn assertWarnsRegex(warn, r, fun, args, *kwds) fun(args, *kwds) raises warn and the message matches regex r assertLogs(logger, level) The with block logs on logger with minimum level assertNoLogs(logger, level) The with block does not log on logger with minimum level"},{"location":"python/testing/3_unittest/#custom-error-message","title":"Custom error message","text":"<p>All the assert methods accept a msg argument that, if specified, is used as the error message on failure.</p> <p>Note that the msg keyword argument can be passed to assertRaises(), assertRaisesRegex(), assertWarns(), assertWarnsRegex() only when they are used as a context manager.</p> <pre><code>import unittest\n\nclass TestStringMethods(unittest.TestCase):\n\n    def test_upper(self):  \n        self.assertEqual('foo'.upper(), 'FOO', \"Test string uppercase equal\")\n\n    def test_isupper(self):\n        self.assertTrue('FOO'.isupper(), \"Test string uppercase True\")\n        self.assertFalse('Foo'.isupper(), \"Test string uppercase False\")\n\n    def test_split(self):\n        s = 'hello world'\n        self.assertEqual(s.split(), ['hello', 'world'])\n\n        with self.assertRaises(TypeError, \"check that s.split fails when the separator is not a string\"):\n            s.split(2)\n\nif __name__ == '__main__':\n    unittest.main()\n</code></pre>"},{"location":"python/testing/3_unittest/#anatomy-of-a-django-test","title":"Anatomy of a Django test","text":"<pre><code>from django.test import TestCase  # is a subclass of unittest.TestCase that \n                                  # runs each test inside a transaction to provide isolation\nfrom myapp.models import Animal\n\nclass AnimalTestCase(TestCase):\n    # Django provides an additional way of defining fixtures\n    # these can be generated by running:\n    #    python manage.py dumpdata animal_app.Status -o animal_app/fixtures/initial_data/status.json\n    fixtures = [\"animal_app/fixtures/initial_data/status.json\",]\n\n    def setUp(self):\n        # the unittest way of defining fixtures\n        Animal.objects.create(name=\"lion\", sound=\"roar\")\n        Animal.objects.create(name=\"cat\", sound=\"meow\")\n\n    # The individual tests are defined with methods whose names start with the \n    # letters test. This naming convention informs the test runner about which methods \n    # represent tests.\n    def test_animals_can_speak(self):\n        \"\"\"Animals that can speak are correctly identified\"\"\"\n        lion = Animal.objects.get(name=\"lion\")\n        cat = Animal.objects.get(name=\"cat\")\n        self.assertEqual(lion.speak(), 'The lion says \"roar\"')\n        self.assertEqual(cat.speak(), 'The cat says \"meow\"')\n</code></pre> <p>The default behavior of the test utility is to find all the test cases (that is, subclasses of unittest.TestCase) in any file whose name begins with test, automatically build a test suite out of those test cases, and run that suite.</p>"},{"location":"python/testing/3_unittest/#django-models-in-tests","title":"Django models in tests","text":"<p>If your tests rely on database access such as creating or querying models, be sure to create your test classes as subclasses of django.test.TestCase rather than unittest.TestCase. subclasses of django.test.TestCase rather than unittest.TestCase.</p>"},{"location":"python/testing/3_unittest/#test-execution-order","title":"Test execution order","text":"<p>Using unittest.TestCase avoids the cost of running each test in a transaction and flushing the database, but if your tests interact with the database their behavior will vary based on the order that the test runner executes them. This can lead to unit tests that pass when run in isolation but fail when run in a suite.</p>"},{"location":"python/testing/3_unittest/#command-line_1","title":"Command Line","text":"<p>Similarly to what happens with unittest, Django allows the same functionality where a module can be used from the  command line to run tests from modules, classes or even individual test methods.</p> <p>The example below show how tests can be executed from the most general, which runs all the tests, to the most  particular, where only one individual test is executed.</p> <pre><code>python manage.py test\npython manage.py test animal_app.tests.AnimalTestCase\npython manage.py test animal_app.tests.AnimalTestCase.test_animals_can_speak\n</code></pre>"},{"location":"python/testing/3_unittest/#preserve-database-between-test-execution","title":"Preserve database between test execution","text":"<p>The test --keepdb option preserves the test database between test runs. It skips the create and destroy actions  which can greatly decrease the time to run tests.</p> <pre><code>python manage.py test --keepdb\n</code></pre>"},{"location":"python/testing/3_unittest/#automatically-recover-from-a-test-run-that-was-forcefully-interrupted","title":"Automatically recover from a test run that was forcefully interrupted","text":"<p>If a test run is forcefully interrupted, the test database may not be destroyed. On the next run, you\u2019ll be asked  whether you want to reuse or destroy the database. Use the test --noinput option to suppress that prompt and  automatically destroy the database. This can be useful when running tests on a continuous integration server where  tests may be interrupted by a timeout, for example.</p> <pre><code>python manage.py test --noinput\n</code></pre>"},{"location":"python/testing/4_pytest/","title":"Pytest","text":"<p>this document was inspired/copied from of https://docs.python.org/3/library/unittest.html, https://docs.pytest.org and https://pytest-django.readthedocs.io/en/latest/</p> <p>Pytest is a Python testing framework that originated from the PyPy project. It can be used to write various types of software tests, including unit tests, integration tests, end-to-end tests, and functional tests. Its features include parametrized testing, fixtures, and assert re-writing.</p> <p>Python doesn't include Pytest out of the box, you must install it. Please check https://docs.pytest.org</p>"},{"location":"python/testing/4_pytest/#anatomy-of-a-test","title":"Anatomy of a test","text":"<p>Pytest divides a test into four steps:</p> <ul> <li>Arrange is where we prepare everything for our test. This means pretty much everything except for the \u201cact\u201d. This can mean preparing objects, starting/killing services, entering records into a database, or even things like defining a URL to query, generating some credentials for a user that doesn't exist yet, or just waiting for some process to finish.</li> <li>Act is the singular, state-changing action that kicks off the behavior we want to test. This typically takes the form of a function/method call.</li> <li>Assert is where we take that measurement/observation on our test and apply our judgement to it.</li> <li>Cleanup is where the test picks up after itself, so other tests aren\u2019t being accidentally influenced by it.</li> </ul> <pre><code>import pytest\n\nclass TestStringMethods:  # A testcase\n\n    # The individual tests are defined with methods whose names start with the \n    # letters test. This naming convention informs the test runner about which methods \n    # represent tests.\n    def test_upper(self):  \n        assert 'foo'.upper() == 'FOO'\n\n    def test_isupper(self):\n        assert 'FOO'.isupper() is True\n        assert 'Foo'.isupper() is False\n\n    def test_split(self):\n        s = 'hello world'\n        assert s.split() == ['hello', 'world']\n        # check that s.split fails when the separator is not a string\n        with pytest.raises(TypeError):\n            s.split(2)\n</code></pre>"},{"location":"python/testing/4_pytest/#fixture","title":"Fixture","text":"<p>Fixtures are everything that needs to happen/exist in order to run a test.They're part of the arrange steps.</p> <pre><code>import pytest\n\n\nclass Fruit:\n    def __init__(self, name):\n        self.name = name\n\n    def __eq__(self, other):\n        return self.name == other.name\n\n\n@pytest.fixture\ndef my_fruit():\n    return Fruit(\"apple\")\n\n\n@pytest.fixture\ndef fruit_basket(my_fruit):\n    return [Fruit(\"banana\"), my_fruit]\n\n\ndef test_my_fruit_in_basket(my_fruit, fruit_basket):\n    assert my_fruit in fruit_basket\n</code></pre>"},{"location":"python/testing/4_pytest/#fixture-scopes","title":"Fixture scopes","text":"<p>Fixtures are created when first requested by a test, and are destroyed based on their scope:</p> <ul> <li>function: the default scope, the fixture is destroyed at the end of the test.</li> <li>class: the fixture is destroyed during teardown of the last test in the class.</li> <li>module: the fixture is destroyed during teardown of the last test in the module.</li> <li>package: the fixture is destroyed during teardown of the last test in the package.</li> <li>session: the fixture is destroyed at the end of the test session.</li> </ul> <pre><code>@pytest.fixture(scope=\"session\")\ndef smtp_connection():\n    # the returned fixture value will be shared for\n    # all tests requesting it\n    ...\n</code></pre>"},{"location":"python/testing/4_pytest/#command-line","title":"Command Line","text":"<p>The pytest module can be used from the command line to run tests from modules, classes or even individual test methods:</p> <pre><code>pytest test_module1 test_module2\npytest test_module.TestClass\npytest test_module.TestClass.test_method\n</code></pre> <p>For a list of all the command-line options:</p> <pre><code>pytest -h\n</code></pre>"},{"location":"python/testing/4_pytest/#assertions","title":"Assertions","text":"<p>Pytest allows you to use the standard Python assert for verifying expectations and values in Python tests.</p>"},{"location":"python/testing/4_pytest/#custom-error-message","title":"Custom error message","text":"<p>Assert supports a message, which should be used to make assert statements more clear.</p> <pre><code>import pytest\n\nclass TestStringMethods:\n    def test_upper(self):  \n        assert 'foo'.upper() == 'FOO', \"Test string uppercase equal\"\n\n    def test_isupper(self):\n        assert 'FOO'.isupper() is True, \"Test string uppercase True\"\n        assert 'Foo'.isupper() is False, \"Test string uppercase False\"\n\n    def test_split(self):\n        s = 'hello world'\n        assert s.split() == ['hello', 'world'], \"Test split string\"\n\n        with pytest.raises(TypeError, match=\"must be str or None, not int\"):\n            s.split(2)\n</code></pre>"},{"location":"python/testing/4_pytest/#parametrize","title":"Parametrize","text":"<p>The builtin pytest.mark.parametrize decorator enables parametrization of arguments for a test function.</p> <pre><code>import pytest\n\n@pytest.mark.parametrize(\"test_input,expected\", [(\"3+5\", 8), (\"2+4\", 6), (\"6*9\", 42)])\ndef test_eval(test_input, expected):\n    assert eval(test_input) == expected\n</code></pre> <p>which can also be declared as</p> <pre><code>@pytest.mark.parametrize(\"test_input\", [\"3+5\", \"2+4\", \"6*9\"])\n@pytest.mark.parametrize(\"expected\", [8, 6, 42])\ndef test_eval(test_input, expected):\n    assert eval(test_input) == expected\n</code></pre>"},{"location":"python/testing/4_pytest/#anatomy-of-a-django-test","title":"Anatomy of a Django test","text":"<p>To use Pytest with Django you must install https://pytest-django.readthedocs.io</p> <pre><code>import pytest\n\nfrom myapp.models import Animal\n\n\n@pytest.fixture(scope=\"function\")\ndef cat():\n    Animal.objects.create(name=\"cat\", sound=\"meow\")\n\n\n@pytest.fixture(scope=\"function\")\ndef lion():\n    Animal.objects.create(name=\"lion\", sound=\"roar\")\n\n\n@pytest.fixture(scope=\"function\")\ndef felines(lion, cat):\n    ...\n\n@pytest.mark.django_db\ndef test_animals_can_speak(felines):\n    \"\"\"Animals that can speak are correctly identified\"\"\"\n    lion = Animal.objects.get(name=\"lion\")\n    cat = Animal.objects.get(name=\"cat\")\n\n    assert lion.speak() == 'The lion says \"roar\"'\n    assert cat.speak() == 'The cat says \"meow\"'\n</code></pre>"},{"location":"python/testing/4_pytest/#why-would-i-use-this-instead-of-djangos-managepy-test-command","title":"Why would I use this instead of Django\u2019s manage.py test command?","text":"<p>Running the test suite with pytest offers some features that are not present in Django\u2019s standard test mechanism:</p> <ul> <li>Less boilerplate: no need to import unittest, create a subclass with methods. Just write tests as regular functions.</li> <li>Manage test dependencies with fixtures.</li> <li>Run tests in multiple processes for increased speed.</li> <li>There are a lot of other nice plugins available for pytest.</li> <li>Easy switching: Existing unittest-style tests will still work without any modifications.</li> </ul>"},{"location":"python/testing/5_mock/","title":"unittest.Mock","text":"<p>this document was inspired/copied from of https://docs.python.org/3/library/unittest.mock.html</p> <p><code>unittest.mock</code> is a library for testing in Python. It allows you to replace parts of your system under test with mock objects and make assertions about how they have been used.</p> <p>Mock an item where it is used, not where it came from.</p> <p>Let's consider the following example, where there's a function that returns yesterday's date.</p> <pre><code># utils.py\nfrom django.utils import timezone\nfrom datetime import timedelta\n\n\ndef yesterday():\n    dt = timezone.now() + timedelta(days=-1)\n    return dt.date()\n</code></pre> <p>Let's compare the equivalent approaches we can have for that code sample.</p>"},{"location":"python/testing/5_mock/#mock","title":"Mock","text":"<p>Mock is a flexible mock object intended to replace the use of stubs and test doubles throughout your code. Mocks are callable and create attributes as new mocks when you access them. Accessing the same attribute will always return the same mock. Mocks record how you use them, allowing you to make assertions about what your code has done to them.</p> <pre><code>from datetime import datetime\n\nfrom django.test import TestCase\n\nfrom ..utils import yesterday, timezone\nfrom unittest.mock import Mock\n\nclass YesterdayTestMock(TestCase):\n\n    def test_success(self):\n        \"\"\"\n        Mock an item where it is used, not where it came from.\n        In this case even though timezone in utils.py is imported\n        from django.utils, we want to mock timezone from utils.\n        \"\"\"\n        tz = timezone\n        tz.now = Mock(return_value=datetime(1945, 2, 12, 0, 0, 0))\n\n        expected = datetime(1945, 2, 11).date()\n\n        self.assertEqual(yesterday(), expected)\n</code></pre>"},{"location":"python/testing/5_mock/#magicmock","title":"MagicMock","text":"<p>MagicMock is a subclass of Mock with all the magic methods pre-created and ready to use. There are also non-callable variants, useful when you are mocking out objects that aren\u2019t callable: NonCallableMock and NonCallableMagicMock.</p> <pre><code>from datetime import datetime\n\nfrom django.test import TestCase\n\nfrom ..utils import yesterday, timezone\nfrom unittest.mock import MagicMock\n\n\nclass YesterdayTestMagicMock(TestCase):\n\n    def test_success(self):\n        \"\"\"\n        Mock an item where it is used, not where it came from.\n        In this case even though timezone in utils.py is imported\n        from django.utils, we want to mock timezone from utils.\n        \"\"\"\n        tz = timezone\n        tz.now = MagicMock(return_value=datetime(1945, 2, 12, 0, 0, 0))\n\n        expected = datetime(1945, 2, 11).date()\n\n        self.assertEqual(yesterday(), expected)\n</code></pre>"},{"location":"python/testing/5_mock/#patch","title":"Patch","text":"<p>The <code>patch()</code> decorators makes it easy to temporarily replace classes in a particular module with a Mock object. By default patch() will create a MagicMock for you. You can specify an alternative class of Mock using the new_callable argument to patch().</p> <pre><code>from datetime import datetime\n\nfrom django.test import TestCase\n\nfrom ..utils import yesterday\nfrom unittest.mock import patch\n\nclass YesterdayTest(TestCase):\n\n    @patch(\"library.utils.timezone.now\")\n    def test_success(self, mock):\n        \"\"\"\n        Mock an item where it is used, not where it came from.\n        In this case even though timezone in utils.py is imported\n        from django.utils, we want to mock timezone from utils.\n        \"\"\"\n        mock.return_value = datetime(1945, 2, 12, 0, 0, 0)\n\n        expected = datetime(1945, 2, 11).date()\n\n        self.assertEqual(yesterday(), expected)\n</code></pre>"},{"location":"python/testing/5_mock/#side-effect","title":"Side effect","text":"<p>This can either be a function to be called when the mock is called, an iterable or an exception (class or instance) to be raised.</p> <p>An example of a mock that raises an exception (to test exception handling of an API):</p> <pre><code>&gt;&gt;&gt;mock = Mock()\n&gt;&gt;&gt;mock.side_effect = Exception('Boom!')\n&gt;&gt;&gt;mock()\nTraceback (most recent call last):\n  ...\nException: Boom!\n</code></pre> <p>Using side_effect to return a sequence of values:</p> <pre><code>&gt;&gt;&gt;mock = Mock()\n&gt;&gt;&gt;mock.side_effect = [3, 2, 1]\n&gt;&gt;&gt;mock(), mock(), mock()\n(3, 2, 1)\n</code></pre> <p>Setting side_effect to None clears it:</p> <pre><code>&gt;&gt;&gt;mock = Mock(side_effect=KeyError, return_value=3)\n&gt;&gt;&gt;mock()\nTraceback (most recent call last):\n ...\nKeyError\n&gt;&gt;&gt;mock.side_effect = None\n&gt;&gt;&gt;mock()\n3\n</code></pre>"},{"location":"python/testing/5_mock/#asserts","title":"Asserts","text":""},{"location":"python/testing/5_mock/#assert_called","title":"assert_called()","text":"<p>Assert that the mock was called at least once.</p> <pre><code>&gt;&gt;&gt;mock = Mock()\n&gt;&gt;&gt;mock.method()\n&lt;Mock name='mock.method()' id='...'&gt;\n&gt;&gt;&gt;mock.method.assert_called()\n</code></pre>"},{"location":"python/testing/5_mock/#assert_called_once","title":"assert_called_once()","text":"<p>Assert that the mock was called exactly once.</p> <pre><code>&gt;&gt;&gt;mock = Mock()\n&gt;&gt;&gt;mock.method()\n&lt;Mock name='mock.method()' id='...'&gt;\n&gt;&gt;&gt;mock.method.assert_called_once()\n&gt;&gt;&gt;mock.method()\n&lt;Mock name='mock.method()' id='...'&gt;\n&gt;&gt;&gt;mock.method.assert_called_once()\nTraceback (most recent call last):\n...\nAssertionError: Expected 'method' to have been called once. Called 2 times.\n</code></pre>"},{"location":"python/testing/5_mock/#assert_called_withargs-kwargs","title":"assert_called_with(args, *kwargs)","text":"<p>This method is a convenient way of asserting that the last call has been made in a particular way:</p> <pre><code>&gt;&gt;&gt;mock = Mock()\n&gt;&gt;&gt;mock.method(1, 2, 3, test='wow')\n&lt;Mock name='mock.method()' id='...'&gt;\n&gt;&gt;&gt;mock.method.assert_called_with(1, 2, 3, test='wow')\n</code></pre>"},{"location":"python/testing/5_mock/#assert_called_once_withargs-kwargs","title":"assert_called_once_with(args, *kwargs)","text":"<p>Assert that the mock was called exactly once and that call was with the specified arguments.</p> <pre><code>&gt;&gt;&gt;mock = Mock(return_value=None)\n&gt;&gt;&gt;mock('foo', bar='baz')\n&gt;&gt;&gt;mock.assert_called_once_with('foo', bar='baz')\n&gt;&gt;&gt;mock('other', bar='values')\n&gt;&gt;&gt;mock.assert_called_once_with('other', bar='values')\nTraceback (most recent call last):\n    ...\nAssertionError: Expected 'mock' to be called once. Called 2 times.\n</code></pre>"},{"location":"python/testing/5_mock/#assert_any_callargs-kwargs","title":"assert_any_call(args, *kwargs)","text":"<p>Assert the mock has been called with the specified arguments.</p> <p>The assert passes if the mock has ever been called, unlike <code>assert_called_with()</code> and <code>assert_called_once_with()</code> that only pass if the call is the most recent one, and in the case of <code>assert_called_once_with()</code> it must also be the only call.</p> <pre><code>&gt;&gt;&gt;mock = Mock(return_value=None)\n&gt;&gt;&gt;mock(1, 2, arg='thing')\n&gt;&gt;&gt;mock('some', 'thing', 'else')\n&gt;&gt;&gt;mock.assert_any_call(1, 2, arg='thing')\n</code></pre>"},{"location":"python/testing/5_mock/#assert_has_callscalls-any_orderfalse","title":"assert_has_calls(calls, any_order=False)","text":"<p>Assert the mock has been called with the specified calls. The mock_calls list is checked for the calls.</p> <p>If any_order is false then the calls must be sequential. There can be extra calls before or after the specified calls.</p> <p>If any_order is true then the calls can be in any order, but they must all appear in mock_calls.</p> <pre><code>&gt;&gt;&gt;mock = Mock(return_value=None)\n&gt;&gt;&gt;mock(1)\n&gt;&gt;&gt;mock(2)\n&gt;&gt;&gt;mock(3)\n&gt;&gt;&gt;mock(4)\n&gt;&gt;&gt;calls = [call(2), call(3)]\n&gt;&gt;&gt;mock.assert_has_calls(calls)\n&gt;&gt;&gt;calls = [call(4), call(2), call(3)]\n&gt;&gt;&gt;mock.assert_has_calls(calls, any_order=True)\n</code></pre>"},{"location":"python/testing/5_mock/#assert_not_called","title":"assert_not_called()","text":"<p>Assert the mock was never called.</p> <pre><code>&gt;&gt;&gt;m = Mock()\n&gt;&gt;&gt;m.hello.assert_not_called()\n&gt;&gt;&gt;obj = m.hello()\n&gt;&gt;&gt;m.hello.assert_not_called()\nTraceback (most recent call last):\n    ...\nAssertionError: Expected 'hello' to not have been called. Called 1 times.\n</code></pre>"},{"location":"python/testing/5_mock/#reset_mock-return_valuefalse-side_effectfalse","title":"reset_mock(*, return_value=False, side_effect=False)","text":"<p>The reset_mock method resets all the call attributes on a mock object:</p> <pre><code>&gt;&gt;&gt;mock = Mock(return_value=None)\n&gt;&gt;&gt;mock('hello')\n&gt;&gt;&gt;mock.called\nTrue\n&gt;&gt;&gt;mock.reset_mock()\n&gt;&gt;&gt;mock.called\nFalse\n</code></pre> <p>This can be useful where you want to make a series of assertions that reuse the same object. Note that <code>reset_mock()</code> doesn\u2019t clear the return value, side_effect or any child attributes you have set using normal assignment by default. In case you want to reset return_value or side_effect, then pass the corresponding parameter as True. Child mocks and the return value mock (if any) are reset as well.</p>"},{"location":"python/testing/6_factory_boy/","title":"Factory boy","text":"<p>https://factoryboy.readthedocs.io/en/stable/index.html</p> <p>As a fixtures replacement tool, it aims to replace static, hard to maintain fixtures with easy-to-use factories for complex objects.</p> <p>This is a simple how to guide for how to use this library:</p>"},{"location":"python/testing/6_factory_boy/#django-models","title":"Django models","text":"<p>Let's consider we have the following django models:</p> <pre><code>class MusicTrack(models.Model):\n    name = models.TextField()\n    duration = models.IntegerField() # in seconds\n    band = models.ForeignKey(Band, on_delete=models.DO_NOTHING)\n    release_date = models.DateTimeField(null=True)\n\n    class Meta:\n        unique_together = [\"name\", \"band\"]\n\nclass Band(models.Model):\n    name = models.TextField()\n    custom_id = models.TextField(unique=True)\n</code></pre>"},{"location":"python/testing/6_factory_boy/#faker","title":"Faker","text":"<ul> <li>factory boy with faker https://factoryboy.readthedocs.io/en/stable/reference.html#faker</li> <li>faker https://faker.readthedocs.io/en/latest/</li> </ul> <p>Factory boy has many ways to generate data, usually we prefer to use Faker, factory boy provides a wrapper for faker</p>"},{"location":"python/testing/6_factory_boy/#simple-faker-attribute","title":"Simple faker attribute","text":"<pre><code>class MusicTrackFactory(factory.django.DjangoModelFactory):\n    name = factory.Faker('name')\n\n    class Meta:\n        model = MusicTrack\n</code></pre> <p>Basically this is calling the <code>name</code> function in Faker library https://faker.readthedocs.io/en/latest/providers/faker.providers.person.html#faker.providers.person.Provider.name</p>"},{"location":"python/testing/6_factory_boy/#faker-attribute-with-params","title":"Faker attribute with params","text":"<p>Let's say we want our music durations to be between 30 and 500 seconds we can use <code>random_int</code> like in https://faker.readthedocs.io/en/latest/providers/baseprovider.html#faker.providers.BaseProvider.random_int</p> <p> Common Mistake:</p> <pre><code>class MusicTrackFactory(factory.django.DjangoModelFactory):\n    name = factory.Faker('name')\n    duration = fake.random_int(min=30, max=500)\n</code></pre> <p>the issue with the implementation above is that it will generate a random number only once.</p> <p>For example, when you would run a <code>MusicTrackFactory.create()</code> the duration would be some random number like 154, and the next time you run <code>MusicTrackFactory.create()</code> the duration would all be 154, instead of a new random number</p> <p> Correct Implementation:</p> <pre><code>class MusicTrackFactory(factory.django.DjangoModelFactory):\n    name = factory.Faker('name')\n    duration = factory.Faker('random_int',min=30, max=500)\n</code></pre>"},{"location":"python/testing/6_factory_boy/#other-providers","title":"Other providers","text":"<p>Here is a list of other fakers classes https://faker.readthedocs.io/en/latest/providers.html</p>"},{"location":"python/testing/6_factory_boy/#relations-and-foreign-keys","title":"Relations and Foreign Keys","text":"<p>https://factoryboy.readthedocs.io/en/stable/recipes.html#dependent-objects-foreignkey</p> <p>You can use SubFactory to create other dependent models like:</p> <pre><code>class BandFactory(factory.django.DjangoModelFactory):\n    name = factory.Faker('name')\n\nclass MusicTrackFactory(factory.django.DjangoModelFactory):\n    name = factory.Faker('name')\n    Band = factory.SubFactory(BandFactory)\n</code></pre>"},{"location":"python/testing/6_factory_boy/#unique-constraints","title":"Unique constraints","text":"<p>https://factoryboy.readthedocs.io/en/stable/orms.html#factory.django.DjangoOptions.django_get_or_create</p> <p>for django we use django_get_or_create, for unique fields so we don't have exception when running tests</p> <pre><code>class BandFactory(factory.django.DjangoModelFactory):\n    name = factory.Faker('name')\n    custom_id = factory.Faker(\"lexify\", text=\"???-###-????????-????\")\n\n    class Meta:\n        model = Band\n        django_get_or_create = (\"custom_id\",)\n\nclass MusicTrackFactory(factory.django.DjangoModelFactory):\n    name = factory.Faker('name')\n    band = factory.SubFactory(BandFactory)\n\n    class Meta:\n        model = MusicTrack\n        django_get_or_create = (\"name\",\"band\",)\n</code></pre>"},{"location":"python/testing/6_factory_boy/#final-factories","title":"Final Factories","text":"<pre><code>class BandFactory(factory.django.DjangoModelFactory):\n    name = factory.Faker('name')\n    custom_id = factory.Faker(\"lexify\", text=\"???-###-????????-????\")\n\n    class Meta:\n        model = Band\n        django_get_or_create = (\"custom_id\",)\n\nclass MusicTrackFactory(factory.django.DjangoModelFactory):\n    name = factory.Faker('name')\n    band = factory.SubFactory(BandFactory)\n    release_date = factory.Faker('date_this_century',before_today=True)\n    duration = factory.Faker('random_int',min=30, max=500)\n    class Meta:\n        model = MusicTrack\n        django_get_or_create = (\"name\",\"band\",)\n</code></pre>"},{"location":"python/testing/6_factory_boy/#if-conditions-within-factory-factorymaybe-method","title":"If conditions within Factory (factory.Maybe Method)","text":"<p>Let's consider the following Factory example:</p> <pre><code>class BandFactory(factory.django.DjangoModelFactory):\n    name = factory.Faker(\"name\")\n    custom_id = factory.Faker(\"lexify\", text=\"???-###-????????-????\")\n    pseudonym = factory.Faker(\"boolean\", chance_of_getting_true=0)\n    band = factory.Maybe(\n        \"pseudonym\",\n        yes_declaration=factory.SubFactory(\"&lt;project_name&gt;.factories.BandFactory\", pseudonym=False),\n        no_declaration=None,\n    )\n</code></pre> <p>In this example the attribute <code>band</code> will only be not null if the <code>pseudonym</code> attribute (which is a boolean field) has a value of true. To implement this we can use the <code>factory.Maybe</code> Method.</p>"},{"location":"python/testing/7_github_actions/","title":"Github Actions","text":"<p>Documentation</p> <p>Github Actions is a feature that allows you to automate and execute different processes related to the development of your repository.</p> <p>For example, you can always run all tests locally before you commit any changes to a repository or you can automate this process by using Github Actions.</p> <p>Read this page carefully as it explains the basics of how Github Actions works.</p>"},{"location":"python/testing/7_github_actions/#example-for-a-django-project-with-poetry","title":"Example for a Django project with Poetry","text":"<p>The following code covers how to setup a workflow in Github Actions that will run your tests. It makes some assumptions, like the python version that it uses, poetry for package dependency and postgres for the database. However, you can infer what may be necessary according to your environment setup, as the workflow is neatly divided into steps.</p>"},{"location":"python/testing/7_github_actions/#setup","title":"Setup","text":"<p>The basic setup is creating a .github folder in the root of your repository, then a workflows folder inside the .github folder, and then a django.yml file inside the workflows folder. Folder structure summary:</p> <ul> <li>.github</li> <li>workflows<ul> <li>django.yml</li> </ul> </li> </ul>"},{"location":"python/testing/7_github_actions/#the-workflow","title":"The Workflow","text":"<p>The workflow is defined in the django.yml file.</p> <pre><code># The visible name that will appear in the github interface.\nname: Django Tests\n\n# You can specify branches and other actions, but the on: push directive is one of the most basics. Everytime there's a push on any branch, this workflow will be triggered and will execute its jobs.\non: push\n\n# You can define environment variables on a global level for the workflow.\n# Here we define variables for the postgres database.\n# More information on how environment variables work here: https://docs.github.com/en/actions/learn-github-actions/variables#defining-environment-variables-for-a-single-workflow\nenv:\n  POSTGRES_DB: postgres\n  POSTGRES_USER: postgres\n  POSTGRES_PASSWORD: postgres\n  POSTGRES_HOST: 127.0.0.1\n  POSTGRES_PORT: 15433\n\njobs:\n  # Defines the name of the first and only job in this workflow.\n  django_tests:\n    # This job will run on a Ubuntu Linux runner with the Ubuntu version 22.04.\n    # Because this job requires a postgres database and it uses a docker container for that purpose. The Runner must be a Linux-based OS.\n    runs-on: ubuntu-22.04\n\n    # Services are Docker containers. For this job, we are using a postgres container for the database.\n    # See more information regarding the services here: https://docs.github.com/en/actions/using-containerized-services/about-service-containers\n    services:\n      # A name for the service.\n      postgres:\n        # Use postgres version 15.\n        image: \"postgres:15\"\n        # Define the variables for the postgres container.\n        env:\n          POSTGRES_PASSWORD: ${{env.POSTGRES_PASSWORD}}\n          POSTGRES_USER: ${{env.POSTGRES_USER}}\n          POSTGRES_DB: ${{env.POSTGRES_DB}}\n        # Note that we didn't use ${{env.POSTGRES_PORT}} in the ports section. That's because you don't have access to the env object in the ports section.\n        # Read more about this here: https://docs.github.com/en/actions/learn-github-actions/contexts#context-availability\n        # TODO: If anyone knows if there's a way to use the env value in the ports, feel free to make a PR.\n        ports:\n          # Here we are saying that the port 5432 is exposed outwards through port 15433.\n          - 15433:5432\n        # These are just some optional yet recommended health checks.\n        options: &gt;-\n          --health-cmd pg_isready\n          --health-interval 10s\n          --health-timeout 5s\n          --health-retries 5\n\n    # steps is important as it divides a job into smaller parts.\n    # In this case, the job is to run django tests, and we can divide that job into smaller steps.\n    steps:\n      # This is the first step. The \"uses\" keyword specifies that this step will run v3 of the actions/checkout action. This is an action that checks out your repository onto the runner, allowing you to run scripts or other actions against your code (such as build and test tools).\n      # You can read more about this action here: https://github.com/actions/checkout#readme\n      - uses: actions/checkout@v3\n\n      # You can also name your steps so that everything is clearer.\n      - name: Set up Python 3.11\n        # This setup-python action will do exactly what it suggests, install python in your runner's OS. Note that v4 is not the version of python, it's the version of the action.\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.11\"\n\n      # I chose to do a custom installation of poetry, but there are actions that will handle this for you. For example: https://github.com/snok/install-poetry\n      - name: Install and configure Poetry\n        run: |\n          INSTALL_PATH=\"$HOME/.local\"\n          INSTALLATION_SCRIPT=\"$(mktemp)\"\n          VIRTUALENVS_PATH=\"{cache-dir}/virtualenvs/#\\~/$HOME\"\n\n          curl -sSL https://install.python-poetry.org/ --output \"$INSTALLATION_SCRIPT\"\n\n          POETRY_HOME=$INSTALL_PATH python3 \"$INSTALLATION_SCRIPT\" --yes --version=\"1.3.2\"\n\n          export PATH=\"/root/.local/bin:$PATH\"\n\n          poetry config virtualenvs.create true\n          poetry config virtualenvs.in-project true\n          poetry config virtualenvs.path \"$VIRTUALENVS_PATH\"\n\n          echo \"VENV=.venv/bin/activate\" &gt;&gt; \"$GITHUB_ENV\"\n\n      # The last step installed and configured poetry, now we can install our dependencies.\n      - name: Install Dependencies\n        run: |\n          export PATH=\"/root/.local/bin:$PATH\"\n          poetry install --no-interaction\n\n      # This is all very self-explanatory, everything is configured and ready for execution, simply invoke the tests.\n      - name: Run Tests\n        run: |\n          source $VENV\n          cd project_name &amp;&amp; python manage.py test\n</code></pre>"},{"location":"python/testing/7_github_actions/#results","title":"Results","text":"<p>Once you've commit-pushed your workflow, you can access the actions page of your repository. For example, this knowledge base, also has a continuous integration workflow defined, you can the actions page here.</p> <p>You can inspect each run individually and see if it's running correctly or not, or if the tests are accusing something wrong.</p> <p>If there is something wrong, like an ill-defined configuration or an error in a step, you have to fix and add-commit-push, which is laborious but a necessary evil. To work around this, you can use something like act. However, these tools aren't perfect and for example, act doesn't work with the services containers, so this workflow would never work in act; but it can be used for smaller and simple workflows because you can test locally without having to constantly commit-push every change you want to test.</p>"},{"location":"python/testing/8_coverage/","title":"Coverage","text":"<p>https://coverage.readthedocs.io/</p> <p>Coverage.py is a tool for measuring code coverage of Python programs. It monitors your program, noting which parts of the code have been executed, then analyzes the source to identify code that could have been executed but was not. Coverage.py won't fix anything in your code but will show you parts of your code that aren't being tested.</p> <p>This guide will show you how to run coverage.py and generate a final report in html.</p>"},{"location":"python/testing/8_coverage/#running-coverage","title":"Running coverage","text":"<p>After installing, running coverage on your test suite is as simple as calling the following command:</p> <pre><code>coverage run manage.py test\n</code></pre> <p>This will seemingly run your tests and do nothing else, but it will create a .coverage file in the current working directory. You can open that file but it will be complicated to read anything, run the command <code>coverage report</code> to see the results.</p>"},{"location":"python/testing/8_coverage/#better-output","title":"Better output","text":"<p>You can just do a <code>coverage run</code> and then a <code>coverage report</code> or you can have something better like storing the results in an html file and then open it in the browser. To do this, instead of running <code>coverage report</code>, you can run:</p> <pre><code>coverage html --skip-empty\n</code></pre> <p>This will generate a folder called htmlcov. Inside that folder, you will see a lot of files, mainly the index.html file; if you open this file, you will see a much better result.</p> <p>The flag <code>--skip-empty</code> will skip empty files. For example, empty \"init.py\" files won't be included in the html report because you don't need that kind of noise in your report.</p>"},{"location":"python/testing/8_coverage/#ignoring-folders-files-or-code","title":"Ignoring folders, files or code","text":"<p>To ignore code that you don't want to be tested, you can create a file called \".coveragerc\" in the same directory you will be running coverage. Here's an example:</p> <pre><code>[run]\nomit =\n    */tests/*\n    folder1/*\n    folder2/*\n    folder3/*\n    path/to/file/the_file.py\n    if self.debug:\n    if settings.DEBUG\n</code></pre> <p>This example will do the following:</p> <ul> <li>\"*/tests/*\": Ignore any directory called \"tests\" in the project structure and its contents;</li> <li>\"folder1/*\", \"folder2/*\", \"folder3/*\": These 3 folders and their contents will be ignored;</li> <li>\"path/to/file/the_file.py\": Ignore any directory called \"tests\" in the project structure and its contents;</li> <li><code>if self.debug:</code> and <code>if settings.DEBUG</code>: The line itself and everything inside its scope will be ignored;</li> </ul>"},{"location":"python/testing/8_coverage/#complete-example-for-django-project-using-make","title":"Complete example for Django project using Make","text":"<p>With coverage installed and the .coveragerc file defined, you can use this make command to run your tests, generate and open an html report:</p> <pre><code>test_with_report:\n    cd tutorial &amp;&amp; coverage run manage.py test &amp;&amp; coverage html --skip-empty --skip-covered\n    open tutorial/htmlcov/index.html\n</code></pre> <p>This will get your basics covered. It will run your tests through coverage, then it will generate an html report where it skips empty files and files where you have 100% coverage, and finally it will open the generated html report in your browser.</p>"},{"location":"python/testing/8_coverage/#final-note","title":"Final note","text":"<p>This doesn't work with pytest. For pytest, you need something called pytest-cov.</p>"},{"location":"python/testing/8_coverage/#documentation","title":"Documentation","text":"<p>You can and should check the documentation here so you can see all that is possible with the coverage.py tool.</p>"},{"location":"python/testing/9_performance/","title":"Performance","text":"<p>If the battery of tests in a project starts taking too long to complete, here's a few tips to improve performance:</p>"},{"location":"python/testing/9_performance/#setuptestdata-and-setupclass","title":"setUpTestData and setUpClass","text":"<p>If you have several test_ methods in your TestCase class and all of them (or most of them) are using data created in the method \"setUp\", you should pass that data creation to the method setUpTestData. Here's some key points about setUpTestData:</p> <ul> <li>It's not a method of unittest, but it's a django creation that was created to help speed up test execution.</li> <li>It's invoked by setUpClass. So, if you also override setUpClass to do something else, setUpTestData will only be called when you invoke the super's setUpClass.</li> <li>Will only execute once for all tests inside a TestCase class, which means you can place data creation operations there to optimize your tests performance. After running all tests, there will be a rollback for all data created in setUpTestData.</li> <li>It's a classmethod, so don't just rename setUp to setUpTestData.</li> </ul> <p>Here's an example, consider this TestCase:</p> <pre><code>class PostTestCase(TestCase):\n    def setUp(self):\n        self.url = reverse(\"post-url\")\n        self.data = {\"field\": \"value\"}\n        self.model = factories.ModelFactory()\n\n    def test_success(self):\n        result = self.client.post(f\"{self.url}\", data=self.data)\n        self.assertEqual(result.status_code, 201)\n        self.assertEqual(models.Model.objects.all().count(), 2)\n\n    def test_failure(self):\n        result = self.client.post(f\"{self.url}\", data={})\n        self.assertEqual(result.status_code, 400)\n        self.assertEqual(models.Model.objects.all().count(), 1)\n</code></pre> <p>The setUp method here is not optimal because the line <code>self.model = factories.ModelFactory()</code> is going to be executed twice, meaning that it will create and destroy an object before running the first test, and then it's going to do it again for the other test (and again for all other tests in the TestCase).</p> <p>Ideally, this line would only be created at the beginning of test execution and destroyed after all tests in the TestCase have ran. We can achieve this with setUpTestData like so:</p> <pre><code>class PostTestCase(TestCase):\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        cls.url = reverse(\"post-url\")\n        cls.data = {\"field\": \"value\"}\n\n    @classmethod\n    def setUpTestData(cls):\n        cls.model = factories.ModelFactory()\n\n    ...\n</code></pre> <ul> <li>Note that we also used setUpClass for setting up the url and the data dictionary. This is also an optimization although a very minor one for this example. setUpClass will only run once for all tests in a TestCase class. However, there will be no rollback of data created there, so setUpClass becomes ideal for defining necessary program variables for all tests to use.</li> <li>Also note that it's necessary to call <code>super().setUpClass()</code>. TestCase's setUpClass is the one responsible for calling the setUpTestData method.</li> <li>This is merely an example to show how time can be optimized in django tests; More tests and data that is more complex will benefit much more from a correct usage of setUpClass and setUpTestData.</li> </ul>"},{"location":"python/testing/9_performance/#simpletestcase-vs-testcase-vs-transactiontestcase","title":"SimpleTestCase vs TestCase vs TransactionTestCase","text":"<p>This is a simple optimization to implement. All you have to do is pay attention to the following: Do the tests in a class use the database?</p> <p>If you aren't using the database, use SimpleTestCase. If you do need the database, then consider the following: Do any of your tests actually need to test any database transaction-related behavior? If the answer is no or you are unsure, the answer is in most cases, use TestCase. If you need to test specific database behavior when you're commiting data, use TransactionTestCase.</p> <p>This is an important distintion to understand. TestCase makes use of database transactions to speed up tests. This means that within a test method, any database modifications are only visible to that test and aren't actually committed to the database. If you use the --keepdb flag, run a test (that uses TestCase), and use an external tool to inspect the test database, you won't find the changes that a test in a TestCase makes.</p> <p>Django's documentation on TestCase and TransactionTestCase is excellent and further explains what's going.</p>"},{"location":"python/testing/9_performance/#flags","title":"Flags","text":"<p>When invoking django's test suite, there's a few flags that you can use locally to not only improve performance, but also make the execution more convenient:</p> <ul> <li>--parallel x: Where x is the number of threads you want to launch for parallel execution of your tests. If you want to use this flag consistently, you should consider it a last resource. First, try to optimize your tests to have better performance (If time allows) and after all has been optimized, you can use this flag more freely.</li> <li>--keepdb: This flag will persist your test database and will skip creating/destroying the database when you invoke the test suite.</li> <li>--failfast: This flag is particularly useful when you want to make sure your tests are running locally and you don't want to wait for all of them to execute. As soon as a test fails, the test suite execution will be aborted, and you can start analyzing why the test failed and fix it. This is becomes more and more useful, the longer your tests take to run.</li> </ul>"},{"location":"rails/","title":"knowledge-base rails","text":""},{"location":"rails/on_boarding/01_base/chapter_I/","title":"Chapter I - Introduction to Ruby","text":"<p><code>(avr. time for this chapter: 1 day)</code></p> <p>Ruby is an interpreted, high-level, general-purpose programming language that supports multiple programming paradigms. Designed with an emphasis on programming productivity and simplicity, Ruby treats everything as an object, including primitive data types. It was developed in the mid-1990s by Yukihiro \"Matz\" Matsumoto in Japan.</p> <p>Ruby is dynamically typed and uses garbage collection and just-in-time compilation. It supports procedural, object-oriented, and functional programming paradigms.</p> <p>In this chapter, the objective is to explore the fundamentals of Ruby, which will serve as the foundation for the next chapters where you will delve into Rails, one of the most popular frameworks built with Ruby.</p>"},{"location":"rails/on_boarding/01_base/chapter_I/#ruby-in-twenty-minutes","title":"Ruby in Twenty Minutes","text":"<p>A concise Ruby tutorial that should take no more than 20 minutes to complete.</p>"},{"location":"rails/on_boarding/01_base/chapter_I/#steps-to-implement","title":"Steps to implement:","text":"<ol> <li>Ensure Ruby is installed on your system</li> <li>Complete the quick start tutorial</li> <li>Experiment with the Interactive Ruby Shell (IRB)</li> </ol> <p>Reference: Ruby Quick Start</p> <p>Installation Guide: Ruby Installation</p>"},{"location":"rails/on_boarding/01_base/chapter_I/#ruby-koans","title":"Ruby Koans","text":"<p>The Koans guide you along the path to enlightenment in order to learn Ruby. The goal is to understand the Ruby language, syntax, structure, and common functions and libraries through test-driven exercises.</p>"},{"location":"rails/on_boarding/01_base/chapter_I/#steps-to-implement_1","title":"Steps to implement:","text":"<ol> <li>Clone or download the Ruby Koans repository</li> <li>Run the koans and fix each failing test</li> <li>Focus on the recommended koans listed below</li> </ol> <p>Reference: Ruby Koans</p>"},{"location":"rails/on_boarding/01_base/chapter_I/#recommended-koans","title":"Recommended Koans:","text":"<ul> <li><code>about_arrays.rb</code></li> <li><code>about_blocks.rb</code></li> <li><code>about_classes.rb</code></li> <li><code>about_control_statements.rb</code></li> <li><code>about_hashes.rb</code></li> <li><code>about_iteration.rb</code></li> <li><code>about_methods.rb</code></li> <li><code>about_strings.rb</code></li> <li><code>about_symbols.rb</code></li> </ul>"},{"location":"rails/on_boarding/01_base/chapter_I/#ruby-tutorial","title":"Ruby Tutorial","text":"<p>This tutorial covers basic to advanced concepts related to Ruby scripting.</p>"},{"location":"rails/on_boarding/01_base/chapter_I/#steps-to-implement_2","title":"Steps to implement:","text":"<ol> <li>Read and practice the following topics</li> <li>Write small code snippets to reinforce each concept</li> <li>Understand how these concepts apply to real-world scenarios</li> </ol> <p>Reference: TutorialsPoint Ruby</p>"},{"location":"rails/on_boarding/01_base/chapter_I/#topics-to-cover","title":"Topics to cover:","text":"<ul> <li>Ruby - Classes and Objects</li> <li>Ruby - Variables</li> <li>Ruby - Operators</li> <li>Ruby - Methods</li> <li>Ruby - Blocks</li> <li>Ruby - Modules</li> <li>Ruby - Exceptions</li> </ul>"},{"location":"rails/on_boarding/01_base/chapter_II/","title":"Chapter II - Introduction to Ruby on Rails","text":"<p><code>(avr. time for this chapter: 2 days)</code></p> <p>Ruby on Rails (commonly referred to as Rails) is a server-side web application framework written in Ruby under the MIT License. Rails follows the Model-View-Controller (MVC) architectural pattern, providing default structures for databases, web services, and web pages.</p> <p>In this chapter, the objective is to understand how Rails works. This is primarily a learning phase\u2014you do not need to write extensive code, only what helps reinforce your understanding. Focus on comprehension rather than implementation.</p> <p>The recommended course has several chapters, but for this onboarding, we focus on the essentials that will prepare you for the practical exercises ahead.</p>"},{"location":"rails/on_boarding/01_base/chapter_II/#udemy-course","title":"Udemy Course","text":"<p>We recommend the following comprehensive course for learning Ruby on Rails.</p>"},{"location":"rails/on_boarding/01_base/chapter_II/#steps-to-implement","title":"Steps to implement:","text":"<ol> <li>Obtain your course credentials from your tutor</li> <li>Watch the recommended chapters listed below</li> <li>Take notes on key concepts</li> <li>Repeat sections if clarification is needed</li> </ol> <p>Reference: The Complete Ruby on Rails Developer Course</p>"},{"location":"rails/on_boarding/01_base/chapter_II/#recommended-chapters","title":"Recommended Chapters:","text":"<p>Chapter 2 - Introduction</p> <ul> <li>Rails basics Introduction (1st video)</li> </ul> <p>Chapter 3 - MVC and Application Structure</p> <ul> <li>Model, View, Controller and Rails App Structure</li> <li>Root route, controller, more MVC and say 'Hello World!'</li> <li>Structure of a Rails application</li> <li>The back-end: CRUD, scaffold and wrap-up</li> </ul> <p>Chapter 4 - Working with Data</p> <ul> <li>Tables, migrations and naming conventions</li> <li>Validations</li> <li>Show articles (route, action and view)</li> <li>Articles index</li> <li>Forms - build a new article creation form</li> <li>Edit and update: update existing articles</li> <li>Delete: delete articles</li> <li>DRY (Don't Repeat Yourself) code - refactoring and partials</li> </ul> <p>Chapter 6 - Associations (Part 1)</p> <ul> <li>One to many association</li> </ul> <p>Chapter 7 - Associations (Part 2)</p> <ul> <li>Many-to-many association - introduction</li> <li>Many-to-many association - back-end implementation</li> </ul>"},{"location":"rails/on_boarding/01_base/chapter_III/","title":"Chapter III - Practical Exercise: Ebook Store (Part 1)","text":"<p><code>(avr. time for this chapter: 3 to 4 days)</code></p> <p>Moving from theory to practice, this chapter presents a hands-on exercise where you will build a Rails application from scratch.</p> <p>The requirements listed below are intentionally generic. This approach mirrors real-world scenarios where client requirements are often open to interpretation. As an engineer, you are expected to analyze, design, and implement solutions based on the given specifications. If you have questions or need clarification, discuss ideas with your tutor\u2014just as you would in a professional environment.</p> <p>These topics align with what you learned in the previous chapter, so the implementation should feel familiar.</p>"},{"location":"rails/on_boarding/01_base/chapter_III/#project-setup","title":"Project Setup","text":""},{"location":"rails/on_boarding/01_base/chapter_III/#steps-to-implement","title":"Steps to implement:","text":"<ol> <li>Create a new Rails application using MVC architecture</li> <li>Initialize a Git repository and grant access to your tutor</li> <li>(Optional) Deploy the application to a platform of your choice (e.g., Render, Heroku)</li> </ol>"},{"location":"rails/on_boarding/01_base/chapter_III/#application-requirements","title":"Application Requirements","text":"<p>Build an online ebook store application. The platform will have sellers and buyers. You will not implement shopping cart logic, but you will implement functionalities related to purchasing actions.</p>"},{"location":"rails/on_boarding/01_base/chapter_III/#core-features","title":"Core Features","text":""},{"location":"rails/on_boarding/01_base/chapter_III/#user-management","title":"User Management","text":"<ul> <li>Create CRUD operations for Users (can be either seller or buyer)</li> <li>Implement user status management (enable/disable)</li> </ul>"},{"location":"rails/on_boarding/01_base/chapter_III/#ebook-management","title":"Ebook Management","text":"<ul> <li>Create CRUD operations for Ebooks</li> <li>Implement status workflow for Ebooks (Draft \u2192 Pending \u2192 Live)</li> <li>Store PDF preview drafts available for download</li> </ul>"},{"location":"rails/on_boarding/01_base/chapter_III/#navigation","title":"Navigation","text":"<ul> <li>Add a navigation menu with links to Ebooks and Users sections</li> </ul>"},{"location":"rails/on_boarding/01_base/chapter_III/#purchase-functionality","title":"Purchase Functionality","text":"<p>Note: For email functionality, you do not need to send actual emails. Create the notification logic only, or use Mailcatcher for local testing.</p>"},{"location":"rails/on_boarding/01_base/chapter_III/#steps-to-implement_1","title":"Steps to implement:","text":"<ol> <li>Implement an ebook purchase action (without cart logic)</li> <li>The purchase button should trigger the following actions:</li> <li>Send an email to the seller with their commission (10% of the book price)</li> <li>Send an email with ebook statistics (examples below)</li> <li>Register the purchase in the database</li> </ol>"},{"location":"rails/on_boarding/01_base/chapter_III/#suggested-statistics-to-track","title":"Suggested Statistics to Track:","text":"<ul> <li>Number of times the ebook was purchased</li> <li>Number of times the preview PDF was viewed</li> <li>Number of times the ebook page was visited</li> <li>Visitor information (IP, browser, location, etc.)</li> </ul> <p>Note: You may define different metrics based on your implementation approach.</p>"},{"location":"rails/on_boarding/01_base/chapter_III/#database-transactions-acid-properties","title":"Database Transactions (ACID Properties)","text":"<p>When processing a purchase, multiple database operations must succeed or fail together. This is where database transactions become essential to maintain data integrity.</p>"},{"location":"rails/on_boarding/01_base/chapter_III/#understanding-acid-properties","title":"Understanding ACID Properties:","text":"<ul> <li>Atomicity - All operations succeed or all fail (no partial updates)</li> <li>Consistency - Database moves from one valid state to another</li> <li>Isolation - Concurrent transactions don't interfere with each other</li> <li>Durability - Committed changes persist even after system failure</li> </ul>"},{"location":"rails/on_boarding/01_base/chapter_III/#steps-to-implement_2","title":"Steps to implement:","text":"<ol> <li>Wrap the purchase flow in a transaction:</li> <li>Create the Purchase record</li> <li>Update the ebook's purchase count</li> <li>Update the seller's earnings/balance</li> <li> <p>All operations must succeed, or none should persist</p> </li> <li> <p>Handle transaction failures:</p> </li> <li>Use <code>ActiveRecord::Base.transaction</code> block</li> <li>Raise exceptions to trigger rollback</li> <li> <p>Implement proper error handling and user feedback</p> </li> <li> <p>Test the transaction behavior:</p> </li> <li>Verify that if one operation fails, all changes are rolled back</li> <li>Simulate failures (e.g., validation errors) and confirm no partial data exists</li> </ol> <p>Reference: Rails Active Record Transactions</p>"},{"location":"rails/on_boarding/01_base/chapter_III/#scenario-to-implement","title":"Scenario to implement:","text":"<p>When a buyer purchases an ebook, the following must happen atomically:</p> <ol> <li>Validate the buyer has sufficient balance (if implementing wallet)</li> <li>Create a <code>Purchase</code> record with buyer, ebook, and amount</li> <li>Increment the ebook's <code>purchase_count</code></li> <li>Credit the seller's balance with the commission (10%)</li> <li>Send notification emails (outside the transaction)</li> </ol> <p>If any step fails (e.g., ebook becomes unavailable, validation error), the entire operation should roll back, and the user should see an appropriate error message.</p>"},{"location":"rails/on_boarding/01_base/chapter_IV/","title":"Chapter IV - Practical Exercise: Ebook Store (Part 2)","text":"<p><code>(avr. time for this chapter: 3 days)</code></p> <p>This chapter concludes the base onboarding with advanced features for your ebook store application. The topics are broader and will require more careful planning before implementation.</p> <p>Continue building upon the application from Chapter III.</p>"},{"location":"rails/on_boarding/01_base/chapter_IV/#project-finalization","title":"Project Finalization","text":""},{"location":"rails/on_boarding/01_base/chapter_IV/#steps-to-implement","title":"Steps to implement:","text":"<ol> <li>Create pull requests for your features</li> <li>Merge all completed code into the main branch</li> <li>(Optional) Deploy the final application to a platform of your choice (e.g., Render, Heroku)</li> </ol>"},{"location":"rails/on_boarding/01_base/chapter_IV/#advanced-features","title":"Advanced Features","text":""},{"location":"rails/on_boarding/01_base/chapter_IV/#image-management","title":"Image Management","text":"<ul> <li>Implement user profile image upload and storage</li> <li>Implement ebook cover image upload and storage</li> </ul>"},{"location":"rails/on_boarding/01_base/chapter_IV/#authentication-system","title":"Authentication System","text":"<p>Implement a custom authentication system without using Devise.</p>"},{"location":"rails/on_boarding/01_base/chapter_IV/#steps-to-implement_1","title":"Steps to implement:","text":"<ol> <li>Create user registration (sign up) functionality</li> <li>Create user login (sign in) functionality</li> <li>Implement <code>sign_in</code> and <code>sign_out</code> helper methods</li> <li>On successful login, redirect users to their previous location (not the root URL)</li> </ol>"},{"location":"rails/on_boarding/01_base/chapter_IV/#password-management","title":"Password Management","text":"<ul> <li>Create a Rake task that forces password updates every 6 months</li> <li>Send a welcome email when a new user registers</li> </ul> <p>Note: For email functionality, create the notification logic only, or use Mailcatcher for local testing.</p>"},{"location":"rails/on_boarding/01_base/chapter_IV/#tags-system","title":"Tags System","text":""},{"location":"rails/on_boarding/01_base/chapter_IV/#steps-to-implement_2","title":"Steps to implement:","text":"<ol> <li>Implement a tagging system for ebooks</li> <li>Add filtering functionality:</li> <li>Filter ebooks by tags</li> <li>Filter ebooks by user (seller)</li> </ol> <p>Note: Users with no ebooks should not appear in the filter list.</p>"},{"location":"rails/on_boarding/01_base/chapter_V/","title":"Chapter V - Background Jobs and Process Management with Sidekiq","text":"<p><code>(avr. time for this chapter: 1 day)</code></p> <p>This chapter introduces background job processing with Sidekiq and essential process management skills. You will learn how to handle long-running tasks asynchronously and manage system processes effectively.</p> <p>Continue building upon the application from Chapter III.</p>"},{"location":"rails/on_boarding/01_base/chapter_V/#introduction-to-background-jobs","title":"Introduction to Background Jobs","text":"<p>In web applications, some operations are too slow to handle during a regular HTTP request. Examples include sending emails, processing files, generating reports, or calling external APIs. Background jobs allow these operations to run asynchronously, improving user experience and application responsiveness.</p>"},{"location":"rails/on_boarding/01_base/chapter_V/#why-use-background-jobs","title":"Why Use Background Jobs?","text":"<ul> <li>Better User Experience - Users don't wait for slow operations</li> <li>Reliability - Jobs can be retried if they fail</li> <li>Scalability - Workers can be scaled independently</li> <li>Scheduling - Jobs can run at specific times</li> </ul>"},{"location":"rails/on_boarding/01_base/chapter_V/#sidekiq-setup","title":"Sidekiq Setup","text":"<p>Sidekiq is a popular background job processor for Ruby. It uses Redis to manage job queues and is known for its efficiency and reliability.</p>"},{"location":"rails/on_boarding/01_base/chapter_V/#steps-to-implement","title":"Steps to implement:","text":"<ol> <li>Add Sidekiq gem to your project</li> <li>Install and start Redis (required by Sidekiq)</li> <li>Configure Active Job to use Sidekiq as the queue adapter</li> <li>Verify the setup by starting Sidekiq and checking for errors</li> </ol> <p>Reference: Sidekiq Documentation</p>"},{"location":"rails/on_boarding/01_base/chapter_V/#creating-background-jobs","title":"Creating Background Jobs","text":""},{"location":"rails/on_boarding/01_base/chapter_V/#steps-to-implement_1","title":"Steps to implement:","text":"<ol> <li>Generate a new job using Rails generator</li> <li>Implement job logic that sends purchase notification emails</li> <li>Call the job from your purchase controller using <code>perform_later</code></li> <li>Experiment with scheduling jobs for future execution</li> </ol> <p>Note: Always pass primitive types (IDs, strings) to jobs instead of objects. Objects may change between when the job is enqueued and when it runs.</p>"},{"location":"rails/on_boarding/01_base/chapter_V/#practical-exercise-async-purchase-notifications","title":"Practical Exercise: Async Purchase Notifications","text":"<p>Refactor your ebook store to use background jobs for email notifications.</p>"},{"location":"rails/on_boarding/01_base/chapter_V/#steps-to-implement_2","title":"Steps to implement:","text":"<ol> <li>Create a <code>PurchaseNotificationJob</code> that handles:</li> <li>Sending commission email to the seller</li> <li>Sending statistics email</li> <li> <p>Any other email notifications from Chapter III</p> </li> <li> <p>Update your purchase flow:</p> </li> <li>Keep the database transaction synchronous</li> <li>Move email sending to background jobs (outside the transaction)</li> <li> <p>Ensure the user receives immediate feedback</p> </li> <li> <p>Create a <code>StatisticsReportJob</code> that:</p> </li> <li>Calculates daily/weekly ebook statistics</li> <li>Can be scheduled to run periodically</li> </ol>"},{"location":"rails/on_boarding/01_base/chapter_V/#running-sidekiq","title":"Running Sidekiq","text":""},{"location":"rails/on_boarding/01_base/chapter_V/#steps-to-implement_3","title":"Steps to implement:","text":"<ol> <li>Start Sidekiq in a terminal</li> <li>Configure and mount the Sidekiq Web UI to monitor jobs</li> <li>Make a purchase and observe the job being processed</li> </ol> <p>Note: In production, Sidekiq should run as a managed process (systemd, Docker, etc.)</p>"},{"location":"rails/on_boarding/01_base/chapter_V/#process-management-in-linuxmacos","title":"Process Management in Linux/macOS","text":"<p>Understanding how to manage processes is essential for any developer. You will frequently need to start, stop, and debug background processes.</p>"},{"location":"rails/on_boarding/01_base/chapter_V/#steps-to-implement_4","title":"Steps to implement:","text":"<ol> <li>Learn to list all running processes using <code>ps aux</code></li> <li>Filter processes by name using <code>grep</code></li> <li>Use <code>top</code> or <code>htop</code> for interactive process monitoring</li> <li>Understand the key columns in process output:</li> <li>PID - Process ID, unique identifier</li> <li>%CPU - CPU usage percentage</li> <li>%MEM - Memory usage percentage</li> <li>STAT - Process state (S=sleeping, R=running, Z=zombie)</li> </ol>"},{"location":"rails/on_boarding/01_base/chapter_V/#killing-processes","title":"Killing Processes","text":""},{"location":"rails/on_boarding/01_base/chapter_V/#steps-to-implement_5","title":"Steps to implement:","text":"<ol> <li> <p>Practice finding process IDs (PIDs) for Sidekiq, Redis, and Ruby processes</p> </li> <li> <p>Learn different ways to terminate processes:</p> </li> <li>Kill a single process by PID</li> <li>Force kill a process that won't terminate</li> <li>Kill all processes matching a name</li> <li> <p>Kill a process using a specific port</p> </li> <li> <p>Understand signal types:</p> </li> <li>SIGTERM (15) - Graceful shutdown, process can clean up</li> <li>SIGKILL (9) - Immediate termination, no cleanup</li> <li>SIGINT (2) - Interrupt (like pressing Ctrl+C)</li> <li>SIGHUP (1) - Hangup, often used to reload configuration</li> </ol> <p>Reference: Linux Signals</p>"},{"location":"rails/on_boarding/01_base/chapter_V/#practical-exercise-process-management","title":"Practical Exercise: Process Management","text":""},{"location":"rails/on_boarding/01_base/chapter_V/#steps-to-implement_6","title":"Steps to implement:","text":"<ol> <li> <p>Start multiple Sidekiq processes with different queue configurations</p> </li> <li> <p>Practice managing these processes:</p> </li> <li>Find all Sidekiq processes and note their PIDs</li> <li>Kill one process gracefully and observe the shutdown behavior</li> <li>Force kill another and compare the behavior</li> <li> <p>Clean up all remaining Sidekiq processes</p> </li> <li> <p>Verify processes are terminated using <code>ps aux | grep sidekiq</code></p> </li> </ol>"},{"location":"rails/on_boarding/01_base/chapter_V/#handling-stuck-ports","title":"Handling Stuck Ports","text":"<p>A common issue when Rails or other servers don't shut down properly.</p>"},{"location":"rails/on_boarding/01_base/chapter_V/#steps-to-implement_7","title":"Steps to implement:","text":"<ol> <li>Learn to identify what process is using a specific port</li> <li>Practice killing processes that are blocking ports</li> <li>(Optional) Create a helper function in your shell configuration to simplify this task</li> </ol>"},{"location":"rails/on_boarding/01_base/chapter_V/#job-queues-and-priorities","title":"Job Queues and Priorities","text":""},{"location":"rails/on_boarding/01_base/chapter_V/#steps-to-implement_8","title":"Steps to implement:","text":"<ol> <li>Configure multiple queues in Sidekiq configuration file (critical, default, low)</li> <li>Assign different jobs to specific queues based on their importance</li> <li>Start Sidekiq with queue priority weights</li> <li>Test that critical jobs are processed before low priority jobs</li> </ol>"},{"location":"rails/on_boarding/01_base/chapter_V/#error-handling-and-retries","title":"Error Handling and Retries","text":"<p>Sidekiq automatically retries failed jobs. Understanding this behavior is crucial.</p>"},{"location":"rails/on_boarding/01_base/chapter_V/#steps-to-implement_9","title":"Steps to implement:","text":"<ol> <li>Configure custom retry behavior for specific jobs</li> <li>Implement error handling for:</li> <li>Temporary failures that should be retried</li> <li>Permanent failures that should be discarded</li> <li>Test job failure scenarios and observe retry behavior</li> </ol>"},{"location":"rails/on_boarding/01_base/chapter_V/#final-exercise-complete-integration","title":"Final Exercise: Complete Integration","text":""},{"location":"rails/on_boarding/01_base/chapter_V/#steps-to-implement_10","title":"Steps to implement:","text":"<ol> <li> <p>Refactor all email sending in your ebook store to use background jobs</p> </li> <li> <p>Implement a scheduled job for daily statistics:</p> </li> <li>Create <code>DailyStatisticsJob</code></li> <li>Calculate total sales, popular ebooks, active sellers</li> <li> <p>Discuss scheduling options with your tutor (e.g., <code>sidekiq-scheduler</code> gem)</p> </li> <li> <p>Practice the full development workflow:</p> </li> <li>Start Redis, Sidekiq, and Rails server</li> <li>Make a purchase and verify emails are sent asynchronously</li> <li>View job status in Sidekiq Web UI</li> <li>Practice killing and restarting Sidekiq</li> <li> <p>Observe how pending jobs are processed after restart</p> </li> <li> <p>Document the startup process:</p> </li> <li>Create a README section or script that lists all required processes</li> <li>Include commands to start and stop each service</li> </ol> <p>Tip: Consider using tools like <code>foreman</code> or <code>overmind</code> to manage multiple processes during development.</p>"},{"location":"rails/on_boarding/01_base/chapter_V/#checklist","title":"Checklist","text":"<p>Before moving to the next chapter, ensure you can:</p> <ul> <li>[ ] Explain why background jobs are important</li> <li>[ ] Set up and configure Sidekiq with Redis</li> <li>[ ] Create and enqueue background jobs</li> <li>[ ] Use <code>ps</code>, <code>grep</code>, <code>kill</code>, and <code>pkill</code> commands confidently</li> <li>[ ] Find and kill processes using specific ports</li> <li>[ ] Understand the difference between SIGTERM and SIGKILL</li> <li>[ ] Configure job queues and priorities</li> <li>[ ] Handle job failures and retries</li> </ul>"},{"location":"rails/on_boarding/02_tests/chapter_I/","title":"Chapter I - Introduction to RSpec","text":"<p><code>(avr. time for this chapter: 1 day)</code></p> <p>Testing is a crucial part of software development. In the Ruby on Rails ecosystem, RSpec is one of the most popular testing frameworks. It provides a behavior-driven development (BDD) approach that makes tests readable and maintainable.</p> <p>In this chapter, the objective is to understand the basics of RSpec, how to set it up in your ebook application, and how to write your first tests.</p>"},{"location":"rails/on_boarding/02_tests/chapter_I/#rspec-setup","title":"RSpec Setup","text":"<p>Before writing tests, you need to set up RSpec in your Rails application.</p>"},{"location":"rails/on_boarding/02_tests/chapter_I/#steps-to-implement","title":"Steps to implement:","text":"<ol> <li>Add the <code>rspec-rails</code> gem to your Gemfile (in the <code>:development</code> and <code>:test</code> groups)</li> <li>Run <code>bundle install</code></li> <li>Initialize RSpec with <code>rails generate rspec:install</code></li> <li>Understand the folder structure created:</li> <li><code>spec/</code> - main folder for all tests</li> <li><code>spec/spec_helper.rb</code> - RSpec configuration</li> <li><code>spec/rails_helper.rb</code> - Rails-specific configuration</li> <li><code>.rspec</code> - command line options</li> </ol> <p>Reference: RSpec Rails Documentation</p>"},{"location":"rails/on_boarding/02_tests/chapter_I/#basic-test-structure","title":"Basic Test Structure","text":"<p>Understand the anatomy of an RSpec test file:</p> <ul> <li><code>describe</code> blocks - group related tests</li> <li><code>context</code> blocks - describe different scenarios</li> <li><code>it</code> blocks - individual test cases</li> <li><code>expect</code> - make assertions</li> </ul>"},{"location":"rails/on_boarding/02_tests/chapter_I/#steps-to-implement_1","title":"Steps to implement:","text":"<ol> <li>Generate the spec file for your Ebook model: <code>rails generate rspec:model Ebook</code></li> <li>Open <code>spec/models/ebook_spec.rb</code> and examine the generated structure</li> <li>Write your first test checking if the model can be instantiated</li> <li>Run tests with <code>bundle exec rspec</code></li> </ol>"},{"location":"rails/on_boarding/02_tests/chapter_I/#rspec-matchers","title":"RSpec Matchers","text":"<p>Matchers are used to define expected outcomes. Practice with your Ebook and User models.</p>"},{"location":"rails/on_boarding/02_tests/chapter_I/#steps-to-implement_2","title":"Steps to implement:","text":"<ol> <li>Practice using equality matchers: <code>eq</code>, <code>eql</code>, <code>equal</code>, <code>be</code></li> <li>Example: <code>expect(ebook.status).to eq('draft')</code></li> <li>Practice using comparison matchers: <code>be &gt;</code>, <code>be &lt;</code>, <code>be_between</code></li> <li>Example: <code>expect(ebook.price).to be &gt; 0</code></li> <li>Practice using truthiness matchers: <code>be_truthy</code>, <code>be_falsy</code>, <code>be_nil</code></li> <li>Example: <code>expect(user.enabled?).to be_truthy</code></li> <li>Practice using collection matchers: <code>include</code>, <code>match_array</code>, <code>contain_exactly</code></li> <li>Example: <code>expect(Ebook.statuses.keys).to include('draft', 'pending', 'live')</code></li> <li>Practice using predicate matchers: <code>be_empty</code>, <code>be_valid</code></li> <li>Example: <code>expect(ebook).to be_valid</code></li> </ol> <p>Reference: RSpec Built-in Matchers</p>"},{"location":"rails/on_boarding/02_tests/chapter_I/#hooks-before-and-after","title":"Hooks: before and after","text":"<p>Hooks allow you to run code before or after tests.</p>"},{"location":"rails/on_boarding/02_tests/chapter_I/#steps-to-implement_3","title":"Steps to implement:","text":"<ol> <li>Use <code>before(:each)</code> to set up an ebook for each test:    <pre><code>before(:each) do\n  @ebook = Ebook.new(title: \"Ruby Guide\", price: 19.99)\nend\n</code></pre></li> <li>Use <code>before(:all)</code> to set up data once for all tests in a group</li> <li>Understand when to use <code>after</code> hooks for cleanup</li> <li>Practice using <code>let</code> and <code>let!</code> for lazy and eager evaluation:    <pre><code>let(:user) { User.create(name: \"John\", email: \"john@example.com\") }\nlet(:ebook) { Ebook.create(title: \"Rails Tutorial\", seller: user) }\n</code></pre></li> </ol>"},{"location":"rails/on_boarding/02_tests/chapter_I/#exercise","title":"Exercise","text":"<p>Apply these concepts to your ebook application:</p> <ol> <li>Set up RSpec in your ebook application (if not already done)</li> <li>Generate spec files for your existing models:</li> <li><code>rails generate rspec:model Ebook</code></li> <li><code>rails generate rspec:model User</code></li> <li>Write basic model specs for your <code>Ebook</code> model:</li> <li>Test that an ebook can be created with valid attributes (title, price, seller)</li> <li>Test that an ebook without a title is invalid</li> <li>Test the status enum values (Draft, Pending, Live)</li> <li>Write basic model specs for your <code>User</code> model:</li> <li>Test that a user can be created with name and email</li> <li>Test the enable/disable status functionality</li> </ol>"},{"location":"rails/on_boarding/02_tests/chapter_II/","title":"Chapter II - Model and Validation Specs","text":"<p><code>(avr. time for this chapter: 1 day)</code></p> <p>Model specs are the foundation of your test suite. They test the business logic, validations, associations, and methods of your models. These are unit tests\u2014they test small, isolated pieces of code.</p> <p>In this chapter, you will learn how to properly test your Ebook, User, and Purchase models, including validations, associations, callbacks, and custom methods.</p>"},{"location":"rails/on_boarding/02_tests/chapter_II/#testing-validations","title":"Testing Validations","text":"<p>Rails validations ensure data integrity. Your tests should verify that these validations work as expected.</p>"},{"location":"rails/on_boarding/02_tests/chapter_II/#steps-to-implement","title":"Steps to implement:","text":"<ol> <li>Create tests for presence validations on your Ebook model:</li> <li>Test that an ebook is invalid without a title</li> <li>Test that an ebook is invalid without a price</li> <li>Test the error messages are correct</li> <li>Create tests for uniqueness validations on your User model:</li> <li>Test that duplicate emails are rejected</li> <li>Create tests for format validations:</li> <li>Test valid and invalid email formats for User</li> <li>Create tests for numericality validations:</li> <li>Test that ebook price must be greater than zero</li> <li>Test that price cannot be negative</li> <li>Create tests for inclusion validations:</li> <li>Test that ebook status must be one of: draft, pending, live</li> </ol> <p>Reference: Rails Testing Guide</p>"},{"location":"rails/on_boarding/02_tests/chapter_II/#testing-associations","title":"Testing Associations","text":"<p>Your ebook application has relationships between models. Test that these associations are properly defined.</p>"},{"location":"rails/on_boarding/02_tests/chapter_II/#steps-to-implement_1","title":"Steps to implement:","text":"<ol> <li>Test <code>belongs_to</code> associations:</li> <li>Ebook belongs to User (seller)</li> <li>Purchase belongs to User (buyer)</li> <li>Purchase belongs to Ebook</li> <li>Test <code>has_many</code> associations:</li> <li>User has many Ebooks (as seller)</li> <li>User has many Purchases (as buyer)</li> <li>Ebook has many Purchases</li> <li>Test <code>has_many :through</code> associations (if applicable):</li> <li>User has many purchased ebooks through Purchases</li> <li>Verify dependent destroy behavior:</li> <li>When a user is deleted, what happens to their ebooks?</li> </ol> <p>Tip: You can use the <code>shoulda-matchers</code> gem to simplify association tests</p>"},{"location":"rails/on_boarding/02_tests/chapter_II/#testing-scopes","title":"Testing Scopes","text":"<p>Your ebook application should have scopes for filtering ebooks by status.</p>"},{"location":"rails/on_boarding/02_tests/chapter_II/#steps-to-implement_2","title":"Steps to implement:","text":"<ol> <li>Test the <code>published</code> scope (ebooks with status 'live'):    <pre><code>describe '.published' do\n  it 'returns only live ebooks' do\n    draft_ebook = Ebook.create(title: \"Draft\", status: :draft, ...)\n    live_ebook = Ebook.create(title: \"Live\", status: :live, ...)\n\n    expect(Ebook.published).to include(live_ebook)\n    expect(Ebook.published).not_to include(draft_ebook)\n  end\nend\n</code></pre></li> <li>Test the <code>by_seller</code> scope:</li> <li>Create ebooks for different users</li> <li>Verify the scope returns only ebooks from a specific seller</li> <li>Test scope chaining:</li> <li>Example: <code>Ebook.published.by_seller(user)</code></li> </ol>"},{"location":"rails/on_boarding/02_tests/chapter_II/#testing-instance-methods","title":"Testing Instance Methods","text":"<p>Your models should have custom methods. Test them thoroughly.</p>"},{"location":"rails/on_boarding/02_tests/chapter_II/#steps-to-implement_3","title":"Steps to implement:","text":"<ol> <li>Test User status methods:</li> <li><code>user.enable!</code> should set status to enabled</li> <li><code>user.disable!</code> should set status to disabled</li> <li><code>user.enabled?</code> should return true/false</li> <li>Test Ebook status transition methods:</li> <li><code>ebook.publish!</code> should change status from pending to live</li> <li><code>ebook.submit_for_review!</code> should change status from draft to pending</li> <li>Test ebook statistics methods (if implemented):</li> <li><code>ebook.view_count</code></li> <li><code>ebook.purchase_count</code></li> </ol>"},{"location":"rails/on_boarding/02_tests/chapter_II/#testing-callbacks","title":"Testing Callbacks","text":"<p>Your ebook application likely has callbacks for sending emails and tracking statistics.</p>"},{"location":"rails/on_boarding/02_tests/chapter_II/#steps-to-implement_4","title":"Steps to implement:","text":"<ol> <li>Test <code>after_create</code> callback on Purchase:</li> <li>Verify that purchase creation triggers the notification logic</li> <li>Test <code>before_save</code> callbacks:</li> <li>If you normalize data (e.g., downcase email), test it</li> <li>Test callbacks that update statistics:</li> <li>When a purchase is created, ebook statistics should update</li> </ol> <p>Warning: Avoid over-testing callbacks. Test the behavior, not the implementation.</p>"},{"location":"rails/on_boarding/02_tests/chapter_II/#exercise","title":"Exercise","text":"<p>Apply these concepts to your ebook application:</p> <ol> <li>Write comprehensive model specs for <code>Ebook</code>:</li> <li>Test all validations (title presence, status inclusion, price numericality)</li> <li>Test the association with User (seller)</li> <li>Test status transition methods</li> <li> <p>Test scopes: <code>published</code>, <code>by_seller</code>, <code>draft</code>, <code>pending</code></p> </li> <li> <p>Write comprehensive model specs for <code>User</code>:</p> </li> <li>Test email format validation</li> <li>Test the enable/disable status methods</li> <li>Test associations with ebooks (as seller)</li> <li> <p>Test associations with purchases (as buyer)</p> </li> <li> <p>Write specs for the <code>Purchase</code> model:</p> </li> <li>Test the association between buyer, seller, and ebook</li> <li>Test that a purchase records the correct price</li> <li>Test any callbacks that trigger notifications</li> </ol>"},{"location":"rails/on_boarding/02_tests/chapter_III/","title":"Chapter III - Mocking and Stubbing","text":"<p><code>(avr. time for this chapter: 1 to 2 days)</code></p> <p>When testing, you often need to isolate the code under test from its dependencies. Mocking and stubbing allow you to replace real objects with test doubles, making your tests faster, more reliable, and focused on the specific behavior you're testing.</p> <p>In this chapter, you will learn how to use RSpec's built-in mocking framework to create test doubles, stubs, and mocks\u2014particularly useful for testing your ebook purchase flow and email notifications.</p>"},{"location":"rails/on_boarding/02_tests/chapter_III/#what-are-test-doubles","title":"What are Test Doubles?","text":"<p>Test doubles are objects that stand in for real objects in your tests. They come in several flavors:</p> <ul> <li>Dummy - objects passed around but never used</li> <li>Stub - provides canned answers to calls made during the test</li> <li>Mock - objects pre-programmed with expectations</li> <li>Spy - records information about how it was called</li> <li>Fake - working implementations with shortcuts</li> </ul> <p>Reference: RSpec Mocks Documentation</p>"},{"location":"rails/on_boarding/02_tests/chapter_III/#creating-doubles","title":"Creating Doubles","text":""},{"location":"rails/on_boarding/02_tests/chapter_III/#steps-to-implement","title":"Steps to implement:","text":"<ol> <li>Create a simple double: <code>double(\"ebook\")</code></li> <li>Create a double with methods:    <pre><code>user_double = double(\"user\", name: \"John\", email: \"john@example.com\")\n</code></pre></li> <li>Use <code>instance_double</code> for verified doubles (recommended):    <pre><code>ebook_double = instance_double(Ebook, title: \"Ruby Guide\", price: 19.99)\n</code></pre></li> <li>Use <code>class_double</code> for stubbing class methods:    <pre><code>ebook_class = class_double(Ebook)\nallow(ebook_class).to receive(:published).and_return([ebook_double])\n</code></pre></li> </ol>"},{"location":"rails/on_boarding/02_tests/chapter_III/#stubbing-methods","title":"Stubbing Methods","text":"<p>Stubbing replaces method implementations with predetermined responses.</p>"},{"location":"rails/on_boarding/02_tests/chapter_III/#steps-to-implement_1","title":"Steps to implement:","text":"<ol> <li>Stub a method on a double:    <pre><code>allow(ebook).to receive(:price).and_return(29.99)\n</code></pre></li> <li>Stub a method on a real object (useful for User.find in controllers):    <pre><code>allow(User).to receive(:find).and_return(user_double)\nallow(Ebook).to receive(:published).and_return([ebook1, ebook2])\n</code></pre></li> <li>Stub with different return values for consecutive calls:    <pre><code>allow(ebook).to receive(:view_count).and_return(10, 11, 12)\n</code></pre></li> <li>Stub to raise an exception:    <pre><code>allow(ebook).to receive(:publish!).and_raise(InvalidStatusTransition)\n</code></pre></li> <li>Stub with block for dynamic responses:    <pre><code>allow(Ebook).to receive(:by_seller) { |user| ebooks.select { |e| e.seller == user } }\n</code></pre></li> </ol>"},{"location":"rails/on_boarding/02_tests/chapter_III/#mocking-with-expectations","title":"Mocking with Expectations","text":"<p>Mocks verify that methods are called as expected\u2014essential for testing your email notifications.</p>"},{"location":"rails/on_boarding/02_tests/chapter_III/#steps-to-implement_2","title":"Steps to implement:","text":"<ol> <li>Set expectation that the mailer is called when purchasing an ebook:    <pre><code>expect(PurchaseMailer).to receive(:seller_notification)\n</code></pre></li> <li>Verify method is called with specific arguments:    <pre><code>expect(PurchaseMailer).to receive(:seller_notification).with(seller, ebook, purchase)\n</code></pre></li> <li>Verify method is called a specific number of times:    <pre><code>expect(StatisticsTracker).to receive(:record_view).exactly(3).times\n</code></pre></li> <li>Use argument matchers:    <pre><code>expect(PurchaseMailer).to receive(:buyer_confirmation).with(anything, hash_including(ebook_id: ebook.id))\n</code></pre></li> </ol>"},{"location":"rails/on_boarding/02_tests/chapter_III/#using-spies","title":"Using Spies","text":"<p>Spies allow you to verify calls after the fact, which can make tests more readable.</p>"},{"location":"rails/on_boarding/02_tests/chapter_III/#steps-to-implement_3","title":"Steps to implement:","text":"<ol> <li>Create a spy for your mailer:    <pre><code>mailer_spy = spy(\"PurchaseMailer\")\n</code></pre></li> <li>Perform the action in your test</li> <li>Verify calls were made:    <pre><code>expect(mailer_spy).to have_received(:seller_notification).with(seller, ebook)\n</code></pre></li> </ol>"},{"location":"rails/on_boarding/02_tests/chapter_III/#mocking-external-services","title":"Mocking External Services","text":"<p>Your ebook application sends emails and tracks statistics. Mock these to avoid side effects in tests.</p>"},{"location":"rails/on_boarding/02_tests/chapter_III/#steps-to-implement_4","title":"Steps to implement:","text":"<ol> <li>Stub mailer deliveries:    <pre><code>allow(PurchaseMailer).to receive(:seller_notification).and_return(double(deliver_later: true))\nallow(PurchaseMailer).to receive(:buyer_confirmation).and_return(double(deliver_later: true))\n</code></pre></li> <li>Mock statistics tracking service:    <pre><code>allow(StatisticsService).to receive(:track_view)\nallow(StatisticsService).to receive(:track_download)\n</code></pre></li> <li>Use WebMock for external HTTP requests (if you have external APIs):    <pre><code>stub_request(:post, \"https://analytics.example.com/events\")\n  .to_return(status: 200, body: '{\"success\": true}')\n</code></pre></li> </ol> <p>Reference: WebMock</p>"},{"location":"rails/on_boarding/02_tests/chapter_III/#best-practices","title":"Best Practices","text":"<ul> <li>Mock what you don't own - mock email services, external APIs, not your own models</li> <li>Don't mock the object under test - test real behavior of Ebook, User, Purchase</li> <li>Use verified doubles - <code>instance_double(Ebook)</code> catches method name typos</li> <li>Prefer stubs over mocks - only mock when you need to verify calls</li> <li>Keep mocks simple - complex mock setups indicate design problems</li> </ul>"},{"location":"rails/on_boarding/02_tests/chapter_III/#exercise","title":"Exercise","text":"<p>Apply these concepts to your ebook application:</p> <ol> <li>Mock the email service in purchase tests:</li> <li>Stub <code>PurchaseMailer.seller_notification</code> to avoid sending real emails</li> <li>Verify that the mailer is called with correct seller and ebook</li> <li> <p>Verify that <code>buyer_confirmation</code> is called with purchase details</p> </li> <li> <p>Mock the statistics tracking:</p> </li> <li>Stub <code>Ebook#record_view</code> when testing the show action</li> <li>Verify view count is recorded with correct data (IP, browser, etc.)</li> <li> <p>Stub PDF download tracking</p> </li> <li> <p>Stub complex queries in controller tests:</p> </li> <li>Stub <code>Ebook.published</code> to return predictable data</li> <li>Stub <code>User.find</code> to return a test user</li> <li> <p>Use <code>instance_double</code> for ebook instances</p> </li> <li> <p>Test error handling:</p> </li> <li>Stub <code>ebook.purchase!</code> to raise an exception</li> <li>Verify your controller handles the error gracefully</li> <li>Test what happens when email delivery fails</li> </ol>"},{"location":"rails/on_boarding/02_tests/chapter_IV/","title":"Chapter IV - Factory Bot and Faker","text":"<p><code>(avr. time for this chapter: 1 day)</code></p> <p>Creating test data is a common need in testing. Instead of manually creating Ebook, User, and Purchase objects in every test, Factory Bot provides a flexible way to build test objects. Combined with Faker, you can generate realistic random data that makes your tests more robust.</p> <p>In this chapter, you will learn how to set up and use Factory Bot with Faker to create test data for your ebook application.</p>"},{"location":"rails/on_boarding/02_tests/chapter_IV/#setup","title":"Setup","text":""},{"location":"rails/on_boarding/02_tests/chapter_IV/#steps-to-implement","title":"Steps to implement:","text":"<ol> <li>Add <code>factory_bot_rails</code> gem to your Gemfile (<code>:development, :test</code> group)</li> <li>Add <code>faker</code> gem to your Gemfile</li> <li>Run <code>bundle install</code></li> <li>Configure Factory Bot in <code>spec/rails_helper.rb</code>:    <pre><code>RSpec.configure do |config|\n  config.include FactoryBot::Syntax::Methods\nend\n</code></pre></li> <li>Create <code>spec/factories/</code> directory for your factory files</li> </ol> <p>Reference: Factory Bot Rails</p>"},{"location":"rails/on_boarding/02_tests/chapter_IV/#basic-factories","title":"Basic Factories","text":"<p>Create factories for your ebook application models.</p>"},{"location":"rails/on_boarding/02_tests/chapter_IV/#steps-to-implement_1","title":"Steps to implement:","text":"<ol> <li>Create the User factory:    <pre><code># spec/factories/users.rb\nFactoryBot.define do\n  factory :user do\n    name { \"John Doe\" }\n    email { \"john@example.com\" }\n    password { \"password123\" }\n    status { :enabled }\n  end\nend\n</code></pre></li> <li>Create the Ebook factory:    <pre><code># spec/factories/ebooks.rb\nFactoryBot.define do\n  factory :ebook do\n    title { \"Ruby Programming Guide\" }\n    price { 19.99 }\n    status { :draft }\n    association :seller, factory: :user\n  end\nend\n</code></pre></li> <li>Use factories in tests:</li> <li><code>build(:user)</code> - creates an instance without saving</li> <li><code>create(:user)</code> - creates and saves to database</li> <li><code>build_stubbed(:ebook)</code> - creates a stub (faster, no DB)</li> <li><code>attributes_for(:ebook)</code> - returns a hash of attributes</li> </ol>"},{"location":"rails/on_boarding/02_tests/chapter_IV/#using-faker","title":"Using Faker","text":"<p>Faker generates realistic random data. This prevents test failures due to uniqueness constraints.</p>"},{"location":"rails/on_boarding/02_tests/chapter_IV/#steps-to-implement_2","title":"Steps to implement:","text":"<ol> <li>Update User factory with Faker:    <pre><code>factory :user do\n  name { Faker::Name.name }\n  email { Faker::Internet.unique.email }\n  password { Faker::Internet.password(min_length: 8) }\n  status { :enabled }\nend\n</code></pre></li> <li>Update Ebook factory with Faker:    <pre><code>factory :ebook do\n  title { Faker::Book.title }\n  description { Faker::Lorem.paragraph(sentence_count: 3) }\n  price { Faker::Commerce.price(range: 9.99..99.99) }\n  status { :draft }\n  association :seller, factory: :user\nend\n</code></pre></li> <li>Explore useful Faker generators for your ebook app:</li> <li><code>Faker::Book.title</code> - realistic book titles</li> <li><code>Faker::Book.author</code> - author names</li> <li><code>Faker::Commerce.price</code> - prices in range</li> <li><code>Faker::Internet.email</code> - email addresses</li> <li><code>Faker::Lorem.paragraph</code> - descriptions</li> <li><code>Faker::Date.backward(days: 30)</code> - past dates for purchases</li> </ol> <p>Reference: Faker Ruby</p>"},{"location":"rails/on_boarding/02_tests/chapter_IV/#sequences","title":"Sequences","text":"<p>Use sequences to generate unique values when Faker isn't enough.</p>"},{"location":"rails/on_boarding/02_tests/chapter_IV/#steps-to-implement_3","title":"Steps to implement:","text":"<ol> <li>Create a sequence for unique emails:    <pre><code>factory :user do\n  sequence(:email) { |n| \"user#{n}@example.com\" }\nend\n</code></pre></li> <li>Create a sequence for ebook titles:    <pre><code>factory :ebook do\n  sequence(:title) { |n| \"Ebook Volume #{n}\" }\nend\n</code></pre></li> </ol>"},{"location":"rails/on_boarding/02_tests/chapter_IV/#associations","title":"Associations","text":"<p>Factory Bot handles the relationships in your ebook application elegantly.</p>"},{"location":"rails/on_boarding/02_tests/chapter_IV/#steps-to-implement_4","title":"Steps to implement:","text":"<ol> <li>Define Ebook factory with seller association:    <pre><code>factory :ebook do\n  title { Faker::Book.title }\n  price { Faker::Commerce.price(range: 9.99..49.99) }\n  association :seller, factory: :user\nend\n</code></pre></li> <li>Define Purchase factory with all associations:    <pre><code>factory :purchase do\n  association :buyer, factory: :user\n  association :ebook\n  purchased_at { Time.current }\n  amount { ebook.price }\nend\n</code></pre></li> <li>Override associations when creating:    <pre><code>seller = create(:user, name: \"Seller Joe\")\nbuyer = create(:user, name: \"Buyer Jane\")\nebook = create(:ebook, seller: seller, price: 29.99)\npurchase = create(:purchase, buyer: buyer, ebook: ebook)\n</code></pre></li> </ol>"},{"location":"rails/on_boarding/02_tests/chapter_IV/#traits","title":"Traits","text":"<p>Traits allow you to define variations of your factories\u2014perfect for ebook statuses and user roles.</p>"},{"location":"rails/on_boarding/02_tests/chapter_IV/#steps-to-implement_5","title":"Steps to implement:","text":"<ol> <li>Define traits for Ebook statuses:    <pre><code>factory :ebook do\n  title { Faker::Book.title }\n  price { Faker::Commerce.price(range: 9.99..49.99) }\n  status { :draft }\n  association :seller, factory: :user\n\n  trait :draft do\n    status { :draft }\n  end\n\n  trait :pending do\n    status { :pending }\n  end\n\n  trait :published do\n    status { :live }\n  end\n\n  trait :expensive do\n    price { Faker::Commerce.price(range: 79.99..199.99) }\n  end\nend\n</code></pre></li> <li>Define traits for User roles:    <pre><code>factory :user do\n  name { Faker::Name.name }\n  email { Faker::Internet.unique.email }\n\n  trait :seller do\n    # Add seller-specific attributes if any\n  end\n\n  trait :buyer do\n    # Add buyer-specific attributes if any\n  end\n\n  trait :disabled do\n    status { :disabled }\n  end\nend\n</code></pre></li> <li>Use traits when building:    <pre><code>create(:ebook, :published)\ncreate(:ebook, :published, :expensive)\ncreate(:user, :disabled)\n</code></pre></li> </ol>"},{"location":"rails/on_boarding/02_tests/chapter_IV/#callbacks-and-transient-attributes","title":"Callbacks and Transient Attributes","text":"<p>Use these for complex setup scenarios.</p>"},{"location":"rails/on_boarding/02_tests/chapter_IV/#steps-to-implement_6","title":"Steps to implement:","text":"<ol> <li>Create a user with ebooks using transient attributes:    <pre><code>factory :user do\n  name { Faker::Name.name }\n  email { Faker::Internet.unique.email }\n\n  transient do\n    ebooks_count { 0 }\n  end\n\n  after(:create) do |user, evaluator|\n    create_list(:ebook, evaluator.ebooks_count, seller: user)\n  end\nend\n</code></pre></li> <li>Use in tests:    <pre><code>seller = create(:user, ebooks_count: 5)\nexpect(seller.ebooks.count).to eq(5)\n</code></pre></li> <li>Create ebook with purchases:    <pre><code>factory :ebook do\n  # ... other attributes ...\n\n  transient do\n    purchases_count { 0 }\n  end\n\n  after(:create) do |ebook, evaluator|\n    create_list(:purchase, evaluator.purchases_count, ebook: ebook)\n  end\nend\n</code></pre></li> </ol>"},{"location":"rails/on_boarding/02_tests/chapter_IV/#exercise","title":"Exercise","text":"<p>Apply these concepts to your ebook application:</p> <ol> <li>Create factories for all your models:</li> <li><code>User</code> factory with Faker for name and email</li> <li><code>Ebook</code> factory with Faker for title, description, and price</li> <li><code>Purchase</code> factory with proper associations</li> <li> <p><code>Tag</code> factory (if you implemented the tags system)</p> </li> <li> <p>Add traits to your factories:</p> </li> <li>Ebook: <code>:draft</code>, <code>:pending</code>, <code>:published</code>, <code>:with_pdf</code></li> <li> <p>User: <code>:enabled</code>, <code>:disabled</code>, <code>:seller_with_ebooks</code></p> </li> <li> <p>Create complex factories with transient attributes:</p> </li> <li>User with N ebooks: <code>create(:user, ebooks_count: 10)</code></li> <li> <p>Ebook with N purchases: <code>create(:ebook, :published, purchases_count: 5)</code></p> </li> <li> <p>Refactor existing tests:</p> </li> <li>Replace all manual <code>User.create</code> and <code>Ebook.create</code> with factories</li> <li>Use traits to simplify test setup</li> <li>Use <code>build</code> instead of <code>create</code> when you don't need database persistence</li> </ol>"},{"location":"rails/on_boarding/02_tests/chapter_V/","title":"Chapter V - Shared Examples and Shared Contexts","text":"<p><code>(avr. time for this chapter: 1 day)</code></p> <p>As your test suite grows, you'll notice patterns of repeated test code. Both User and Ebook have status functionality. Both need authentication in controller tests. RSpec provides powerful mechanisms for sharing test code: shared examples for reusable test cases and shared contexts for reusable setup.</p> <p>In this chapter, you will learn how to identify opportunities for sharing test code and implement shared examples and contexts for your ebook application.</p>"},{"location":"rails/on_boarding/02_tests/chapter_V/#what-are-shared-examples","title":"What are Shared Examples?","text":"<p>Shared examples are reusable groups of tests that can be included in multiple spec files. They're perfect for testing common behavior\u2014like the status functionality shared by User and Ebook models.</p> <p>Reference: RSpec Shared Examples</p>"},{"location":"rails/on_boarding/02_tests/chapter_V/#creating-shared-examples","title":"Creating Shared Examples","text":""},{"location":"rails/on_boarding/02_tests/chapter_V/#steps-to-implement","title":"Steps to implement:","text":"<ol> <li> <p>Create a shared example for status behavior (shared by User and Ebook):    <pre><code># spec/support/shared_examples/statusable.rb\nRSpec.shared_examples \"a model with status\" do\n  it \"has a status attribute\" do\n    expect(subject).to respond_to(:status)\n  end\n\n  it \"has a default status\" do\n    expect(subject.status).to be_present\n  end\nend\n</code></pre></p> </li> <li> <p>Include shared examples in your model specs:    <pre><code># spec/models/user_spec.rb\ndescribe User do\n  subject { create(:user) }\n  it_behaves_like \"a model with status\"\nend\n\n# spec/models/ebook_spec.rb\ndescribe Ebook do\n  subject { create(:ebook) }\n  it_behaves_like \"a model with status\"\nend\n</code></pre></p> </li> <li> <p>Configure RSpec to load support files in <code>spec/rails_helper.rb</code>:    <pre><code>Dir[Rails.root.join('spec/support/**/*.rb')].each { |f| require f }\n</code></pre></p> </li> </ol>"},{"location":"rails/on_boarding/02_tests/chapter_V/#shared-examples-for-ebook-status-transitions","title":"Shared Examples for Ebook Status Transitions","text":""},{"location":"rails/on_boarding/02_tests/chapter_V/#steps-to-implement_1","title":"Steps to implement:","text":"<ol> <li> <p>Create shared examples for publishable models:    <pre><code># spec/support/shared_examples/publishable.rb\nRSpec.shared_examples \"a publishable resource\" do\n  describe \"status transitions\" do\n    context \"when draft\" do\n      before { subject.status = :draft }\n\n      it \"can be submitted for review\" do\n        subject.submit_for_review!\n        expect(subject.status).to eq(\"pending\")\n      end\n    end\n\n    context \"when pending\" do\n      before { subject.status = :pending }\n\n      it \"can be published\" do\n        subject.publish!\n        expect(subject.status).to eq(\"live\")\n      end\n    end\n  end\nend\n</code></pre></p> </li> <li> <p>Use in Ebook spec:    <pre><code>describe Ebook do\n  subject { create(:ebook, :draft) }\n  it_behaves_like \"a publishable resource\"\nend\n</code></pre></p> </li> </ol>"},{"location":"rails/on_boarding/02_tests/chapter_V/#shared-examples-for-authentication","title":"Shared Examples for Authentication","text":""},{"location":"rails/on_boarding/02_tests/chapter_V/#steps-to-implement_2","title":"Steps to implement:","text":"<ol> <li> <p>Create shared examples for protected controller actions:    <pre><code># spec/support/shared_examples/requires_authentication.rb\nRSpec.shared_examples \"requires authentication\" do\n  it \"redirects to login when not authenticated\" do\n    expect(response).to redirect_to(login_path)\n  end\n\n  it \"returns unauthorized status for API requests\" do\n    # If you have API endpoints\n  end\nend\n</code></pre></p> </li> <li> <p>Use in controller specs:    <pre><code>describe EbooksController do\n  describe \"GET #new\" do\n    before { get :new }\n    it_behaves_like \"requires authentication\"\n  end\n\n  describe \"POST #create\" do\n    before { post :create, params: { ebook: attributes_for(:ebook) } }\n    it_behaves_like \"requires authentication\"\n  end\nend\n</code></pre></p> </li> </ol>"},{"location":"rails/on_boarding/02_tests/chapter_V/#shared-contexts","title":"Shared Contexts","text":"<p>Shared contexts are reusable setup blocks. They define <code>let</code> declarations, <code>before</code> hooks, and helper methods.</p> <p>Reference: RSpec Shared Context</p>"},{"location":"rails/on_boarding/02_tests/chapter_V/#creating-shared-contexts","title":"Creating Shared Contexts","text":""},{"location":"rails/on_boarding/02_tests/chapter_V/#steps-to-implement_3","title":"Steps to implement:","text":"<ol> <li> <p>Create an authenticated user context:    <pre><code># spec/support/shared_contexts/authentication.rb\nRSpec.shared_context \"authenticated user\" do\n  let(:current_user) { create(:user) }\n\n  before do\n    sign_in(current_user)\n  end\nend\n\nRSpec.shared_context \"authenticated seller\" do\n  let(:current_user) { create(:user, :seller) }\n  let!(:seller_ebooks) { create_list(:ebook, 3, seller: current_user) }\n\n  before do\n    sign_in(current_user)\n  end\nend\n</code></pre></p> </li> <li> <p>Include shared context in controller specs:    <pre><code>describe EbooksController do\n  include_context \"authenticated seller\"\n\n  describe \"GET #index\" do\n    it \"returns seller's ebooks\" do\n      get :index\n      expect(assigns(:ebooks)).to match_array(seller_ebooks)\n    end\n  end\nend\n</code></pre></p> </li> </ol>"},{"location":"rails/on_boarding/02_tests/chapter_V/#common-shared-contexts-for-ebook-application","title":"Common Shared Contexts for Ebook Application","text":""},{"location":"rails/on_boarding/02_tests/chapter_V/#steps-to-implement_4","title":"Steps to implement:","text":"<ol> <li> <p>Create a context with published ebooks:    <pre><code># spec/support/shared_contexts/ebook_data.rb\nRSpec.shared_context \"with published ebooks\" do\n  let(:seller) { create(:user) }\n  let!(:published_ebooks) { create_list(:ebook, 5, :published, seller: seller) }\n  let!(:draft_ebooks) { create_list(:ebook, 2, :draft, seller: seller) }\nend\n</code></pre></p> </li> <li> <p>Create a context for purchase testing:    <pre><code>RSpec.shared_context \"with purchase setup\" do\n  let(:seller) { create(:user) }\n  let(:buyer) { create(:user) }\n  let(:ebook) { create(:ebook, :published, seller: seller, price: 29.99) }\n\n  before do\n    sign_in(buyer)\n  end\nend\n</code></pre></p> </li> <li> <p>Create a frozen time context for testing time-sensitive features:    <pre><code>RSpec.shared_context \"frozen time\" do\n  let(:frozen_time) { Time.zone.parse(\"2024-06-15 10:00:00\") }\n\n  before { travel_to(frozen_time) }\n  after { travel_back }\nend\n</code></pre></p> </li> </ol>"},{"location":"rails/on_boarding/02_tests/chapter_V/#metadata-based-inclusion","title":"Metadata-based Inclusion","text":"<p>Automatically include contexts using RSpec metadata tags.</p>"},{"location":"rails/on_boarding/02_tests/chapter_V/#steps-to-implement_5","title":"Steps to implement:","text":"<ol> <li> <p>Define shared context with metadata:    <pre><code>RSpec.shared_context \"authenticated user\" do\n  let(:current_user) { create(:user) }\n  before { sign_in(current_user) }\nend\n</code></pre></p> </li> <li> <p>Configure automatic inclusion in <code>rails_helper.rb</code>:    <pre><code>RSpec.configure do |config|\n  config.include_context \"authenticated user\", :authenticated\nend\n</code></pre></p> </li> <li> <p>Use metadata to include context:    <pre><code>describe EbooksController, :authenticated do\n  # authenticated user context is automatically included\n\n  describe \"GET #new\" do\n    it \"returns success\" do\n      get :new\n      expect(response).to be_successful\n    end\n  end\nend\n</code></pre></p> </li> </ol>"},{"location":"rails/on_boarding/02_tests/chapter_V/#organization-best-practices","title":"Organization Best Practices","text":""},{"location":"rails/on_boarding/02_tests/chapter_V/#steps-to-implement_6","title":"Steps to implement:","text":"<ol> <li> <p>Organize shared examples by domain:    <pre><code>spec/support/shared_examples/\n\u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 statusable.rb\n\u2502   \u2514\u2500\u2500 publishable.rb\n\u2514\u2500\u2500 controllers/\n    \u2514\u2500\u2500 requires_authentication.rb\n</code></pre></p> </li> <li> <p>Organize shared contexts by purpose:    <pre><code>spec/support/shared_contexts/\n\u251c\u2500\u2500 authentication.rb\n\u251c\u2500\u2500 ebook_data.rb\n\u2514\u2500\u2500 time_helpers.rb\n</code></pre></p> </li> <li> <p>Document expected setup:    <pre><code># Requires subject to be a model with status attribute\n# subject { create(:ebook) }\nRSpec.shared_examples \"a model with status\" do\n  # ...\nend\n</code></pre></p> </li> </ol>"},{"location":"rails/on_boarding/02_tests/chapter_V/#exercise","title":"Exercise","text":"<p>Apply these concepts to your ebook application:</p> <ol> <li>Create shared examples for your models:</li> <li><code>\"a model with status\"</code> - test status attribute for User and Ebook</li> <li><code>\"a publishable resource\"</code> - test status transitions for Ebook</li> <li> <p><code>\"a model with timestamps\"</code> - test created_at and updated_at</p> </li> <li> <p>Create shared examples for controllers:</p> </li> <li><code>\"requires authentication\"</code> - test redirect for unauthenticated users</li> <li> <p><code>\"requires seller ownership\"</code> - test that users can only edit their own ebooks</p> </li> <li> <p>Create shared contexts:</p> </li> <li><code>\"authenticated user\"</code> - logged-in user setup</li> <li><code>\"authenticated seller\"</code> - seller with existing ebooks</li> <li><code>\"with published ebooks\"</code> - seed data for listing tests</li> <li> <p><code>\"with purchase setup\"</code> - buyer, seller, and ebook ready for purchase tests</p> </li> <li> <p>Refactor existing tests:</p> </li> <li>Identify repeated <code>before</code> blocks across specs</li> <li>Extract into shared contexts</li> <li>Identify common test patterns (status, authentication)</li> <li> <p>Extract into shared examples</p> </li> <li> <p>Use metadata for cleaner specs:</p> </li> <li>Tag controller specs with <code>:authenticated</code></li> <li>Configure automatic context inclusion</li> </ol>"},{"location":"rails/on_boarding/03_continuous_integration/chapter_I/","title":"Chapter I - Continuous Integration","text":"<p><code>(avr. time for this chapter: 1 to 2 days)</code></p> <p>Continuous Integration (CI) is the practice of automating the validation of your code. This process helps reduce the risk of introducing bugs and ensures your project remains in a deployable state at all times.</p> <p>In this chapter, you will set up an automated pipeline that validates your code on every push to the repository.</p>"},{"location":"rails/on_boarding/03_continuous_integration/chapter_I/#platform-selection","title":"Platform Selection","text":"<p>Choose one of the following CI/CD platforms for this exercise:</p> <ul> <li>GitHub Actions</li> <li>Bitbucket Pipelines</li> <li>GitLab CI/CD</li> </ul>"},{"location":"rails/on_boarding/03_continuous_integration/chapter_I/#pipeline-components","title":"Pipeline Components","text":"<p>Your CI pipeline should include the following stages:</p>"},{"location":"rails/on_boarding/03_continuous_integration/chapter_I/#1-project-setup","title":"1. Project Setup","text":"<p>Configure the environment with the correct tool versions.</p>"},{"location":"rails/on_boarding/03_continuous_integration/chapter_I/#steps-to-implement","title":"Steps to implement:","text":"<p>For Bitbucket Pipelines and GitLab:</p> <ol> <li>Use an official Docker image from Docker Hub with the required Ruby version</li> <li>Alternatively, create and use a custom Docker image</li> </ol> <p>For GitHub Actions:</p> <ol> <li>Choose the operating system (Linux, Windows, or macOS)</li> <li>Use the official Ruby setup action to configure the Ruby environment</li> </ol>"},{"location":"rails/on_boarding/03_continuous_integration/chapter_I/#2-run-tests","title":"2. Run Tests","text":"<p>Automated tests ensure your application behaves as expected as you add new features.</p>"},{"location":"rails/on_boarding/03_continuous_integration/chapter_I/#steps-to-implement_1","title":"Steps to implement:","text":"<ol> <li>Configure the pipeline to run your RSpec test suite</li> <li>Ensure all tests pass before proceeding to subsequent stages</li> <li>Configure test output for visibility in the CI dashboard</li> </ol> <p>Note: At this point in the onboarding, you should have RSpec tests from the testing chapters.</p>"},{"location":"rails/on_boarding/03_continuous_integration/chapter_I/#3-code-styling-linting","title":"3. Code Styling (Linting)","text":"<p>Consistent code styling improves readability and maintainability across the team.</p>"},{"location":"rails/on_boarding/03_continuous_integration/chapter_I/#steps-to-implement_2","title":"Steps to implement:","text":"<ol> <li>Add RuboCop to your project</li> <li>Configure RuboCop rules according to your team's standards</li> <li>Add a linting step to your CI pipeline</li> <li>Ensure the pipeline fails if linting errors are detected</li> </ol>"},{"location":"rails/on_boarding/03_continuous_integration/chapter_I/#4-test-coverage","title":"4. Test Coverage","text":"<p>Test coverage metrics help identify areas of your codebase that need additional testing.</p>"},{"location":"rails/on_boarding/03_continuous_integration/chapter_I/#steps-to-implement_3","title":"Steps to implement:","text":"<ol> <li>Add SimpleCov to your project</li> <li>Configure SimpleCov to generate coverage reports</li> <li>(Optional) Use simplecov-json for machine-readable output</li> <li>Add coverage reporting to your CI pipeline</li> <li>(Optional) Set minimum coverage thresholds</li> </ol> <p>Note: If parsing JSON output from the command line, consider using jq.</p>"},{"location":"rails/on_boarding/04_continuous_delivery/chapter_I/","title":"Chapter I - Continuous Delivery","text":"<p><code>(avr. time for this chapter: 2 days)</code></p> <p>Now that your project is automatically validated through Continuous Integration, the next step is to automate the deployment process. This practice is known as Continuous Delivery (CD).</p> <p>In this chapter, you will extend your CI pipeline to include automated deployments.</p>"},{"location":"rails/on_boarding/04_continuous_delivery/chapter_I/#platform-selection","title":"Platform Selection","text":"<p>Use the same platform you configured for Continuous Integration:</p> <ul> <li>GitHub Actions</li> <li>Bitbucket Pipelines</li> <li>GitLab CI/CD</li> </ul>"},{"location":"rails/on_boarding/04_continuous_delivery/chapter_I/#deployment-strategy","title":"Deployment Strategy","text":"<p>Before implementing automated deployments, consider the following important principles:</p>"},{"location":"rails/on_boarding/04_continuous_delivery/chapter_I/#key-considerations","title":"Key Considerations","text":"<ol> <li>Selective Deployment: Not every commit should trigger a deployment</li> <li>Avoid Redundancy: Do not repeat validation steps that were already executed in CI</li> <li>Branch Strategy: Define which branches trigger deployments</li> </ol>"},{"location":"rails/on_boarding/04_continuous_delivery/chapter_I/#git-branching-strategies","title":"Git Branching Strategies","text":"<p>Choose a branching strategy that fits your workflow. Common approaches include:</p> <ul> <li>Git Flow: Separate branches for features, releases, and hotfixes</li> <li>GitHub Flow: Simple workflow with feature branches and main</li> <li>Trunk-Based Development: Short-lived branches merged frequently to main</li> </ul> <p>Note: Discuss branching strategies with your tutor if you need guidance.</p>"},{"location":"rails/on_boarding/04_continuous_delivery/chapter_I/#pipeline-configuration","title":"Pipeline Configuration","text":""},{"location":"rails/on_boarding/04_continuous_delivery/chapter_I/#steps-to-implement","title":"Steps to implement:","text":"<ol> <li>Configure deployment to trigger only on specific branches (e.g., <code>main</code>, <code>production</code>)</li> <li>Ensure CI validation passes before deployment begins</li> <li>Set up deployment credentials securely using environment variables or secrets</li> <li>Configure deployment to your hosting platform (Render, Heroku, etc.)</li> <li>(Optional) Implement staging and production environments with separate deployment rules</li> </ol>"},{"location":"rails/on_boarding/04_continuous_delivery/chapter_I/#deployment-targets","title":"Deployment Targets","text":"<p>If you deployed your application in previous chapters, configure automated deployment to the same platform:</p> <ul> <li>Render</li> <li>Heroku</li> <li>Railway</li> <li>Other platform of your choice</li> </ul>"},{"location":"rails/on_boarding/04_continuous_delivery/chapter_I/#best-practices","title":"Best Practices","text":"<ul> <li>Store sensitive credentials as encrypted secrets, never in code</li> <li>Implement rollback procedures for failed deployments</li> <li>Consider implementing deployment notifications (Slack, email, etc.)</li> <li>Test your deployment pipeline with non-critical changes first</li> </ul>"},{"location":"rails/react_onboarding/Overview/","title":"Overview","text":""},{"location":"rails/react_onboarding/Overview/#welcome-to-the-react-knowledge-base-for-the-rails-unit","title":"Welcome to the React Knowledge Base for the Rails unit!","text":"<p>Following the links below, you will have all the resources you'll need to bootstrap your learning process of the JavaScript language and the React framework. There are also more advanced topics to explore, in case you want to expand your knowledge!</p> <p>Should you find any error or incorrect information written on these docs, please contact one of the contributors via Slack (you can find them at the bottom of this page), so that we can fix the issue at hand \ud83d\ude0a</p> <p>You are also welcome to contact us if you have any new resources that you find interesting to share!</p> <ul> <li>The Basics<ul> <li>Chapter I - An Introduction to JavaScript</li> <li>Chapter II - Managing packages and versions</li> <li>Chapter III - React for beginners</li> </ul> </li> <li>Advanced Topics<ul> <li>React Router</li> <li>Redux</li> <li>Next.js</li> <li>Authentication</li> </ul> </li> <li>Practice Exercise<ul> <li>Build your own app</li> </ul> </li> </ul>"},{"location":"rails/react_onboarding/01_The_Basics/chapter_I/","title":"Chapter I","text":""},{"location":"rails/react_onboarding/01_The_Basics/chapter_I/#an-introduction-to-javascript","title":"An introduction to Javascript","text":"<p>If you already have knowledge regarding the basics of JavaScript, feel free to proceed to the next chapter.</p> <p>Otherwise, if you are not yet very comfortable with JavaScript or if you are simply interested in reading, we also have something for you! Follow this guide linked below, which contains most of the topics you will ever need, to get started on the JS language:</p> <p>JavaScript Algorithms and Data Structures</p> <p>If you have limited time, we recommend reading at least the following topics:</p> <ul> <li>Basic JS</li> <li>ES6</li> <li>Functional Programming</li> </ul>"},{"location":"rails/react_onboarding/01_The_Basics/chapter_II/","title":"Chapter II","text":""},{"location":"rails/react_onboarding/01_The_Basics/chapter_II/#managing-packages-and-versions","title":"Managing packages and versions","text":"<p>When working with any JavaScript web application, theres at least two important things you will need to setup:</p> <ul> <li>a Node.js environment</li> <li>a package manager (yarn, npm, etc.)</li> </ul>"},{"location":"rails/react_onboarding/01_The_Basics/chapter_II/#node-version-manager","title":"Node Version Manager","text":"<p>During your journey with React.js, you will probably work on many projects that use different versions and environments of Node.js. That is where a version manager comes in handy.</p> <p>For example, a tool like Node Version Manager (NVM), allows you to download any of the remote Long Term Support (LTS) versions of Node.js with a simple command; to easily switch between multiple versions of Node.js, right from the command line, and set up aliases to switch between different downloaded versions of Node.js with ease. This switching can even be done automatically by setting up some scripts on your command line tools!</p> <p>All the information regarding the aforementioned features is available on the NVM official documentation, here: Node Version Manager (NVM).</p>"},{"location":"rails/react_onboarding/01_The_Basics/chapter_II/#alternatives","title":"Alternatives","text":"<p>Before proceeding with the installation, check if you have any restrictions on which version manager tool you can use. There are some alternatives, which you can find below:</p> <ul> <li>Nodenv</li> <li>ASDF <sup>[1]</sup></li> <li>N</li> </ul> <p><sup>[1]</sup> ASDF can actually manage multiple runtimes, not just Node. If you are already using it in other parts of your project, then you should consider using it over NVM or any of the other tools.</p>"},{"location":"rails/react_onboarding/01_The_Basics/chapter_II/#installation","title":"Installation","text":""},{"location":"rails/react_onboarding/01_The_Basics/chapter_II/#remove-existing-versions","title":"Remove existing versions","text":"<p>First, remove existing Node.js versions with the following commands (this is needed to avoid version conflicts later on). If this is your first time using Node.js and you don't have any versions installed yet you can just skip this step.</p> <pre><code>brew uninstall --ignore-dependencies node\nbrew uninstall --force node\n</code></pre>"},{"location":"rails/react_onboarding/01_The_Basics/chapter_II/#install-nvm","title":"Install NVM","text":"<p>Follow the official NVM documentation to find the most recent and up-to-date way to install NVM on your machine.</p>"},{"location":"rails/react_onboarding/01_The_Basics/chapter_II/#installing-a-nodejs-version-with-nvm","title":"Installing a Node.js version with NVM","text":"<p>First of all, see which Node versions are available to install. To see available versions, type:</p> <pre><code>nvm ls-remote\n</code></pre> <p>Now, you can install any version listed in the above output. You can also use aliases names like <code>node</code> for latest version, <code>lts</code> for latest LTS version, etc.</p> <pre><code>nvm install node     # Installing latest version\nnvm install 18       # Installing Node.js 18.X version\n</code></pre> <p>After installing, you can verify your locally available versions with the following command:</p> <pre><code>nvm ls\n</code></pre> <p>If you have multiple versions installed, you can set any of those versions as the default one, at any time. For example, to set the node 18.X as the default version, type the following:</p> <pre><code>nvm alias default 18\n</code></pre> <p>To switch between versions, use the command:</p> <pre><code>nvm use 18\n</code></pre>"},{"location":"rails/react_onboarding/01_The_Basics/chapter_II/#node-package-manager","title":"Node Package Manager","text":"<p>Once you start working on a new JavaScript web application project, you will be required to install certain package dependencies, which are like Ruby gems, but for JS.</p> <p>There are a few tools that facilitate this work, such as Node Package Manager (NPM). When installing a node version on your system, it usually already includes a working version of NPM that you can use!</p> <p>Essentially, NPM consists of two important components: a Command Line Interface (CLI) to interact with NPM via the terminal, and a registry that you can access, containing an extensive public database of JavaScript packages. You can even publish your own packages there!</p>"},{"location":"rails/react_onboarding/01_The_Basics/chapter_II/#alternatives_1","title":"Alternatives","text":"<p>Once more, before proceeding, check if you're already using another package manager tool in your project. If so, consider using it over NPM. Nevertheless, there is another popular alternative, called Yarn. The main advantage is that it offers faster installations due to being able to perform parallel installations. On the other hand NPM has better security features in (most recent versions at least). You can read more about Yarn by following the link below:</p> <ul> <li>Yarn</li> </ul>"},{"location":"rails/react_onboarding/01_The_Basics/chapter_II/#installation_1","title":"Installation","text":"<p>As previously mentioned, NPM itself usually does not need to be installed, since it is provided by default by your node version manager installation. We do not recommend installing NPM manually!</p> <p>Furthermore, to install any package using NPM you can first search for it on their website, then follow the instructions provided by the package author. For example, if you want to install the React package, you can open the previous link and follow the installation guide.</p> <p>In general, you can run the following commands to install any package:</p> <pre><code>npm install &lt;package name&gt;\n</code></pre> <p>You may also uninstall a package, simply by calling the npm uninstall command followed by the package name.</p> <pre><code>npm uninstall &lt;package name&gt;\n</code></pre>"},{"location":"rails/react_onboarding/01_The_Basics/chapter_III/","title":"Chapter III","text":""},{"location":"rails/react_onboarding/01_The_Basics/chapter_III/#react-for-beginners","title":"React for beginners","text":"<p>React is a popular open-source JavaScript library developed by Facebook for building user interfaces, particularly for single-page applications. It allows developers to create large web applications that can update and render efficiently in response to data changes. Here are some key features and concepts of React:</p> <ul> <li>Component-Based Architecture: React applications are built using components, which are reusable and independent pieces of the UI. Each component manages its own state and can be composed to build complex UIs.</li> <li>Virtual DOM: React uses a virtual DOM to optimize rendering. When the state of an object changes, React updates the virtual DOM first, then compares it with the real DOM and updates only the parts that have changed. This makes updates faster and more efficient.</li> <li>JSX Syntax: React uses JSX, a syntax extension for JavaScript that allows developers to write HTML-like code within JavaScript. This makes it easier to create and visualize the structure of the UI components.</li> <li>Unidirectional Data Flow: React enforces a one-way data flow, meaning data flows from parent components to child components. This makes it easier to understand and debug the application state.</li> <li>Hooks: React provides hooks, such as <code>useState</code> and <code>useEffect</code>, which allow developers to use state and other React features in functional components.</li> <li>Ecosystem and Community: React has a large ecosystem of libraries and tools, as well as a strong community, which provides a wealth of resources, tutorials, and third-party components.</li> </ul> <p>React is widely used in the industry and is known for its performance, scalability, and simplicity.</p>"},{"location":"rails/react_onboarding/01_The_Basics/chapter_III/#getting-started","title":"Getting started","text":"<p>There is no better way to get started in React than reading their official documentation.</p>"},{"location":"rails/react_onboarding/01_The_Basics/chapter_III/#improving-your-knowledge","title":"Improving your knowledge","text":"<p>Once you're done with the official documentation, and if you want to expand your knowledge, then follow the Beginner's Guide to React. It is a 28-part course with videos and lessons that will help you learn things like:</p> <ul> <li>What is JSX and how to use it effectively with React</li> <li>Create simple and reusable React components</li> <li>Style React components with className and inline Styles</li> <li>What are hooks and how to create your own</li> <li>Make and manage basic forms</li> <li>Make HTTP requests</li> <li>Install and use React DevTools for debugging</li> <li>Build and deploy a React Application with Codesandbox, GitHub, and Netlify</li> </ul>"},{"location":"rails/react_onboarding/01_The_Basics/chapter_III/#extras","title":"Extras","text":"<p>Your journey through React does not have to end here though! Keep reading the following sections to know more about specific frameworks, packages, tools, etc.</p> <ul> <li>Redux</li> <li>React-Router</li> <li>Next.js</li> <li>Authentication</li> </ul>"},{"location":"rails/react_onboarding/02_Advanced_Topics/authentication/","title":"Authentication","text":"<p>When building a front-end web application in React, you will most likely end up needing a mechanism to perform authenticated requests to your Rails application.</p> <p>To achieve that, usually the best approach is to implement a way to store some credentials in your front-end, either via cookies or local storage.</p>"},{"location":"rails/react_onboarding/02_Advanced_Topics/authentication/#the-process","title":"The Process","text":"<p>In general, when implementing any type of authentication in your app, you will want to follow these guidelines:</p> <ul> <li>(Backend) Set up an endpoint to create/start a new session and to return the session credentials (either via response payload or via headers)</li> <li>(Frontend) Store the returned credentials somewhere in your app (can be browser LocalStorage, Cookies, or something else custom)</li> <li>(Frontend) Implement some call to your backend API and include the stored session credentials, via request headers for example</li> <li>(Backend) Validate the credentials received from the request, and deny/accept the request as necessary.</li> <li>(Backend) If the request had invalid/expired credentials, return some error. Otherwise, execute the requested endpoint</li> <li>(Frontend) Add some error handling for the scenarios where your request may be invalid or if your credentials may have already expired.</li> </ul>"},{"location":"rails/react_onboarding/02_Advanced_Topics/authentication/#all-in-one-guide","title":"All-in-One Guide","text":"<p>Additionally, we have a more complete guide on building authentication between Rails and React apps.</p> <p>This guide contains recommendations on how to set up a simple authentication mechanism, using JWT tokens and Cookies, to communicate with the back-end Rails app and to store the credentials, respectively.</p>"},{"location":"rails/react_onboarding/02_Advanced_Topics/next_js/","title":"Next.js","text":"<p>Next.js is a popular open-source React framework developed by Vercel that enables developers to build server-side rendered (SSR) and statically generated web applications. It provides a range of features and optimizations out of the box, making it easier to build performant and scalable applications. Key features of Next.js include:</p> <ul> <li>Server-Side Rendering (SSR): Next.js allows pages to be rendered on the server, improving performance and SEO.</li> <li>Static Site Generation (SSG): Next.js can generate static HTML at build time, which can be served directly by a CDN.</li> <li>API Routes: Next.js provides a way to create API endpoints within the same application, simplifying the development of full-stack applications.</li> <li>File-Based Routing: Next.js uses a file-based routing system where the file structure in the pages directory maps directly to the routes of the application.</li> <li>Automatic Code Splitting: Next.js automatically splits the code for each page, ensuring that only the necessary code is loaded for each page.</li> <li>Built-in CSS and Sass Support: Next.js supports importing CSS and Sass files directly in JavaScript files.</li> <li>Image Optimization: Next.js includes an image component that optimizes images for faster loading and better performance.</li> <li>TypeScript Support: Next.js has built-in TypeScript support, making it easy to use TypeScript in your projects.</li> </ul> <p>Next.js is widely used for building modern web applications due to its flexibility, performance optimizations, and ease of use.</p>"},{"location":"rails/react_onboarding/02_Advanced_Topics/next_js/#more-information","title":"More information","text":"<p>If you want to get started with Next.js, you can follow their quickstart guide or even build a test app with the help of their free tutorial.</p>"},{"location":"rails/react_onboarding/02_Advanced_Topics/react_router/","title":"React Router","text":"<p>React Router is a standard library for routing in React applications. It enables the navigation among views of various components in a React application, allows changing the browser URL, and keeps the UI in sync with the URL. React Router provides a collection of navigational components like <code>&lt;Router&gt;</code>, <code>&lt;Route&gt;</code>, <code>&lt;Link&gt;</code>, and <code>&lt;Switch&gt;</code> to build single-page applications with dynamic routing.</p> <p>Here's an example of how you can create and set up a router for your web application.</p> <ol> <li> <p>Install <code>react-router-dom</code> package:</p> <pre><code>npm install react-router-dom\n</code></pre> </li> <li> <p>Set up the router somewhere in your application code, as follows:</p> <pre><code>import React from 'react';\nimport ReactDOM from 'react-dom';\nimport { RouterProvider, createBrowserRouter } from 'react-router-dom';\n\nconst router = createBrowserRouter([\n  {\n    path: '/',\n    element: &lt;BaseAppLayout /&gt;,\n    children: [\n      { path: '/', element: &lt;HomePage /&gt; },\n      { path: 'about', element: &lt;AboutPage /&gt; },\n      { path: '*', element: &lt;NotFoundPage /&gt; },\n    ],\n  },\n]);\n\nReactDOM.render(\n  &lt;RouterProvider router={router} /&gt;,\n  document.getElementById('root')\n);\n</code></pre> </li> <li> <p>On your <code>BaseAppLayout</code> component, add an <code>Outlet</code>:</p> <pre><code>import React from 'react';\nimport { Outlet } from 'react-router-dom';\n\nfunction BaseAppLayout() {\n  return (\n    &lt;div&gt;\n      &lt;h1&gt;My App&lt;/h1&gt;\n      &lt;Outlet /&gt;\n    &lt;/div&gt;\n  );\n}\n\nexport default App;\n</code></pre> <p>This <code>Outlet</code> will render all the nested routes in your app, depending on which one is active at a time, of course.</p> </li> </ol>"},{"location":"rails/react_onboarding/02_Advanced_Topics/react_router/#more-information","title":"More information","text":"<p>If you're interested in learning more about React Router, you can follow their tutorial on creating an Address Book.</p>"},{"location":"rails/react_onboarding/02_Advanced_Topics/redux/","title":"Redux (Legacy)","text":"<p>Redux is a global state management library. It has an official integration with React, called React-Redux.</p>"},{"location":"rails/react_onboarding/02_Advanced_Topics/redux/#read-before-using","title":"Read before using","text":"<p>Before the advent of functional programming and the existence of hooks, Redux was a frequently used tool, when you needed to make data available across your entire React app. This \"overall state\" approach is, however, not without its drawbacks. Much like the <code>ContextProviders</code> and <code>useContext</code> hooks offered by React, which should be used sparingly, Redux suffers from the same issue. If you start accessing the global \"store\" in multiple places of your app, you may unintentionally be decreasing its performance, because you might be passing too much information to each component, all the time. This is because whenever any part of this state is changed, usually all your components accessing it will have to re-render. <sup>[1]</sup></p> <p>Nowadays, Redux also offers a hook for fetching data from the global store, <code>useSelector</code>, where you can pass an arrow function to select just the value(s) you need from the store. If you need to build more complex state management, usually Redux is coupled with the <code>reselect</code> package, that allows creating custom selectors with better optimizations.</p> <p>In a nutshell, if you are starting a new React application, we advise you to stick to React hooks and internal state management per component, with the exception here and there for sharing contexts. You should only opt for Redux if you have a strict restriction that forces you to use it, or if you're maintaining an ongoing project that uses it!</p> <p><sup>[1]</sup> There are tools within Redux that can help mitigate this issue, namely by implementing memoized selectors.</p>"},{"location":"rails/react_onboarding/02_Advanced_Topics/redux/#getting-started","title":"Getting started","text":"<p>You can learn more about Redux and how to use it in your web application by watching this free course and tutorial, made by Dan Abramov himself, the creator of React-Redux.</p> <p>Ideally, with this tutorial, you will be able to:</p> <ul> <li>Learn how to add proper state management to your React app</li> <li>Dive through reducers and how to manipulate state changes</li> <li>Learn to propagate the changes to the components</li> <li>Build a \"to-do\" list app using Redux</li> </ul>"},{"location":"rails/react_onboarding/03_Practice_Exercise/spotify/","title":"Building your own app","text":"<p>Assuming you've read everything else beforehand (or if you're already comfortable with React), then the next steps for you should be the creation of a React app to communicate with an external API, in this case Spotify's API.</p> <p>Please follow the link above to get started on how to use and set up their API when building your local web app.</p> <p>In terms of requisites for this app you will be building, these are the tasks we want you to complete:</p> <p>(each task's estimated duration is within brackets and color coded in terms of difficulty)</p> <ul> <li>Start up your React app project locally using Vite (1-2 hours)</li> <li>A single public access page, with a button to log in to a Spotify account (4-8 hours)</li> <li>Once logging in, show a top banner with:<ul> <li>a search bar that allows searching for artists, albums and/or tracks, and shows the results of the search on a dropdown container with links to each (3-6 hours)</li> <li>the current user's avatar, name and a logout button (&lt;1 hour)</li> </ul> </li> <li>Several pages protected by authentication, for the following:<ul> <li>a Home page, showing the \"top 5 artists\" and \"top 5 tracks\" with links to each (1-2 hours)</li> <li>a dynamic page for each artist, containing the artist's albums and top tracks, with links to each of them (&lt;1 hour)</li> <li>a dynamic page for each album, containing information and links to all tracks (&lt;1 hour)</li> <li>a dynamic page for each track, showing some basic information (i.e. name, image, release date, artist name, album name, track duration and popularity) (&lt;1 hour)</li> </ul> </li> </ul> <p>If you have any doubts on how to implement some of these pages, we suggest reading back on how to use React Router, perform Authentication and the basics of state management in React!</p> <p>As you complete each task, feel free to publish your code on a public repository (i.e. on Github), so that we can guide you and evaluate your work.</p> <p>Happy coding!</p>"},{"location":"rails/ruby/installation/","title":"Installing Ruby","text":""},{"location":"rails/ruby/installation/#using-rvm-ruby-version-manager","title":"Using RVM (Ruby Version Manager)","text":"<p>RVM is a command-line tool which allows you to easily install, manage, and work with multiple ruby environments from interpreters to sets of gems. https://rvm.io/rvm/install </p>"}]}